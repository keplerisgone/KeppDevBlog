## 1ì£¼ì°¨

![[Media/ai00-2023-1 2.pdf|ai00-2023-1 2.pdf]]

- ê°•ì˜ì‹¤ì´ ì¢ì•„ì„œ ì˜®ê¸¸ ìˆ˜ë„
- ëŒ€ë©´ê°•ì˜ëŠ” ì‹¤ìŠµ ìœ„ì£¼
- ëª©ìš”ì¼ ~ ìˆ˜ìš”ì¼ì´ ì¶œì„ì¸ì •ê¸°ê°„ì¸ë“¯ìš”
- Google Colab
    - ë§ˆí¬ë‹¤ìš´ ì…€
    - ì½”ë“œ ì…€
        - ì„œë²„ì—ì„œ ì‹¤í–‰í•˜ëŠ” ê²ƒì´ê¸° ë•Œë¬¸ì— ì–´ì©Œêµ¬â€¦
- ë³¸ì¸ ì•„ì§ë„ ë”•ì…”ë„ˆë¦¬ ì˜ ì“¸ì¤„ ëª¨ë¦„
- ìë™ìœ¼ë¡œ auto ê°€ ì ìš©ë˜ëŠ” ê°“-íŒŒì´ì¬
- ë‚˜ì¤‘ì— ì‹œí€€ìŠ¤ë‘ ë”•ì…”ë„ˆë¦¬ë‘ í´ë˜ìŠ¤ë§Œ ë³¼ë˜â€¦.

>> google colab : [https://colab.research.google.com/](https://colab.research.google.com/)

## 3ì£¼ì°¨ - Numpy, Matplotlib

## Numpy

- íŒŒì´ì¬ì˜ ìˆ˜ì¹˜ê³„ì‚° ë¼ì´ë¸ŒëŸ¬ë¦¬

### ë‹¤ì°¨ì› ë°°ì—´

- n ì°¨ì› ë°°ì—´ = n-dimension array = ndarray
    - 1ì°¨ì› ë°°ì—´ : `a[5] = {1,2,3,4,5};` ê°™ì€ ê±°,
    - 2ì°¨ì› ë°°ì—´ : a[5][3] = {{1,2,3,4,5},{1,2,3,4,5},{1,2,3,4,5}} ê°™ì€ ê±°

### Import

```Python
import numpy as np
```

### ndarray

- Numpy ì˜ ìë£Œí˜•
- cì–¸ì–´ì˜ arrayë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ê¸° ë•Œë¬¸ì—, pythonì˜ list ë³´ë‹¤ ì—°ì‚°ì´ ë¹ ë¥´ê³  ë©”ëª¨ë¦¬ ê³µê°„ì„ ì ˆì•½í•œë‹¤.

```Python
x = np.ndarray([1,2,3])
print(type(x)) # numpy.ndarray
print(x) # [1,2,3]
```

- np.array()
    
    - 2ì°¨ì› ë°°ì—´ ìƒì„± : [[ 1, 2],[3,4]] ê³¼ ê°™ì´ ìƒì„±
    
    ```Python
    # 1a.3
    x = np.array([[1, 2],[3, 4]])
    
    print(x)
    ```
    
- dtype ì†ì„±
    
    - ì•Œì•„ì„œ ì§€ì •í•˜ê¸°ëŠ” í•œë°
    
    ```Python
    # 1a.4
    x = np.array([1,2,3,4], dtype=np.int64)
    print(x)
    print(x.dtype)
    
    x = np.array([1.0,2.0,3.0,4.0], dtype = np.float64)
    print(x)
    print(x.dtype)
    ```
    
- ndarray.shape
    
    - ë°°ì—´ì˜ ëª¨ì–‘
    - ex) [[1,2,3],[4,5,6]] ì¼ ê²½ìš° (2,3)
    
    ```Python
    # 1a.5
    x = np.array([1,2,3,4])
    x.shape
    ```
    
- ë‹¤ì–‘í•œ ìƒì„± í•¨ìˆ˜
    
    ```Python
    a = np.zeros((2,2)) # Create an array of all zeros
    print(a)
    
    b = np.ones((1,2))  # Create an array of all ones
    print(b)
    
    c = np.full((2,2),7) # Create a constant array
    print(c)
    
    d = np.eye(2) # Create a 2x2 identity matrix
    print(d)
    
    e = np.random.random((2,2)) # Create an array filled with random values
    print(e)
    ```
    

### arange()

- np.arange( start, stop, step )
    
    - start ~ stop ì„ step ë§Œí¼ ë„ì–´ì„œ aray ìƒì„±
    
    ```Python
    # 1a.8
    
    a = np.arange(10)
    b = np.arange(5,10)
    c = np.arange(3,10,2)
    
    print(a)
    print(b) # [5,6,7,8,9]
    print(c) # [3,5,7,9]
    
    print(type(a))
    ```
    

### Array Math

- í˜•ìƒì´ ê°™ìœ¼ë©´ ê°™ì€ ìœ„ì¹˜ì˜ element ë¼ë¦¬ ì—°ì‚°ì„ ìˆ˜í–‰
    
    ```Python
    # 1a.9
    x = np.array([1.0,2.0,3.0])
    y = np.array([2.0, 4.0, 6.0])
    
    print(x + y)
    print(x - y)
    print(x * y)
    print(x / y)
    ```
    
- scalar ì—°ì‚°ì€ ëª¨ë“  elements ì™€ ì—°ì‚° ìˆ˜í–‰
    
    ```Python
    # 1a.11
    x = np.array([1.0, 2.0, 3.0])
    
    print(x + 1.0)
    print( x / 2.0)
    ```
    
- dot() : 1ì°¨ì›ì€ ë‚´ì , 2ì°¨ì›ì€ í–‰ë ¬ê³±ì„ ìˆ˜í–‰í•œë‹¤.
    
    ```Python
    # 1a.13
    
    x = np.array([[1,2],
                  [3,4]])
    y = np. array([[5,6],[7,8]])
    
    v = np.array([9,10])
    w = np.array([11,12])
    
    print(np.dot(v,w)) # 219 = ë‚´ì 
    print(np.dot(x,v))
    print(np.dot(x,y))
    ```
    

### Shape and Reshape

- ì´ë¯¸ ì¡´ì¬í•˜ëŠ” nparrayë¥¼ ì§€ì •í•œ shapeìœ¼ë¡œ ë³€ê²½

```Python
# 1a.14

# np.reshape(newshape) << np : ndarray

y = np.arange(24)
print(y)

y = y.reshape(3,8) # newshape : int or int tuple
print(y)
```

```Python
# 1a.15

# unknown dimension = -1
# ì•Œì•„ì„œ ì¶”ì •í•˜ë¼ëŠ” ëœ»

print(y)
print( y.reshape(2,-1))   # -1 -> 12
print( y.reshape(-1,6))   # -1 -> 4
```

### Broadcasting

```Python
# 1a.16

# í˜•ìƒì´ ë‹¤ë¥¸ ë°°ì—´ë¼ë¦¬ì˜ ê³„ì‚°
  # ì‘ì€ ë°°ì—´ì„ í° ë°°ì—´ë§Œí¼ í™•ëŒ€í•œ í›„ ì—°ì‚°

A = np.array([[1,2]
              ,[3,4]])
B = np.array([10,20])

print(A * B) # B -> [[10,20],[10,20]]
```

### Indexing

```Python
# 1a.18

# indexing ë°©ë²•ì€ Cì–¸ì–´ì™€ ê°™ë‹¤

X = np.array([[51,55],
              [14,19],
              [0,4]])

print(X, '\n')
print(X[0],'\n')
print(X[0][1],'\n')
```

### Slicing

```Python
# 1a.19

# list ì™€ ë¹„ìŠ·í•œ ë°©ì‹, ì°¨ì›ì˜ ê¸¸ì´ë¥¼ ë‚˜ëˆ„ëŠ” ë°©ì‹

a = np.array([[1,2,3,4],
              [5,6,7,8],
              [9,10,11,12]])
print(a, '\n')

b = a[:2,1:3]
print(b)
```

```Python
# 1a.20

# ìŠ¬ë¼ì´ì‹±ì€ ê°™ì€ ì£¼ì†Œë¥¼ ê³µìœ í•˜ëŠ” ê²ƒì— ê°€ê¹ë‹¤

print(a, '\n')

b[0,0] = 77 \#b[0,0] = a[0,1], ê°™ì€ ë°ì´í„° ì €ì¥
print(a)
```

### Loops

```Python
# 1a.21

import numpy as np

a = np.array([ [1,2,3,4], [5,6,7,8], [9,10,11,12]])

for row in a:
    print('-', row) # ê° rowë¥¼ ì¶œë ¥

# - [1,2,3,4]
# - [5,6,7,8]
# - [9,10,11,12]
```

### Advanced Indexing

```Python
# 1a.22

# integer array indexing

X = np.array([[51,55],
              [14,19],
              [0,4]])

Y = X.flatten() # Xë¥¼ 1ì°¨ì›ìœ¼ë¡œ ë§Œë“¤ì–´ Yì— ë„£ëŠ”ë‹¤

print(Y)
print(Y[np.array([0,2,4])]) # index ê°€ 0,2,4ì¸ ê°’ì„ ê°€ì ¸ì˜¤ê¸°
```

```Python
# 1a.23

# Boolran array indexing
print(X,'\n')

z = X > 15 # ê° element ì— ëŒ€í•´ ì—°ì‚° ìˆ˜í–‰
print(z, '\n') 

print( X[z], '\n') # z = True ì¸ ê²½ìš°ë§Œ

[[51 55]
 [14 19]
 [ 0  4]] 

[[ True  True]
 [False  True]
 [False False]] 

[51 55 19]
```

### Data type

```Python
# 1a.24

x = np.array([1,2]) # ë‚´ë¶€ê°€ ì •ìˆ˜ì´ë©´
print(x.dtype) # -> np.int64

x = np.array([1.0, 2,0]) # ë‚´ë¶€ê°€ ì‹¤ìˆ˜ì´ë©´
print(x.dtype) # -> np.float64

x = np.array([1, 2], dtype = np.int64) # Force a particular datatype
print(x.dtype)
```

## Matplotlib

- ì´ê±° ì§„ì§œ ê°ì²´ ìƒì„± ì•ˆí•´ë„ ë¨?????

### import

```Python
# 1a.25

import matplotlib.pyplot as plt
```

### ë‹¨ìˆœí•œ ê·¸ë˜í”„ ê·¸ë¦¬ê¸°

```Python
# 1a.26

# ì§ì„  ê·¸ë˜í”„

plt.plot([1,2,3,4]) # (x,y) -> (index,list)
plt.show()

# ë°ì´í„° ì¶”ê°€

plt.plot([1,2,3,4,5,2,8,3,7]) # (x,y) -> (index,list)
plt.show()

# xì¶• ê°’ ì¶”ê°€

plt.plot( [10, 20, 30, 40, 50, 60],[1,2,3,4,3,3]) # X -> Y
plt.show()

# sin í•¨ìˆ˜ ê·¸ë˜í”„

x = np.arange(0, 6, 0.1) # 0~5.9 ê¹Œì§€ 0.1ë§Œí¼
y = np.sin(x)

plt.plot(x,y)
plt.show()

# clipìœ¼ë¡œ yê°’ ë²”ìœ„ ì œí•œí•˜ê¸°

x = np.arange(0, 6, 0.1)
y = np.log(x)

y = np.clip( y, -0.5, 0.5) # -0.5 ~ 0.5 ë¡œ ì œí•œ

plt.plot(x,y)
plt.show()

# sin() ì™¸ì—ë„ exp(), log(), tanh(), fabs(), clip()...
# fabs() - ë¶€ë™ ì†Œìˆ˜ì  ì ˆëŒ€ê°’ ì—°ì‚°
# clip() - (arr, min, max) - arrì˜ minë³´ë‹¤ ì‘ì€ ê°’ì„ minìœ¼ë¡œ ë°”ê¾¸ê³ , maxë³´ë‹¤ í° ê°’ì„ maxë¡œ ë°”ê¾¼ë‹¤.
```

### ë‹¤ë¥¸ ê¸°ëŠ¥ë“¤

```Python
# 1a.31

# ë‹¤ë¥¸ ê¸°ëŠ¥ë“¤

x = np.arange(0,6,0.1)
y1 = np.sin(x)
y2 = np.cos(x)

plt.plot(x, y1, label='sin') # line label
plt.plot(x, y2, linestyle='--', label = 'cos') # linestyle - '--', '-.', ':','-'

plt.xlabel('x') # x-axis label
plt.ylabel('y') # y-axis label

plt.title('sin & cos') # graph title

plt.legend()

plt.show()
```

[[ğŸ”— LINK]](https://zephyrus1111.tistory.com/17) â€” [ê¸°íƒ€] Matplotlib ì„  ì¢…ë¥˜(Line style) ì§€ì •

### ì´ë¯¸ì§€ í‘œì‹œ

```Python
# ì´ë¯¸ì§€ -> ndarray : imread( URL )
# ndarray -> ì´ë¯¸ì§€ : imshow( ndarray )
# ì´ë¯¸ì§€ ì£¼ì†Œ: 'http://esohn.be/images/yonsei.png'

img = plt.imread('http://esohn.be/images/yonsei.png')

plt.imshow(img)
plt.show()

plt.imshow(img, interpolation = 'bilinear')
plt.axis('off')
plt.show()
```

  

## 4ì£¼ì°¨ : í¼ì…‰íŠ¸ë¡ 

## 1. í¼ì…‰íŠ¸ë¡ 

- ë‰´ëŸ°ì˜ ë™ì‘ ì›ë¦¬ë¥¼ ëª¨ë¸ë§
- ë‹¤ìˆ˜ì˜ ì‹ í˜¸ë¥¼ ì…ë ¥ë°›ì•„ í•˜ë‚˜ì˜ ì‹ í˜¸ë¥¼ ì¶œë ¥
    - íë¥¸ë‹¤ : 1
    - ì•ˆ íë¥¸ë‹¤ : 0
- ì…ë ¥ìœ¼ë¡œ 2ê°œì˜ ì‹ í˜¸ë¥¼ ë°›ëŠ” í¼ì…‰íŠ¸ë¡ 
    
    - Inputs : x1, x2
    - Output : y
    - Weights : w1, w2
    - x1w1 + x2w2 > threshold = 1
    
    ![[Media/Untitled 26.png|Untitled 26.png]]
    

## 2. ë‹¨ìˆœ ë…¼ë¦¬íšŒë¡œ

### AND gate

- ì…ë ¥ 2ê°œ, ì¶œë ¥ 1ê°œ
- truth table -> ëª¨ë‘ 1ì¼ ë•Œë§Œ 1 ì¶œë ¥
- í¼ì…‰íŠ¸ë¡ ?

### NAND gate

- not AND gate

### OR gate

- ë‘˜ ì¤‘ í•˜ë‚˜ë¼ë„ 1ì´ë©´ 1 ì¶œë ¥

### Rosenblatt's Algorithm

- weightë¥¼ ëœë¤ìœ¼ë¡œ ì¡ìŒ
- ì˜ˆì¸¡ì´ 1ì¸ë° ê°’ì´ 0ì´ ë‚˜ì™”ìœ¼ë©´ weightë¥¼ ì¦ê°€
- ì˜ˆì¸¡ì´ 0ì¸ë° ê°’ì´ 1ì´ ë‚˜ì™”ìœ¼ë©´ weightë¥¼ ê°ì†Œ
- ì—ëŸ¬ê°€ ì¤„ì–´ë“¤ ë•Œê¹Œì§€ ê³„ì† ë°˜ë³µ

```Python
# 2.1
# AND êµ¬í˜„

def AND(x1,x2):
  w1, w2, theta = 1.0, 1.0, 1.0

  tmp = w1*x1 + w2*x2 # sigma(ê°€ì¤‘ì¹˜ * input)

  if tmp <= theta:
    return 0
  elif tmp > theta:
    return 1

print(AND(0, 0)) # 0
print(AND(1, 0)) # 0
print(AND(0, 1)) # 0
print(AND(1, 1)) # 1
```

### Bias ë„ì…

- ë‚˜ì¤‘ì„ ìœ„í•´ ë‹¤ë¥¸ ë°©ì‹ìœ¼ë¡œ ìˆ˜ì •
- (-theta) = bias
- 0ë³´ë‹¤ í¬ê±°ë‚˜ ì‘ìŒì„ íŒë‹¨ì˜ ê¸°ì¤€ìœ¼ë¡œ ì‚¼ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```Python
# 2.2
import numpy as np

x = np.array([0,1])
w = np.array([0.5,0.5])
b = -0.7

print(w*x) # 0, 0.5
print(np.sum(w*x)) # 0.5
print(np.sum(w*x)+b) # -0.2
```

- biasë¥¼ ì‚¬ìš©í•´ì„œ AND, NAND, OR gateë¥¼ êµ¬í˜„í•´ë´…ì‹œë‹¤

```Python
# 2.3
# AND gate
import numpy as np

def AND(x1,x2):
  x = np.array([x1,x2])
  w = np.array([0.5,0.5])
  b = -0.7

  tmp = np.sum(w*x) + b
  if tmp <= 0:
    return 0
  else:
    return 1

for xs in [(0,0),(1,0),(0,1),(1,1)]:
  y = AND(xs[0],xs[1])
  print(str(xs) + " -> " + str(y))
```

```Python
# 2.4
# NAND gate

import numpy as np

def NAND(x1,x2):
  x = np.array([x1,x2])
  w = np.array([-0.5,-0.5])
  b = 0.7
  tmp = np.sum(w*x) + b

  if tmp  <= 0:
    return 0
  else:
    return 1

for xs in [(0,0),(1,0),(0,1),(1,1)]:
  y = NAND(xs[0],xs[1])
  print(str(xs) + " -> " + str(y))
```

```Python
# 2.5
# OR gate

import numpy as np

def OR(x1,x2):
  x = np.array([x1,x2])
  w = np.array([0.5,0.5])
  b = -0.3
  tmp = np.sum(w*x) + b

  if tmp  <= 0:
    return 0
  else:
    return 1

for xs in [(0,0),(1,0),(0,1),(1,1)]:
  y = OR(xs[0],xs[1])
  print(str(xs) + " -> " + str(y))
```

### ê°€ì¤‘ì¹˜ì™€ í¸í–¥

- weights : ì…ë ¥ì‹ í˜¸ê°€ ê²°ê³¼ì— ì£¼ëŠ” ì˜í–¥ë ¥ - í•´ë‹¹ ìš”ì†Œê°€? ì–¼ë§ˆë‚˜? ì˜í–¥ì„ ì£¼ëŠ”ê°€?
- Bias : í¼ì…‰íŠ¸ë¡ ì´ ì–¼ë§ˆë‚˜ ì‰½ê²Œ í™œì„±í™”ë˜ëŠ”ê°€

## 4. í¼ì…‰íŠ¸ë¡ ì˜ í•œê³„

### í¼ì…‰íŠ¸ë¡ ì˜ ì‹œê°í™”

- 0ê³¼ 1ì„ ì¶œë ¥í•˜ëŠ” ì˜ì—­ì„ êµ¬ë¶„
- OR ê²Œì´íŠ¸ëŠ” ì›ê³¼ ì‚¼ê°í˜•ì„ ì§ì„ ìœ¼ë¡œ ë‚˜ëˆ ì•¼ í•¨
    - 0 - ì› 1 - ì‚¼ê°í˜•ì¼ ë•Œ ì§ì„ ìœ¼ë¡œ ë‚˜ëˆ 
    - XORì€ ë ê¹Œìš”?
        
        ![[Media/Untitled 1 18.png|Untitled 1 18.png]]
        

### XOR gate

- = Exclusive-OR
- ë‘˜ì´ ë‹¬ë¼ì•¼ 1 ì¶œë ¥
- => ì§ì„ ìœ¼ë¡œ ê·¸ë˜í”„ë¥¼ ê·¸ë¦¬ëŠ” í¼ì…‰íŠ¸ë¡ ì€ ì‹œê°í™”ê°€ ì–´ë ¤ì›€
- Nonlinear ë¼ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤

---

## 5. ë‹¤ì¸µ í¼ì…‰íŠ¸ë¡ 

- í¼ì…‰íŠ¸ë¡ ì„ ì—¬ëŸ¬ ì¸µìœ¼ë¡œ ìŒ“ì€ ëª¨ë¸
- = Neural Network
    
    ### XOR problem
    
- ì—¬ëŸ¬ê°œì˜ ì¸µì„ ì‚¬ìš©í•˜ë©´ ëœë‹¤ (í•˜ë‚˜ë¡œëŠ” ì•ˆ ë˜ì§€ë§Œ)
- NAND + OR -> AND

```Python
# 2.6
# XOR gate

import numpy as np

def XOR(x1,x2):
  s1 = NAND(x1,x2) # ë²„ë¸”ì„ ë‹¤ëŠ” ê²ƒê³¼ ë¹„ìŠ·í•¨
  s2 = OR(x1,x2)
  y = AND(s1,s2)

  return y

for xs in [(0,0),(1,0),(0,1),(1,1)]:
  y = XOR(xs[0],xs[1])
  print(str(xs) + " -> " + str(y))
```

  

## 5ì£¼ì°¨ : ì‹ ê²½ë§

## ì‹ ê²½ë§

- í¼ì…‰íŠ¸ë¡ ì˜ ë¬¸ì œì 
    - 1ì¸µ í¼ì…‰íŠ¸ë¡ ì€ ì„ í˜•ë¶„ë¦¬ ì™¸ì˜ ë¬¸ì œë¥¼ í•´ê²°í•  ìˆ˜ ì—†ìŒ
    - MLP : ì¸µì„ ëŠ˜ë¦¬ë©´ í•´ê²°

## ì‹ ê²½ë§ì˜ ì˜ˆ

- 2ì¸µ ì‹ ê²½ë§
    - ì…ë ¥ì¸µ : input layer
    - ì€ë‹‰ì¸µ : hidden layer
    - ì¶œë ¥ì¸µ : output layer
- ì´ 3ì¸µì´ì§€ë§Œ ê°€ì¤‘ì¹˜ë¥¼ ê°–ëŠ” ì¸µì€ 2ì¸µì´ë‹¤ (ì…ë ¥ì¸µ ì œì™¸)

## í™œì„±í™” í•¨ìˆ˜

- ì…ë ¥ì‹ í˜¸ì˜ ì´í•©ì„ ì¶œë ¥ì‹ í˜¸ë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜
- ì…ë ¥ ì‹ í˜¸ì˜ ì´í•©ì´ í™œì„±í™”ë¥¼ ì¼ìœ¼í‚¤ëŠ”ì§€ íŒë‹¨
- ê°€ì¤‘ì¹˜ì˜ í•©ì„ ë³€ìˆ˜ë¡œ ë°›ì•„ 0,1ì„ ì¶œë ¥í•˜ëŠ” í•¨ìˆ˜
- $a = b + x_1w_1 + x_2w_2$ï»¿ ì¼ ë•Œ, $h(a)$ï»¿ = $0$ï»¿ or $1$ï»¿ = $y$ï»¿

### ê³„ë‹¨ í•¨ìˆ˜

- ì„ê³„ê°’ì„ ê²½ê³„ë¡œ ì¶œë ¥ì´ ë°”ë€œ
- í¼ì…‰íŠ¸ë¡ ì—ì„œì˜ í™œì„±í™” í•¨ìˆ˜

### ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜

- 0ê³¼ 1ì‚¬ì´ì˜ ì—°ì†ì ì¸ output
- ì‹œê·¸ëª¨ì´ë“œ : Sì ëª¨ì–‘
- ì‹ ê²½ë§ë¶€í„°ëŠ” í™œì„±í™” í•¨ìˆ˜ë¡œ ì´ë¥¼ ì‚¬ìš©
    
    ![[Media/Screenshot_2023-05-04_at_5.13.39_PM 2.png|Screenshot_2023-05-04_at_5.13.39_PM 2.png]]
    
- ê³„ë‹¨ í•¨ìˆ˜ êµ¬í˜„
    
    ```Python
    # 3.1
    # ë‹¨ìˆœí•œ í˜•íƒœì˜ ê³„ë‹¨ í•¨ìˆ˜
    
    def step_function(x):
      if x > 0 :
        return 1
      else:
        return 0
    
    # 3.2
    # numpy ë°°ì—´ì…ë ¥ ê°€ëŠ¥ 
    
    def step_function(x):
      y = x > 0 # True / False ë¡œ y ì— ì €ì¥
      return y.astype(np.int64) # ì´ë¥¼ 1ê³¼ 0ë¡œ ë³€í™˜
    
      # ì•„ë˜ ê³¼ì •ìœ¼ë¡œ ìì„¸íˆ ì„¤ëª…
    
    import numpy as np
    
    x = np.array([-1.0 , 1.0, 2.0])
    
    print(x) # -1.0, 1.0, 2.0
    
    y = x > 0
    print(y) \#Boolean -> [False, True, True]
    y.astype(np.int64)
    print(y) # int -> [0,1,1]
    
    # 3.5
    
    import matplotlib.pyplot as plt
    
    X = np.arange(-5.0, 5.0, 0.1)
    Y = step_function(X) # ê³„ë‹¨ í•¨ìˆ˜ êµ¬í˜„
    plt.plot(X,Y)
    plt.ylim(-0.1, 1.1)
    plt.show
    ```
    

### ì‹œê·¸ëª¨ì´ë“œì™€ ê³„ë‹¨ í•¨ìˆ˜ ë¹„êµ

ì°¨ì´ì 

1. ì‹œê·¸ëª¨ì´ë“œëŠ” ê³„ë‹¨ì— ë¹„í•´ ë¶€ë“œëŸ¬ìš´ ê³¡ì„ , ì¶œë ¥ì´ ì—°ì†ì ìœ¼ë¡œ ë³€í™”
2. ì‹œê·¸ëª¨ì´ë“œëŠ” 0~1, ê³„ë‹¨ì€ 0 or 1

ê³µí†µì 

1. ì…ë ¥ì´ ì‘ì„ ë•ŒëŠ” 0, í´ ë•ŒëŠ” 1ì¶œë ¥
2. ì¶œë ¥ì€ í•­ìƒ 0~1
3. ë¹„ì„ í˜• í•¨ìˆ˜ -> ì¸µì„ ê¹Šê²Œ í•  ìˆ˜ ìˆë‹¤
    - í™œì„±í™” í•¨ìˆ˜ê°€ ì„ í˜• í•¨ìˆ˜, cxê°™ì€ ê±°ë¼ë©´ 3ì¸µìœ¼ë¡œ í†µê³¼ì‹œì¼œë„ ê²°êµ­ c^3xìœ¼ë¡œ ì„ í˜• í•¨ìˆ˜ì™€ ê°™ì•„ì§„ë‹¤
    - ì‹œê·¸ëª¨ì´ë“œ, ê³„ë‹¨ í•¨ìˆ˜ ê°™ì€ ê²½ìš°ëŠ” ë¹„ì„ í˜•ì´ê¸° ë•Œë¬¸ì— í†µê³¼ì‹œí‚¬ ìˆ˜ë¡ ê°’ì´ ë‹¬ë¼ì§„ë‹¤.

- ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜ êµ¬í˜„
    
    ```Python
    # 3.6
    # ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜ êµ¬í˜„
    
    def sigmoid(x):
      return 1 / (1 + np.exp(-x))
    ```
    

### ReLU í•¨ìˆ˜

- ì…ë ¥ì´ 0ì„ ë„˜ìœ¼ë©´ ê·¸ëŒ€ë¡œ ì¶œë ¥
- ìŒìˆ˜ì´ë©´ 0 ì¶œë ¥
- ReLU í•¨ìˆ˜ êµ¬í˜„
    
    ```Python
    # 3.9
    # ReLU í•¨ìˆ˜
    
    def relu(x):
      return np.maximum(0,x) # 0ë²¡í„°ì™€ x array ì—ì„œ ìœ„ì¹˜ë§ˆë‹¤ í° ê°’ì„ ê³ ë¥´ëŠ” ê±°ë‹ˆê¹Œ...
    ```
    
    +) np.max ì™€ np.maximumì˜ ì°¨ì´
    
    [[ğŸ”— LINK]](https://jimmy-ai.tistory.com/70) â€” [Numpy] ìµœëŒ€ê°’, ìµœì†Œê°’ í•¨ìˆ˜ np.max vs np.maximum ì°¨ì´ (np.minê³¼ np.minimum)
    

## ë‹¤ì°¨ì› ë°°ì—´ì˜ ê³„ì‚°

### ë‹¤ì°¨ì› ë°°ì—´

- ì‹ ê²½ë§ì„ êµ¬ì„±í•˜ê¸° ìœ„í•´ ì‚¬ìš©, numpyë¡œ êµ¬í˜„ì´ ê°€ëŠ¥
    
    ```Python
    # 3.10
    import numpy as np
    
    A = np.array([1,2,3,4])
    print(A)
    
    print(np.ndim(A)) # 1
    print( A.shape ) #(4,) <- ì´ê±´ ì™œ ì´ë ‡ê²Œ ë‚˜ì˜¬ê¹Œ ì¼ì°¨ì› ë°°ì—´ì€ ê·¸ëƒ¥ ì´ë ‡ê²Œ ë‚˜ì˜¨ë‹¤
    print( A.shape[0]) # 4
    
    # 3.11
    B = np.array([[1,2],[3,4],[5,6]])
    print(B)
    
    print( np.ndim(B)) 
    print( B.shape)
    ```
    

### ì‹ ê²½ë§ì—ì„œì˜ í–‰ë ¬ê³±

![[Media/Screenshot_2023-05-04_at_5.39.03_PM 2.png|Screenshot_2023-05-04_at_5.39.03_PM 2.png]]

```Python
# 3.14

X = np.array([1,2]) # input
print(X.shape)

W = np.array([[1,3,5],[2,4,6]]) # ê°€ì¤‘ì¹˜
print(W.shape)

Y = np.dot(X,W)
print(Y)
```

## 3ì¸µ ì‹ ê²½ë§ êµ¬í˜„

### 3ì¸µ ì‹ ê²½ë§

- ì…ë ¥ì¸µ - 2ê°œ
- ì²«ë²ˆì§¸ ì€ë‹‰ì¸µ - 3ê°œ
- ë‘ë²ˆì§¸ ì€ë‹‰ì¶© - 2ê°œ
- ì¶œë ¥ì¸µ - 2ê°œ

### í‘œê¸°ë²• ì„¤ëª…

- ê°€ì¤‘ì¹˜ ì˜¤ë¥¸ìª½ ìœ„ : nì¸µì˜ ê°€ì¤‘ì¹˜
- ê°€ì¤‘ì¹˜ ì˜¤ë¥¸ìª½ ì•„ë˜ : ë‹¤ìŒì¸µ ì¸ë±ìŠ¤, ì• ì¸µ ì¸ë±ìŠ¤
    
    ![[Media/Screenshot_2023-05-04_at_5.46.55_PM 2.png|Screenshot_2023-05-04_at_5.46.55_PM 2.png]]
    

```Python
# 3.15
# ì²«ë²ˆì§¸ - ì…ë ¥ì¸µ -> ì€ë‹‰ì¸µ1
X = np.array([1.0, 0.5])
W1 = np.array([[0.1,0.3,0.5],[0.2,0.4,0.6]]) # 2->3 ì´ë‹ˆê¹Œ 3x2
B1 = np.array([0.1,0.2,0.3])

A1 = np.dot(X,W1) + B1
print(A1)

Z1 = sigmoid(A1)
print(Z1)

# ë‘ë²ˆì¬ - ì€ë‹‰ì¸µ1 -> ì€ë‹‰ì¸µ2
W2 = np.array([[0.1,0.4],[0.2,0.5],[0.3,0.6]]) # 3->2 ì´ë‹ˆê¹Œ 2x3 
B2 = np.array([0.1,0.2])

print(Z1.shape)
print(W2.shape)
print(B2.shape)

A2 = np.dot(Z1,W2) + B2
Z2 = sigmoid(A2)

print(A2)
print(Z2)

# ì€ë‹‰ì¸µ -> ì¶œë ¥ì¸µë„ ë˜‘ê°™ì´ í•˜ì‹œë©´ ë©ë‹ˆë‹¤
```

- í•˜ë‚˜ë¡œ êµ¬í˜„í•´ë´…ì‹œë‹¤
    
    ```Python
    # 3.20
    
    # ì‹ ê²½ë§ ì´ˆê¸°í™” (ê°€ì¤‘ì¹˜ì™€ í¸í–¥)
    def init_network():
      network = {}
      network['W1'] = np.array([[0.1, 0.3, 0.5], [0.2, 0.4, 0.6]])
      network['b1'] = np.array([0.1, 0.2, 0.3])
      network['W2'] = np.array([[0.1, 0.4], [0.2, 0.4], [0.3, 0.6]])
      network['b2'] = np.array([0.1, 0.2])
      network['W3'] = np.array([[0.1, 0.3], [0.2, 0.4]])
      network['b3'] = np.array([0.1, 0.2])
      return network
    
    # ë„¤íŠ¸ì›Œí¬ë¥¼ ì—´ì‹¬íˆ ëŒë¦°ë‹¤
    def forward( network, x ):
      W1, W2, W3 = network['W1'], network['W2'], network['W3']
      b1, b2, b3 = network['b1'], network['b2'], network['b3']
      a1 = np.dot(x, W1) + b1
      z1 = sigmoid(a1)
      a2 = np.dot(z1, W2) + b2
      z2 = sigmoid(a2)
      a3 = np.dot(z2, W3) + b3
      y = identity_function(a3)
      return y
      
    network = init_network()
    x = np.array([1.0, 0.5])
    y = forward( network, x )
    print(y)
    ```
    

### ì¶œë ¥ì¸µ ì„¤ê³„

---

### ì§€ë„í•™ìŠµ

- ë¶„ë¥˜
    - ë¯¸ë¦¬ ì •ì˜ëœ í´ë˜ìŠ¤ ì¤‘ í•˜ë‚˜ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ê²ƒ
        - ìŠ¤íŒ¸ë©”ì¼, ì •ìƒë©”ì¼ ë¶„ë¥˜
        - ì†ê¸€ì”¨ ë¶„ë¥˜
        - ì†Œí”„íŠ¸ë§¥ìŠ¤ í•¨ìˆ˜ ì˜ˆì¸¡
- íšŒê·€
    - ì—°ì†ì ì¸ ìˆ˜ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ê²ƒ
        - ê¸°ìƒë°ì´í„°ë¡œ ë‚´ì¼ ê¸°ì˜¨ ì˜ˆì¸¡
        - ê³µë¶€ì‹œê°„ìœ¼ë¡œ ì„±ì  ì˜ˆì¸¡
        - í•­ë“± í•¨ìˆ˜

### í•­ë“± í•¨ìˆ˜

- ì…ë ¥ì„ ê·¸ëŒ€ë¡œ ì¶œë ¥

### ì†Œí”„íŠ¸ë§¥ìŠ¤ í•¨ìˆ˜

![[Media/Screenshot_2023-05-04_at_5.54.11_PM 2.png|Screenshot_2023-05-04_at_5.54.11_PM 2.png]]

- í´ë˜ìŠ¤ ë¶„ë¥˜ë¥¼ ìœ„í•´ ë§ˆì§€ë§‰ ì¶œë ¥ê°’ì„ ì •ê·œí™” í•˜ëŠ” í•¨ìˆ˜
- ëª¨ë“  ê°’ì˜ í•©ì´ 1ì´ê¸° ë•Œë¬¸ì— í™•ë¥  ìˆ˜ì¹˜í™”ì— ìœ ë¦¬
- ì‹ì—ì„œ ì•Œ ìˆ˜ ìˆë“¯ì´ ëª¨ë“  ì…ë ¥ê°’ì— ì˜í–¥ì„ ë°›ìŒ

```Python
# 1D ì…ë ¥ì„ ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” ì†Œí”„íŠ¸ë§¥ìŠ¤ í•¨ìˆ˜

def softmax(a): # arrayë¥¼ ë°›ëŠ”ë‹¤
	c = np.max(a) # ì˜¤ë²„í”Œë¡œë¥¼ ë§‰ê¸° ìœ„í•´ arrayì—ì„œ ì œì¼ í° ê°’ì„ ë¹¼ì¤ë‹ˆë‹¤
  exp_a = np.exp(a-c)
  sum_exp_a = np.sum(exp_a)
  y = exp_a / sum_exp_a

  return y

# 2D ì…ë ¥ì„ ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” ì†Œí”„íŠ¸ë§¥ìŠ¤ í•¨ìˆ˜

def softmax(a):
    if a.ndim >= 2:
        c = np.max( a, axis=1, keepdims=True ) # ì²«ë²ˆì§¸ ì¶•ì„ ê¸°ì¤€ìœ¼ë¡œ í•˜ëŠ”ê²Œ ë¬´ìŠ¨ ë§ì¼ê¹Œ
        exp_a = np.exp(a - c)
        sum_exp_a = np.sum( exp_a, axis=1, keepdims=True )
    else:
        c = np.max(a)
        exp_a = np.exp(a - c)
        sum_exp_a = np.sum(exp_a)

    return exp_a / sum_exp_a
```

### ì¶œë ¥ì¸µì˜ ë‰´ëŸ°ìˆ˜ ì •í•˜ê¸°

- ì£¼ë¡œ í´ë˜ìŠ¤ ìˆ˜ = ì¶œë ¥ì¸µì˜ ë‰´ëŸ° ìˆ˜

- ex) 0~9ë¡œ ë¶„ë¥˜í•˜ê³  ì‹¶ìœ¼ë©´ ì¶œë ¥ì¸µì˜ ë‰´ëŸ°ì€ 10ê°œ

## ì†ê¸€ì”¨ ë¶„ë¥˜í•˜ê¸°

- ìš°ì„  ë°ì´í„°ì…‹ì„ ë°›ê³ , testìš© ë°ì´í„°ì…‹ê³¼ trainìš© ë°ì´í„° ì…‹ì„ ë‚˜ëˆ•ë‹ˆë‹¤.
- ê° ì´ë¯¸ì§€ íŒŒì¼ì€ 28x28=784 í¬ê¸°ë¡œ ì´ë£¨ì–´ì ¸ìˆìœ¼ë©°, ê° ë¹„íŠ¸ëŠ” í‘ë°±ì„ í‘œí˜„í•©ë‹ˆë‹¤.

```Python
from sklearn.datasets import fetch_openml
X, y = fetch_openml('mnist_784', version=1, return_X_y=True)

X = X.values.astype(np.uint8)
y = y.values.astype(np.uint8)

x_train = X[:60000] # í›ˆë ¨ìš© 60000ê°œ
t_train = y[:60000] # í›ˆë ¨ìš© ì •ë‹µ 60000ê°œ
x_test = X[60000:]
t_test = y[60000:]

print(x_train.shape) # ê° ì´ë¯¸ì§€ íŒŒì¼ ì •ë³´
print(t_train.shape) # ì •ë‹µ
print(x_test.shape) # testí•  ê²ƒ
print(t_test.shape)

#(60000, 784)
#(60000,)
#(10000, 784)
#(10000,)
```

- ì˜ˆì‹œìš©ìœ¼ë¡œ ì´ë¯¸ì§€ë¥¼ ì¶œë ¥í•´ë´…ì‹œë‹¤.

```Python
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.image as mpimg

img = x_train[0] # í›ˆë ¨ìš© ì´ë¯¸ì§€ ì¤‘ í•œ ì¥ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.
label = t_train[0] # í›ˆë ¨ìš© ì²«ë²ˆì§¸ ì´ë¯¸ì§€ì˜ ì •ë‹µì„ ê°€ì ¸ì˜µë‹ˆë‹¤.

print(img.shape) # (784,)
img = img.reshape(28,28) # 28x28 ìœ¼ë¡œ reshape
print(img.shape) # (28,28)

plt.imshow(img,cmap='gray')
plt.show()
```

- ì—¬ëŸ¬ ê°œ ì´ë¯¸ì§€ë¥¼ ì¶œë ¥í•´ë´…ì‹œë‹¤
    - np.random.choice :

```Python
img_count = 5 * 4

fig = plt.figure(figsize=(10,10)) # 10x10ì¸ì¹˜ì˜ figureë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
indices = np.random.choice(x_train.shape[0],img_count) # x.trainì˜ ì´ë¯¸ì§€ 60000ê°œ ì¤‘ cntë§Œí¼ ì´ë¯¸ì§€ë¥¼ ì„ íƒí•©ë‹ˆë‹¤.

imgs = x_train[indices] # arrayì•ˆì— index arrayë¥¼ ë„£ì–´ë„ ë˜ë‚˜ìš” ë˜ë„¤ ì‹ ê¸°í•˜ë‹¤
labels = t_train[indices] # ì§„ì§œ ì‹ ê¸°í•˜ë„¤ <- link

for i in range(img_count):
  ax = fig.add_subplot(5,4,i+1)
  ax.set_title(labels[i])
  ax.set_xticks([])
  ax.set_yticks([])
  ax.imshow(imgs[i].reshape(28,28), cmap = 'gray')

plt.show
```

- ì‹ ê²½ë§ì„ êµ¬í˜„í•©ì‹œë‹¤

```Python
import pickle

def get_data():
    
    return x_test / 255, t_test

def init_network():
    with open('sample_weights.pkl', 'rb') as f:
        network = pickle.load(f)
        
    return network

def predict(network, x): # ì€ë‹‰ì¸µì´ 3ê°œì¸ 5ì¸µ ì‹ ê²½ë§ì…ë‹ˆë‹¤
  W1,W2,W3 = network['W1'], network['W2'], network['W3']
  b1,b2,b3 = network['b1'], network['b2'], network['b3']

  a1 = np.dot(x,W1) + b1
  z1 = sigmoid(a1)
  a2 = np.dot(z1,W2) + b2
  z2 = sigmoid(a2)
  a3 = np.dot(z2,W3) + b3

  y = softmax(a3)
  return y

x, t = get_data() # x_testì˜ ì±„ë„ ì •ë³´, ì •ë‹µì„ ê°€ì ¸ì˜µë‹ˆë‹¤.
network = init_network() # ë„¤íŠ¸ì›Œí¬ ì´ˆê¸°í™”

accuracy_count = 0 # ì •í™•ë„ ì´ˆê¸°í™”

for i in range(len(x)):
  y = predict(network, x[i]) # ì˜ˆì¸¡ê°’ì„ ë„¤íŠ¸ì›Œí¬ë¡œ ê³„ì‚°í•©ë‹ˆë‹¤
  p = np.argmax(y) # ê°€ì¥ í° ì¸ë±ìŠ¤ë¥¼ ì •ë‹µìœ¼ë¡œ ì˜ˆì¸¡í•©ë‹ˆë‹¤

  if p == t[i]: # ì˜ˆì¸¡ê°’ì´ ì •ë‹µê³¼ ë§ì•˜ë‹¤ë©´
    accuracy_count += 1 # ì •í™•ë„ ì¦ê°€

print( 'Accuracy: {}'.format(accuracy_count / len(x)))

print(x.shape) # 10000, 784
print(x[0].shape) # 784,
print(W1.shape) # 784, 50
print(W2.shape) # 50, 100
print(W3.shape) # 50, 10
```

- ë°°ì¹˜ ì²˜ë¦¬ë¡œ ë”ìš± íš¨ìœ¨ì ìœ¼ë¡œ ê³„ì‚°í•´ë´…ì‹œë‹¤
    - ë°°ì¹˜ ì²˜ë¦¬ëŠ” setì„ ì›í•˜ëŠ” ê°„ê²©ë§Œí¼ ì˜ë¼ ê³„ì‚°í•˜ëŠ” ê²ƒì„ ë§í•©ë‹ˆë‹¤.

```Python
x, t = get_data()
network = init_network()

batch_size = 100
accuracy_count = 0

for i in range(0, len(x), batch_size): # 100ë§Œí¼ ì˜ë¼ì„œ 
  x_batch = x[i:i+batch_size]
  y_batch = predict(network, x_batch) # ê³„ì‚°ì„ í•œ ë‹¤ìŒì—
  p = np.argmax(y_batch, axis=1) # axisëŠ” ì–´ë–¤ ì¶•ì„ ê¸°ì¤€ìœ¼ë¡œ í• ê±´ì§€ (axis=1ì€ ë‘ë²ˆì§¸ ì°¨ì›ì„ ê¸°ì¤€ìœ¼ë¡œ) <- link
  accuracy_count += np.sum(p == t[i:i+batch_size]) # ë°°ì—´ ì¸ë±ì‹±ì´ ì‚¬ìš©ë˜ì—‡ìŠµë‹ˆë‹¤

print('Accuracy: {}'.format(accuracy_count / len(x)))
```

---

### argmax, range, indexing

```Python
print( list( range(0, 10) ) )
print( list( range(0, 10, 3) ) )

x = np.array([[0.1, 0.8, 0.1],
              [0.3, 0.1, 0.6],
              [0.2, 0.5, 0.3],
              [0.8, 0.1, 0.1]])
print( np.argmax(x, axis=1) )
print( np.argmax(x, axis=0) )

y = np.array([1, 2, 1, 0])
t = np.array([1, 2, 0, 0])
print( y == t )
print( np.sum(y == t) )
```

## 6ì£¼ì°¨ : ì‹ ê²½ë§ í•™ìŠµ & ë¯¸ë¶„

### ë°ì´í„° ì£¼ë„ í•™ìŠµ

- End-to-end learning
    - ë³µì¡í•œ í•™ìŠµì‹œìŠ¤í…œì„ í•˜ë‚˜ì˜ ì‹ ê²½ë§ìœ¼ë¡œ í‘œí˜„
    - ì‚¬ëŒì˜ ë‘ë‡Œì™€ ë¹„ìŠ·í•œ ë°©ì‹

### Splitting Data

- training set : í›ˆë ¨ìš© ë°ì´í„°
- Test set : ì„±ëŠ¥ì„ í‰ê°€í•˜ê¸° ìœ„í•œ ë°ì´í„° (ê°ê´€ì )
- ë‚˜ëˆ„ëŠ” ì´ìœ ëŠ” ë²”ìš©ì ìœ¼ë¡œ ì‚¬ìš©í•˜ê¸° ìœ„í•¨
- Overfitting : íŠ¹ì • ë°ì´í„°ì…‹ì— ì§€ë‚˜ì¹˜ê²Œ ìµœì í™”ëœ ìƒíƒœ

---

### ì†ì‹¤ í•¨ìˆ˜

- ì†ì‹¤í•¨ìˆ˜ : ì‹ ê²½ë§ì´ í•™ìŠµí•  ìˆ˜ ìˆë„ë¡ í•´ì£¼ëŠ” ì§€í‘œ, ì‘ì•„ì ¸ì•¼ í•¨
- MSE : íšŒê·€ ë¬¸ì œì—ì„œ ì†ì‹¤ í•¨ìˆ˜ë¡œ ì‚¬ìš©
    
    - ì˜ˆì¸¡ê°’ê³¼ ëª©í‘œê°’ì˜ ì°¨ì´ì˜ ì œê³±ì— ëŒ€í•œ í‰ê· 
    
    ```Python
    def mse_loss(y_hat, y):
      return np.mean( (y- y_hat)**2) # ì˜ˆì¸¡ê°’ê³¼ ëª©í‘œê°’ì˜ ì°¨ì´ì˜ ì œê³±ì— ëŒ€í•œ í‰ê· 
    ```
    
- Cross-Entropy
    
    - ë¶„ë¥˜ ë¬¸ì œì˜ ì„±ëŠ¥ì„ ì¸¡ì •
    - ì˜ˆì¸¡ì˜ ì •í™•ë„ì— ë”°ë¼ ê°’ì´ ë‹¬ë¼ì§ (ê°€ê¹Œìš¸ ìˆ˜ ë¡ loss ê°’ì´ ë‚®ì•„ì§)
    - ì •ë‹µ * ì •ë‹µ í´ë˜ìŠ¤ predictionì˜ ë¡œê·¸ê°’
    
    ```Python
    def ce_loss(y_hat,y):
      delta = 1e-7 # ë¡œê·¸ì— 0 ë“¤ì–´ê°€ëŠ” ê²½ìš°ë¥¼ ë°©ì§€í•˜ê¸° ìœ„í•´ì„œ ì‘ì€ ë¸íƒ€ë¥¼ ë”í•˜ê¸°
      return -np.sum(y*np.log(y_hat+delta)) # (ì •ë‹µ)*(ì •ë‹µ ì˜ˆì¸¡ì˜ ë¡œê·¸ê°’)
    ```
    
- one-hot encoding
    - ì •ë‹µ ì¸ë±ìŠ¤ëŠ” 1, ë‚˜ë¨¸ì§€ëŠ” 0ì¸ ì¸ì½”ë”© ë°©ì‹
    - ìœ„ì˜ ce_loss ì—ì„œ ì‚¬ìš©

### ë¯¸ë‹ˆë°°ì¹˜ í•™ìŠµ

- í›ˆë ¨ ë°ì´í„° ëª¨ë‘ì— ëŒ€í•œ ì†ì‹¤ í•¨ìˆ˜ ê°’ì„ êµ¬í•´ì•¼ í•¨
- ëª¨ë“  í›ˆë ¨ ë°ì´í„°ì— ëŒ€í•œ ì†ì‹¤ í•¨ìˆ˜ì˜ í•©ì„ êµ¬í•˜ëŠ” ë²• : ê°ê°ì˜ ì¶œë ¥ê°’ì— ce_loss ì ìš©í›„ í‰ê· ë‚´ê¸°
- í•˜ì§€ë§Œ ë°ì´í„°ê°€ ë§ìœ¼ë©´ ëª¹ì‹œ ëŠë ¤ì§„ë‹¤
- í›ˆë ¨ ë°ì´í„° ì¼ë¶€ë¥¼ ê³¨ë¼ì„œ ì „ì²´ì˜ ê·¼ì‚¬ì¹˜ë¡œ ì´ìš©
    - ì¼ë¶€ë¥¼ ë¯¸ë‹ˆë°°ì¹˜ë¼ê³  í•©ë‹ˆë‹¤.
- ë°ì´í„°ì…‹ ê°€ì ¸ì˜¤ê¸°

```Python
X = X.values.astype(np.uint8)
y = y.values.astype(np.uint8)

# one-hot encoding
y = np.eye(10)[y] # one-hot encoding ì´ ë“¤ì–´ê°„ 2ì°¨ì› ë°°ì—´ì„ ë§Œë“ ë‹¤. ì´ì œ ê°ê°ì˜ ì´ë¯¸ì§€ì— one-hot encodingì´ ì ìš©ëë„¤ìš”!

x_train = X[:60000]
x_test = X[60000:]
y_train = y[:60000]
y_test = y[60000:]

print(x_train.shape) # 60000, 784
print(y_train.shape) # 60000, 10
print(x_test.shape) # 10000, 784
print(y_test.shape) # 10000, 10

train_size = x_train.shape[0] # í›ˆë ¨í•  ë°ì´í„°ì˜ ì‚¬ì´ì¦ˆëŠ” 60000
batch_size = 10 # ë¯¸ë‹ˆë°°ì¹˜ì˜ ì‚¬ì´ì¦ˆëŠ” 10
batch_mask = np.random.choice(train_size, batch_size) \#60000ê°œ ì¤‘ì—ì„œ ë°°ì¹˜ ì‚¬ì´ì¦ˆ ë§Œí¼ ë½‘ê¸°
x_batch = x_train[batch_mask] # í›ˆë ¨ ë°ì´í„°ì—ì„œ í•´ë‹¹ ì¸ë±ìŠ¤ì˜ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì™€ ë¯¸ë‹ˆë°°ì¹˜ë¥¼ ë§Œë“ ë‹¤
y_batch = y_train[batch_mask]
```

- ë¯¸ë‹ˆë°°ì¹˜ìš© ce_loss

```Python
def ce_loss( y_hat, y ):
    if y_hat.ndim == 1:
        y = y.reshape(1, y.size)
        y_hat = y_hat.reshape(1, y_hat.size)
    
    batch_size = y_hat.shape[0]
    return - np.sum( y * np.log(y_hat + 1e-7)) / batch_size
```

### ì™œ ì†ì‹¤í•¨ìˆ˜ë¥¼ ì‚¬ìš©í• ê¹Œ?

- ë†’ì€ ì •í™•ë„ë¥¼ ìœ„í•´ ì‚¬ìš©í•©ë‹ˆë‹¤.
    - ì •í™•ë„ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì‚¬ìš©í•  ê²½ìš°, ì •í™•ë„ëŠ” ì—°ì†ì ì´ì§€ ì•Šìœ¼ë©° ê°€ì¤‘ì¹˜ì˜ ë³€í™”ì— ë”°ë¼ ì •í™•ë„ì˜ ë³€í™”ëŠ” ë¯¸ë¯¸í•œ ê²½ìš°ê°€ ë§ìŠµë‹ˆë‹¤.
    - ì†ì‹¤í•¨ìˆ˜ëŠ” ë°˜ëŒ€ë¡œ ì—°ì†ì ì´ë©°, ë¯¸ë¯¸í•œ ê°€ì¤‘ì¹˜ì˜ ë³€í™”ì—ë„ ì¦‰ê°ì ìœ¼ë¡œ ë°˜ì‘í•˜ê¸° ë•Œë¬¸ì—, ê¸°ì¤€ìœ¼ë¡œ ì‚¼ê¸° ì¢‹ìŠµë‹ˆë‹¤.

---

## ë¯¸ë¶„

- ë³€ìˆ˜ì˜ ë³€í™”ëŸ‰ì— ë”°ë¥¸ í•¨ìˆ«ê°’ì˜ ë³€í™”ëŸ‰

```Python
def num_diff(f,x): # í•¨ìˆ˜, xê°’ì„ ë°›ì•„ ë¯¸ë¶„ì„ ì§„í–‰
  h = 1e-4
  return (f(x-h)-f(x))/ h
```

- í•´ì„ì  ë¯¸ë¶„ (ì§„ì§œ ì ‘ì„ ì—ì„œì˜ ê¸°ìš¸ê¸°) â‰  ìˆ˜ì¹˜ë¯¸ë¶„ (ëŒ€ì¶© hì— ê°’ ë•Œë ¤ë„£ì€ ë¯¸ë¶„)

### ê¸°ìš¸ê¸°

- ê¸°ìš¸ê¸°ëŠ” ëª¨ë“  ë³€ìˆ˜ì˜ í¸ë¯¸ë¶„ìœ¼ë¡œ ì´ë£¨ì–´ì§„ ë²¡í„°ì…ë‹ˆë‹¤.
- ê¸°ìš¸ê¸°ë¥¼ êµ¬í•˜ëŠ” í•¨ìˆ˜

```Python
def num_grad(f, x):
    h = 1e-4
    grad = np.zeros_like(x)  # xì™€ í˜•ìƒì´ ê°™ê³  ì›ì†Œê°€ ëª¨ë‘ 0ì¸ ë°°ì—´ ìƒì„±
    
    for idx in range(x.size):

        # ê¸°ì¡´ x ê°’ì„ ë³´ê´€
        temp = x[idx]

        # f(x+h) ê³„ì‚°
        x[idx] = temp + h
        fxh1 = f(x)

        # f(x-h) ê³„ì‚°
        x[idx] = temp - h
        fxh2 = f(x)
        
        # ê·¸ë¦¬ê³  ê¸°ìš¸ê¸° ê³„ì‚°
        grad[idx] = (fxh1 - fxh2) / (2*h)

        # ë‹¤ì‹œ xë¥¼ ì›ë˜ëŒ€ë¡œ
        x[idx] = temp
        
    return grad # í•¨ìˆ˜ fì— ëŒ€í•´ì„œ xì— ëŒ€í•œ ê¸°ìš¸ê¸° ê°’ì´ ì €ì¥ëœ ë°°ì—´ì…ë‹ˆë‹¤ 
```

### Gradient Decent

- ìµœì ì˜ ê°€ì¤‘ì¹˜ë¥¼ ì°¾ê¸° ìœ„í•´ ê¸°ìš¸ê¸° ë°©í–¥ìœ¼ë¡œ ì´ë™
- Global minimum : ê°€ì¥ ì‘ì€ê°’
- Local minimum : ê·¹ì†Œê°’
- Saddle Point : ì–´ëŠ ë°©í–¥ì—ì„œ maximum, ë‹¤ë¥¸ ë°©í–¥ì—ì„œëŠ” minimumì´ ë˜ëŠ” ê°’
- Plateau : í‰í‰í•œ ê³³
    
    ![[Media/Screenshot_2023-05-05_at_3.25.37_PM 2.png|Screenshot_2023-05-05_at_3.25.37_PM 2.png]]
    
- í•™ìŠµë¥  : í•œ ë²ˆì˜ í•™ìŠµìœ¼ë¡œ ì–¼ë§ˆë‚˜ ê°±ì‹ í•˜ëŠ”ê°€
    - ë„ˆë¬´ ì‘ì€ ê²½ìš° ê°±ì‹ ë˜ì§€ ì•ŠìŒ
    - ë„ˆë¬´ í° ê²½ìš° ë°œì‚°í•¨
    - ì ë‹¹í•œ ê²½ìš° ìµœì†Œê°’ì„ ì°¾ì„ ìˆ˜ ìˆë‹¤

```Python
def gd( f, init_x, lr=0.01, step_num=100 ):
    x = init_x # x ë°°ì—´ ë°›ê¸°
    for i in range(step_num):
        grad = num_grad(f, x)
        x -= lr * grad
        plt.scatter(x[0], x[1]) # ì‚°ì ë„ í•¨ìˆ˜
    return x
```

### Hyperparamater

- ì‹ ê²½ë§ì´ í•™ìŠµí•´ ë°”ë€ŒëŠ” ê°’ì´ ì•„ë‹Œ, ì‚¬ìš©ìê°€ ì§ì ‘ ìˆ˜ì •í•˜ëŠ” ê°’
    - ë…¸ë“œì˜ ê°œìˆ˜, í•™ìŠµë¥ , ê°€ì¤‘ì¹˜ ì´ˆê¸°ê°’, ë¯¸ë‹ˆë°°ì¹˜ ì‚¬ì´ì¦ˆ ë“±
    - ê°€ì¥ ì˜ í•™ìŠµì´ ë˜ëŠ” ê°’ì„ ì°¾ì•„ì•¼ í•¨

### ì‹ ê²½ë§ì—ì„œì˜ ê¸°ìš¸ê¸°

- ê°€ì¤‘ì¹˜ì— ëŒ€í•œ ì†ì‹¤ í•¨ìˆ˜ì˜ ê¸°ìš¸ê¸° - ì†ì‹¤í•¨ìˆ˜ì˜ ìµœì†Ÿê°’ì„ ì°¾ê¸° ìœ„í•¨

```Python
class SimpleNet:
    def __init__(self):
        self.W = np.random.randn(2, 3)
    
    def predict(self, x):
        return softmax( np.dot(x, self.W) )
    
    def loss(self, x, y):
        y_hat = self.predict(x)

        loss = ce_loss(y_hat, y)
        
        return loss

    def num_grad(self, x, y):
        h = 1e-4
        grad = np.zeros_like( self.W )

        for idx, val in np.ndenumerate( self.W ):

            temp = self.W[idx]

            self.W[idx] = temp + h
            fxh1 = self.loss(x, y)

            self.W[idx] = temp - h
            fxh2 = self.loss(x, y)
            
            grad[idx] = (fxh1 - fxh2) / (2*h) # loss í•¨ìˆ˜ì˜ ê¸°ìš¸ê¸°

            self.W[idx] = temp
            
        return grad
```

---

## ì‹ ê²½ë§ í•™ìŠµì˜ ì ˆì°¨ - SGD

1. ë¯¸ë‹ˆë°°ì¹˜
    1. ë°ì´í„°ì—ì„œ ë¬´ì‘ìœ„ë¡œ ê°€ì ¸ì˜´
2. ê¸°ìš¸ê¸° ì‚°ì¶œ
    1. ê° ê°€ì¤‘ì¹˜ì˜ ê¸°ìš¸ê¸°ë¥¼ êµ¬í•¨
3. ë§¤ê°œë³€ìˆ˜ ê°±ì‹ 
    1. ê°€ì¤‘ì¹˜ë¥¼ ê¸°ìš¸ê¸° ë°©í–¥ìœ¼ë¡œ ì¡°ê¸ˆ ê°±ì‹ 
4. ë°˜ë³µ
    1. ì›í•˜ëŠ” ê²°ê³¼ê°€ ë‚˜ì˜¬ë•Œê¹Œì§€ ê°±ì‹ 

```Python
class TwoLayerNet:
    def __init__(self, i_size, h_size, o_size, init_std=0.01):
        self.params = {}
        self.params['W1'] = init_std * np.random.randn(i_size, h_size)
        self.params['b1'] = np.zeros(h_size)
        self.params['W2'] = init_std * np.random.randn(h_size, o_size)
        self.params['b2'] = np.zeros(o_size)

    def predict(self, x):
        W1, W2 = self.params['W1'], self.params['W2']
        b1, b2 = self.params['b1'], self.params['b2']
    
        a1 = np.dot(x, W1) + b1
        z1 = sigmoid(a1)
        a2 = np.dot(z1, W2) + b2
        return softmax(a2)
        
    def loss(self, x, y):
        y_hat = self.predict(x)
        return ce_loss( y_hat, y )
    
    def accuracy(self, x, y):
        y_hat = self.predict(x)
        y_hat = np.argmax(y_hat, axis=1)
        y = np.argmax(y, axis=1)
        accuracy = np.sum(y_hat == y) / float(x.shape[0])
        return accuracy
        
    def num_grad(self, x, y):
        h = 1e-4
        grads = {}

        for key in ('W1', 'b1', 'W2', 'b2'):
            w = self.params[key]
            grad = np.zeros_like( w )
        
            for idx, val in np.ndenumerate( w ):
                temp = w[idx]

                w[idx] = temp + h
                fxh1 = self.loss(x, y)
                w[idx] = temp - h
                fxh2 = self.loss(x, y)
                grad[idx] = (fxh1 - fxh2) / (2*h)

                w[idx] = temp
            
            grads[key] = grad

        return grads

...
# ê°±ì‹ 
for i in range(iters_num):
  
    # ë¯¸ë‹ˆë°°ì¹˜ íšë“
    batch_mask = np.random.choice(train_size, batch_size)
    x_batch = x_train[batch_mask]
    y_batch = y_train[batch_mask]

    # ê¸°ìš¸ê¸° ê³„ì‚°
    grad = net.num_grad(x_batch, y_batch)

    # ë§¤ê°œë³€ìˆ˜ ê°±ì‹ 
    for key in ('W1', 'b1', 'W2', 'b2'):
        net.params[key] -= learning_rate * grad[key]

    # í•™ìŠµ ê²½ê³¼ ê¸°ë¡
    loss = net.loss(x_batch, y_batch)
    train_loss_list.append(loss)
    print('loss: {}'.format(loss))
```

- epoch : í›ˆë ¨ ë°ì´í„°ë¥¼ ëª¨ë‘ ì†Œì§„í•œ íšŸìˆ˜
- ìœ„ì˜ ê²°ê³¼ë¥¼ epoch ë§Œí¼ ë°˜ë³µ

## 7ì£¼ì°¨ : ì˜¤ì°¨ì—­ì „íŒŒë²•

## ê³„ì‚° ê·¸ë˜í”„

### ê³„ì‚° ê·¸ë˜í”„

- ê·¸ë˜í”„ = ì—¬ëŸ¬ ê°œì˜ ë…¸ë“œ + ë…¸ë“œë¥¼ ì—°ê²°í•˜ëŠ” ì—ì§€
- ê³„ì‚° ê·¸ë˜í”„ : ê³„ì‚° ê³¼ì •ì„ ë…¸ë“œì™€ í™”ì‚´í‘œë¡œ í‘œí˜„, ë³µì¡í•œ ê³„ì‚°ì—ì„œ ìœ ìš©
- ìˆœì „íŒŒ : ì™¼ìª½ -> ì˜¤ë¥¸ìª½ìœ¼ë¡œ ê³„ì‚° ì§„í–‰
- ì—­ì „íŒŒ : ê³„ì‚° ê·¸ë˜í”„ì˜ ì¢…ì°©ì ì—ì„œ ì¶œë°œì ìœ¼ë¡œ ì „íŒŒ (ë¯¸ë¶„ê³„ì‚°)
    
    ![[Media/Screenshot_2023-05-05_at_3.55.22_PM 2.png|Screenshot_2023-05-05_at_3.55.22_PM 2.png]]
    

---

## ì—°ì‡„ë²•ì¹™

### í•©ì„±í•¨ìˆ˜

- ì—¬ëŸ¬ í•¨ìˆ˜ì˜ ì¤‘ì²©ìœ¼ë¡œ êµ¬ì„±ëœ í•¨ìˆ˜
- ê³„ì‚° ê·¸ë˜í”„ëŠ” ì¼ì¢…ì˜ í•©ì„±í•¨ìˆ˜ë¡œ ìƒê°í•  ìˆ˜ ìˆë‹¤
- í•¨ì„± í•¨ìˆ˜ì˜ ë¯¸ë¶„ì€ í•©ì„± í•¨ìˆ˜ë¥¼ êµ¬ì„±í•˜ëŠ” ê° í•¨ìˆ˜ì˜ ë¯¸ë¶„ì˜ ê³± (chain rule)
    
    ![[Media/Screenshot_2023-05-05_at_3.57.45_PM 2.png|Screenshot_2023-05-05_at_3.57.45_PM 2.png]]
    

---

## ì—­ì „íŒŒ

- ë§ì…ˆ ë…¸ë“œì˜ ì—­ì „íŒŒëŠ” í•˜ë¥˜ì˜ ê°’ì„ ê·¸ëŒ€ë¡œ ì˜¬ë ¤ë³´ë‚¸ë‹¤. (z = x+yì˜ ê²½ìš°)
- ê³±ì…ˆ ë…¸ë“œì˜ ì—­ì „íŒŒëŠ” í•˜ë¥˜ì˜ ê°’ì— ìˆœì „íŒŒì˜ ì…ë ¥ì‹ í˜¸ë¥¼ ë°”ê¾¼ ê²ƒì„ ê³±í•´ì„œ ì˜¬ë¼ê°„ë‹¤. -> ì…ë ¥ ì‹ í˜¸ë¥¼ ì €ì¥í•´ ë‘ì–´ì•¼ í•¨
    
    ![[Media/Screenshot_2023-05-05_at_3.59.28_PM 2.png|Screenshot_2023-05-05_at_3.59.28_PM 2.png]]
    

---

## ë‹¨ìˆœí•œ ê³„ì¸µ

### ë‹¨ìˆœ ì—°ì‚° ê³„ì¸µ

- ê³±ì…ˆ, ë§ì…ˆê³¼ ê°™ì€ ë‹¨ìˆœ ë…¸ë“œì˜ ì—­ì „íŒŒë¥¼ í…ŒìŠ¤íŠ¸
- ì‹ ê²½ë§ì„ êµ¬ì„±í•˜ëŠ” layer ë‹¨ìœ„ì˜ í´ë˜ìŠ¤ë¡œ êµ¬í˜„
    - multilayer, addlayer
    - forward(), backward()

---

### **ë‹¨ìˆœí•œ ê³„ì¸µ êµ¬í˜„ - ê³±ì…ˆ ë…¸ë“œ**

```Python
class MultLayer:
    def __init__(self):
      self.x = None
      self.y = None

    def forward(self,x,y): # ìˆœì „íŒŒì˜ ê°’ ì €ì¥
      self.x = x
      self.y = y

      return x*y # ê³±ì…ˆ ë…¸ë“œêµ°

    def backward(self, dout): # ì—­ì „íŒŒ ê°’ ì €ì¥ = í¸ë¯¸ë¶„ê°’
      dx = dout * self.y # ë°”ë¡œ ì•ì˜ ë¯¸ë¶„ê°’ x ë‹¤ë¥¸ ë…¸ë“œì˜ ì…ë ¥ê°’
      dy = dout * self.x

      return dx,dy
```

### ë‹¨ìˆœí•œ ê³„ì¸µ êµ¬í˜„ - ë§ì…ˆ ë…¸ë“œ

```Python
class AddLayer:
    def forward(self,x,y):
      return x+y

    def backward(self,dout):
      return dout,dout
```

### í™œì„±í™” í•¨ìˆ˜ ê³„ì¸µ

- ìˆœì „íŒŒ ì…ë ¥ì´ 0ë³´ë‹¤ í¬ë©´ ì—­ì „íŒŒëŠ” ìƒë¥˜ì˜ ê°’ì„ ê·¸ëŒ€ë¡œ í•˜ë¥˜ë¡œ í˜ë¦¼
- 0ë³´ë‹¤ ì‘ë‹¤ë©´ ì—­ì „íŒŒëŠ” í•˜ë¥˜ë¡œ ì‹ í˜¸ë¥¼ ë³´ë‚´ì§€ ì•ŠìŒ

```Python
class ReLU:
    def __init__(self):
        self.mask = None
    
    def forward(self, x):
        self.mask = (x <= 0)
        out = x.copy()
        out[self.mask] = 0

        return out
    
    def backward(self, dout):
        dout[self.mask] = 0
        dx = dout
        
        return dx
```

### ì‹œê·¸ëª¨ì´ë“œ ê³„ì¸µ

```Python
class Sigmoid:
    def __init__(self):
      self.out = None

    def forward(self,x):
      out = 1 / (1 + np.exp(-x))
      self.out = out1
      return out

    def backward(self,dout):
      dx = dout * (1.0 - self.out) * self.out # ë†€ëê²Œë„ ì‹œê·¸ëª¨ì´ë“œ í¸ë¯¸ë¶„ê°’ì´ë‹¤

      return dx
```

### Affine / softmax ê³„ì¸µ

- affine ê³„ì¸µ - í–‰ë ¬ ê³±

```Python
class Affine:
    def __init__(self, W, b):
        self.W = W
        self.b = b
        self.x = None
        self.dW = None
        self.db = None
    
    def forward(self, x):
        self.x = x
        out = np.dot(x, self.W) + self.b # ì—¬ê¸°ì„œëŠ” ê¸°ì¡´ê³¼ ë‹¤ë¥´ê²Œ bê°€ ê° ë…¸ë“œì— ì •í•´ì ¸ ìˆëŠ” ê²ƒì´ ì•„ë‹Œ, í•œêº¼ë²ˆì— ë”í•´ì§‘ë‹ˆë‹¤
        
        return out
    
    def backward(self, dout):
        dx = np.dot(dout, self.W.T) # í–‰ë ¬ ê³± ê°™ì€ ê²½ìš°ëŠ” ì´ì „ì˜ ë¯¸ë¶„ ê°’ì— ë‹¤ë¥¸ ì…ë ¥ í–‰ë ¬ì˜ transposeë¥¼ ê³±í•œ ê°’ì´ë€ë‹¤!
        self.dW = np.dot(self.x.T, dout)
        self.db = np.sum(dout, axis=0) # ì™œ ë”í•˜ë‚˜ìš”? -> ì…ë ¥ì¸µì˜ ì‚¬ì´ì¦ˆê°€ Nì¼ ê²½ìš°ì—ëŠ” ë¯¸ë¶„ê°’ì´ (N,x) ì˜ ì‚¬ì´ì¦ˆë¥¼ ê°€ì§€ì§€ë§Œ, í¸í–¥ì€ ì¼ì°¨ì› ë°°ì—´ì´ë‹¤ -> sumìœ¼ë¡œ ì¼ì°¨ì›ìœ¼ë¡œ ë§Œë“¦
        
        return dx
```

- softmax

```Python
class SoftmaxWithLoss:
    def __init__(self):
        self.loss = None
        self.o = None
        self.t = None
        
    def forward(self, x, t):
        self.t = t
        self.o = softmax(x)
        self.loss = ce_loss(self.o, self.t)
    
        return self.loss
    
    def backward(self, dout=1):
        batch_size = self.t.shape[0]
        dx = (self.o - self.t) / batch_size
        
        return dx
```

---

## ì˜¤ì°¨ì—­ì „íŒŒë²•

```Python
class TwoLayerNet:
    def __init__(self, i_size, h_size, o_size, init_std = 0.01):
        self.params = {}
        self.params['W1'] = init_std * np.random.randn(i_size, h_size)
        self.params['b1'] = np.zeros(h_size)
        self.params['W2'] = init_std * np.random.randn(h_size, o_size) 
        self.params['b2'] = np.zeros(o_size)

        self.layers = {} # ë ˆì´ì–´ ë”•ì…”ë„ˆë¦¬ ë§Œë“¤ê¸°
        self.layers['Affine1'] = Affine(self.params['W1'], self.params['b1'])
        self.layers['Relu1'] = ReLU()
        self.layers['Affine2'] = Affine(self.params['W2'], self.params['b2'])
        self.layers['SoftmaxWithLoss'] = SoftmaxWithLoss()
        
    def predict(self, x):
        for key in ('Affine1', 'Relu1', 'Affine2'): # ìˆœì „íŒŒë¡œ ì„¸ ì¸µì„ í†µê³¼ì‹œí‚¤ê¸°
            x = self.layers[key].forward(x)
        
        return x
        
    def loss(self, x, t):
        o = self.predict(x) # ì„¸ ì¸µì„ í†µê³¼í•œ ê°’ì„ ê°€ì§€ê³ 
        return self.layers['SoftmaxWithLoss'].forward(o, t) # ì˜¤ì°¨ê°’ êµ¬í•˜ê¸°
    
    def accuracy(self, x, t):
        o = self.predict(x)
        o = np.argmax(o, axis=1) # ê°€ì¥ ë†’ì€ ê°’ = ì •ë‹µì˜ˆì¸¡ì„ ì§„ì§œ ì •ë‹µê³¼ ë¹„êµí•˜ê¸°
        if t.ndim != 1: # ì •ë‹µì´ ì¼ì°¨ì› ë°°ì—´ì´ ì•„ë‹ ê²½ìš°
            t = np.argmax(t, axis=1) # ì œì¼ ë†’ì€ ê°’ì„ ì •ë‹µìœ¼ë¡œ ì •í•˜ê¸°
        
        accuracy = np.sum(o == t) / float(x.shape[0])
        return accuracy
        
    def gradient(self, x, t): # ì—­ì „íŒŒë²•ìœ¼ë¡œ ë¯¸ë¶„ê°’ êµ¬í•˜ê¸°
        
        # forward
        self.loss(x, t)

        # backward
        dout = 1
        for key in ('SoftmaxWithLoss', 'Affine2', 'Relu1', 'Affine1'):
            dout = self.layers[key].backward(dout)

        grads = {}
        grads['W1'] = self.layers['Affine1'].dW
        grads['b1'] = self.layers['Affine1'].db
        grads['W2'] = self.layers['Affine2'].dW
        grads['b2'] = self.layers['Affine2'].db
        
        return grads
```

## 8ì£¼ì°¨ : ìµœì í™”

## ìµœì í™”

### í™•ë¥ ì  ê²½ì‚¬ í•˜ê°•ë²•(SGD)

- ê°€ì¤‘ì¹˜ì— ëŒ€í•œ ê¸°ìš¸ê¸°ë¥¼ êµ¬í•´ ê°€ì¤‘ì¹˜ë¥¼ ê¸°ìš¸ê¸° ë°©í–¥ìœ¼ë¡œ ì¡°ê¸ˆì”© ë³€í™”ì‹œí‚¤ëŠ” ë°©ë²•
- ë¬¸ì œì— ë”°ë¼ì„œ ë¹„íš¨ìœ¨ì ì¸ ë•Œê°€ ìˆìŒ - ë¹„ë“±ë°©ì„± í•¨ìˆ˜ì—ì„œ íƒìƒ‰ ê²½ë¡œê°€ ë¹„íš¨ìœ¨ì 

```Python
class SGD:
    def __init__(self, lr=0.01):
        self.lr = lr
        
    def update(self, params, grads):
        for key in params.keys():
            params[key] -= self.lr * grads[key]
```

### ëª¨ë©˜í…€

- ê´€ì„±ì„ ì´ìš©í•˜ëŠ” ë°©ë²•
    
    ![[Media/Screenshot_2023-05-05_at_8.24.38_PM 2.png|Screenshot_2023-05-05_at_8.24.38_PM 2.png]]
    
- VëŠ” ì†ë„, ì•ŒíŒŒëŠ” ì†ë„ê°€ ì°¨ì¸° ê°ì†Œë˜ë„ë¡ í•˜ëŠ” ì—­í•  (0.9)

```Python
class Momentum:
    def __init__(self, lr=0.01, momentum=0.9):
        self.lr = lr
        self.momentum = momentum
        self.v = None
        
    def update(self, param, grad):
        if self.v is None:
            self.v = {}
            for key, val in params.items():
                self.v[key] = np.zeros_like(val)

        for key in params.keys():
            self.v[key] = self.momentum * self.v[key] - self.lr * grads[key]
            params[key] += self.v[key]
```

### AdaGrad

- í•™ìŠµë¥ ì„ ì¡°ì •í•˜ëŠ” í•™ìŠµë²•
- ê°ê°ì˜ ë§¤ê°œë³€ìˆ˜ì˜ ì›ì†Œë³„ë¡œ ì ì‘ì ì¸ í•™ìŠµë¥ ì„ ê°–ë„ë¡ í•©ë‹ˆë‹¤
    
    ![[Media/Screenshot_2023-05-05_at_8.26.42_PM 2.png|Screenshot_2023-05-05_at_8.26.42_PM 2.png]]
    
- hëŠ” ê¸°ì¡´ ê¸°ìš¸ê¸°ê°’ì„ ì œê³±í•˜ì—¬ ê³„ì† ë”í•´ì¤Œ
- ì € ê¸°í˜¸ëŠ” í–‰ë ¬ì˜ ì›ì†Œë³„ ê³±ì„ ë§í•¨
- ê¸°ìš¸ê¸° ë£¨íŠ¸ì˜ ì—­ìˆ˜ë¥¼ ê³±í•´ì„œ í•™ìŠµë¥ ì˜ ì •ë„ë¥¼ ì—…ë°ì´íŠ¸
- Adam = AdaGrad + momentum

```Python
class AdaGrad:

    def __init__(self, lr=0.01):
        self.lr = lr
        self.h = None
        
    def update(self, params, grads):
        if self.h is None:
            self.h = {}
            for key, val in params.items():
                self.h[key] = np.zeros_like(val)
            
        for key in params.keys():
            self.h[key] += grads[key] * grads[key]
            params[key] -= self.lr * grads[key] / (np.sqrt(self.h[key]) + 1e-7) # 0ìœ¼ë¡œ ë‚˜ëˆ„ê¸° ë°©ì§€
```

---

## ê°€ì¤‘ì¹˜ì˜ ì´ˆê¸°ê°’

- ê°€ì¤‘ì¹˜ê°€ í¬ë©´ ì˜¤ë²„í”¼íŒ…ì´ ì¼ì–´ë‚˜ëŠ” ê²½ìš°ê°€ ë§ìŒ
    - ê°€ëŠ¥í•œ ì‘ì€ ê°’ìœ¼ë¡œ ì‹œì‘í•˜ì
- ê°€ì¤‘ì¹˜ë¥¼ ëª¨ë‘ ê°™ê±°ë‚˜ ëª¨ë‘ 0ìœ¼ë¡œ ì„¤ì •í•˜ì§€ ì•ŠëŠ” ì´ìœ 

### ì€ë‹‰ì¸µì˜ í™œì„±í™”ê°’ ë¶„í¬

- ì€ë‹‰ì¸µì—ì„œì˜ í™œì„±í™”ê°’ ë¶„í¬ë¥¼ ì•Œê¸° ìœ„í•œ ì‹¤í—˜ = 5ì¸µ ì‹ ê²½ë§ì„ êµ¬í˜„í•´ë³´ì
- ê°€ì¤‘ì¹˜ì˜ í‘œì¤€í¸ì°¨ê°€1 ì¼ ë–„ ê°ì¸µì˜ í™œì„±í™”ê°’ë“¤ì€ 0ê³¼ 1ì— ì¹˜ìš°ì³ì„œ ë¶„í¬ â†’ ë¯¸ë¶„ê°’ êµ¬í•  ë•Œ ë¬¸ì œ

### ê¸°ìš¸ê¸° ì†Œì‹¤

- ì—­ì „íŒŒì˜ ê¸°ìš¸ê¸° ê°’ì´ ì ì  ì‘ì•„ì§€ë‹¤ê°€ ì‚¬ë¼ì§€ëŠ” ê²ƒ
    - ì¸µì„ ê¹Šê²Œ í•  ìˆ˜ë¡ ì‹¬ê°í•œ ë¬¸ì œê°€ ëœë‹¤
- ê·¸ë˜ì„œ í‘œì¤€í¸ì°¨ë¥¼ ì‘ê²Œ ë†“ìœ¼ë©´? â†’ ëª¨ë“  ë‰´ëŸ°ì˜ ì¶œë ¥ì´ ì¤‘ê°„ì— ì¹˜ìš°ì¹¨

### Xavier ì´ˆê¸°ê°’

- ì• ê³„ì¸µì˜ ë…¸ë“œê°€ nìƒˆë¼ë©´ í‘œì¤€í¸ì°¨ê°€ $1/ \sqrt{n}$ï»¿ì¸ ì •ê·œë¶„í¬ë¡œ í¼ëœ¨ë¦¬ëŠ” ê²ƒ

### tanh()ì˜ ì‚¬ìš©

- í™œì„±í™” í•¨ìˆ˜ë¡œ ì‚¬ìš©í•˜ë©´ ì‹œê·¸ëª¨ì´ë“œë³´ë‹¤ ì¢€ë” ê´œì°®ì•„ì§„ ë¶„í¬ë¥¼ ë³´ì„

### He ì´ˆê¹ƒê°’

- ReLUì— íŠ¹í™”ëœ ì´ˆê¹ƒê°’ìœ¼ë¡œ, ReLUëŠ” ìŒì˜ ì˜ì—­ì´ ì—†ëŠ” ì…ˆ(0)ì´ë¯€ë¡œ ë¶„í¬ê°€ ë‘ë°°ëŠ” ë„“ì–´ì•¼ í•¨
- í‘œì¤€í¸ì°¨ê°€ $2/ \sqrt{n}$ï»¿ë°°

---

## ë°°ì¹˜ ì •ê·œí™”

- ë°ì´í„° ë¶„í¬ë¥¼ ì •ê·œí™”í•˜ëŠ” ê³¼ì •
- í•™ìŠµ ì‹œ ë¯¸ë‹ˆë°°ì¹˜ë¥¼ ë‹¨ìœ„ë¡œ ì •ê·œí™”í•¨ = ë°ì´í„° ë¶„í¬ê°€ í‰ê· ì´ 0, ë¶„ì‚°ì´ 1ì´ ë˜ë„ë¡ ì •ê·œí™”

![[Media/Screenshot_2023-05-05_at_8.41.31_PM 2.png|Screenshot_2023-05-05_at_8.41.31_PM 2.png]]

- ì•„ë¬´íŠ¼ ì‚¬ìš©í–ˆì„ ë•Œ í•™ìŠµì´ ì˜ ëœë‹¤

```Python
class BatchNormalization:

    def __init__(self, gamma, beta, momentum=0.9, running_mean=None, running_var=None):
        self.gamma = gamma
        self.beta = beta
        self.momentum = momentum
        self.input_shape = None 

        self.running_mean = running_mean
        self.running_var = running_var  
        
        self.batch_size = None
        self.xc = None
        self.std = None
        self.dgamma = None
        self.dbeta = None

    def forward(self, x, train_flg=True):
        self.input_shape = x.shape
        if x.ndim != 2:
            N, C, H, W = x.shape
            x = x.reshape(N, -1)

        out = self.__forward(x, train_flg)
        
        return out.reshape(*self.input_shape)
            
    def __forward(self, x, train_flg):
        if self.running_mean is None:
            N, D = x.shape
            self.running_mean = np.zeros(D)
            self.running_var = np.zeros(D)
                        
        if train_flg:
            mu = x.mean(axis=0)
            xc = x - mu
            var = np.mean(xc**2, axis=0)
            std = np.sqrt(var + 10e-7)
            xn = xc / std
            
            self.batch_size = x.shape[0]
            self.xc = xc
            self.xn = xn
            self.std = std
            self.running_mean = self.momentum * self.running_mean + (1-self.momentum) * mu
            self.running_var = self.momentum * self.running_var + (1-self.momentum) * var            
        else:
            xc = x - self.running_mean
            xn = xc / ((np.sqrt(self.running_var + 10e-7)))
            
        out = self.gamma * xn + self.beta 
        return out

    def backward(self, dout):
        if dout.ndim != 2:
            N, C, H, W = dout.shape
            dout = dout.reshape(N, -1)

        dx = self.__backward(dout)

        dx = dx.reshape(*self.input_shape)
        return dx

    def __backward(self, dout):
        dbeta = dout.sum(axis=0)
        dgamma = np.sum(self.xn * dout, axis=0)
        dxn = self.gamma * dout
        dxc = dxn / self.std
        dstd = -np.sum((dxn * self.xc) / (self.std * self.std), axis=0)
        dvar = 0.5 * dstd / self.std
        dxc += (2.0 / self.batch_size) * self.xc * dvar
        dmu = np.sum(dxc, axis=0)
        dx = dxc - dmu / self.batch_size
        
        self.dgamma = dgamma
        self.dbeta = dbeta
        
        return dx
```

---

## ë°”ë¥¸ í•™ìŠµì„ ìœ„í•´

### ì˜¤ë²„í”¼íŒ…

- ì‹ ê²½ë§ì´ íŠ¹ì • ë°ì´í„°ì…‹ì— ëŒ€í•´ì„œ ì¹˜ìš°ì¹˜ê²Œ í•™ìŠµëœ ê²½ìš°
    - ê°€ì¤‘ì¹˜ê°€ ë§ê³  í‘œí˜„ë ¥ì´ ë–¨ì–´ì§€ëŠ” ëª¨ë¸
    - í›ˆë ¨ ë°ì´í„°ê°€ ì ìŒ

### ê°€ì¤‘ì¹˜ ê°ì†Œ

- ì˜¤ë²„í”¼íŒ…ì„ ì–µì œí•˜ê¸° ìœ„í•´ í° ê°€ì¤‘ì¹˜ì— ëŒ€í•´ ìƒì‘í•˜ëŠ” í° íŒ¨ë„í‹°ë¥¼ ë¶€ê³¼í•˜ëŠ” ë°©ë²•
    - ê°€ì¤‘ì¹˜ì˜ ê° ì›ì†Œì˜ ì œê³±ë“¤ì„ ë”í•œ ê²ƒ(L2 norm)ì„ ì†ì‹¤í•¨ìˆ˜ì— ë”í•´ ê°€ì¤‘ì¹˜ê°€ ì»¤ì§€ëŠ” ê²ƒì„ ì–µì œí•œë‹¤
    - $L = \gamma W$ï»¿

### ë“œë¡­ì•„ì›ƒ

- ë‰´ëŸ°ì„ ì„ì˜ì ìœ¼ë¡œ ì‚­ì œí•˜ë©´ì„œ í•™ìŠµí•˜ëŠ” ë°©ë²•
- í›ˆë ¨ ë•ŒëŠ” ì€ë‹‰ì¸µì˜ ë‰´ëŸ°ì„ ë¬´ì‘ìœ„ë¡œ ê³¨ë¼ ì‚­ì œ
- ì˜ˆì¸¡ ë•ŒëŠ” ëª¨ë“  ë‰´ëŸ­ì„ ì‚¬ìš©í•˜ë˜ ì¶œë ¥ì— ì‚­ì œ ë¹„ìœ¨ì„ ê³±í•´ì„œ ì¶œë ¥

---

## í•˜ì´í¼íŒŒë¼ë¯¸í„°

### ê²€ì¦ ë°ì´í„°

- ì§€ê¸ˆê¹Œì§€ëŠ” ë°ì´í„°ì…‹ì„ test set, train setìœ¼ë¡œ ë‚˜ëˆ„ì–´ì„œ test setì— ëŒ€í•´ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì¡°ì •í–ˆì§€ë§Œ, ê·¸ëŸ¬ë©´ ì˜¤ë²„í”¼íŒ…ë  ê°€ëŠ¥ì„±ì´ ë†’ìŒ
- í•˜ì´í¼íŒŒë¼ë¯¸í„° ì „ìš© ë°ì´í„° - Validition set

### í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”

- ìš°ì„  ë²”ìœ„ë¥¼ ëŒ€ëµì ìœ¼ë¡œ ì§€ì •í•˜ê³ , ë¬´ì‘ìœ„ë¡œ ê°’ì„ ê³¨ë¼ë‚¸ í›„ ì •í™•ë„ë¥¼ í‰ê°€

1. íŒŒë¼ë¯¸í„°ì˜ ê°’ì˜ ë²”ìœ„ë¥¼ ì„¤ì •
2. ì„¤ì •ëœ ë²”ìœ„ì—ì„œ ë¬´ì‘ìœ„ë¡œ ì¶”ì¶œ
3. í•˜ì´í¼íŒŒë¼ë¯¸í„° ê°’ì„ ì‚¬ìš©í•´ ê²€ì¦ ë°ì´í„°ë¡œ í‰ê°€
4. ì •í™•ë„ì˜ ê²°ê³¼ë¡œ ë²”ìœ„ë¥¼ ì ì°¨ ì¢í˜€ë‚˜ê°

## 9ì£¼ì°¨ : pyTorch

## pyTorch

- python ê¸°ë°˜ ê³¼í•™ ê³„ì‚° íŒ¨í‚¤ì§€, GPU ê³„ì‚°ì„ ì§€ì›í•©ë‹ˆë‹¤.

### Tensor

- ndarrayì™€ ë¹„ìŠ·í•©ë‹ˆë‹¤

```Python
x = torch.empty(3,2)

x = torch.rand(3,2)

x = torch.zeros(3,2, dtype=torch.int) # or float

a = torch.ones(5)
print(a)
b = a.numpy()
print(b)
a.add_(1)
print(a)
print(b)
# numpyì™€ tensorì˜ ë©”ëª¨ë¦¬ ê³µìœ 

a = np.ones(5)
b = torch.from_numpy(a)
np.add(a, 1, out=a)
print(a) # [2. 2. 2. 2. 2.]
print(b) # tensor([2. 2. 2. 2. 2.])

# reshapeì€ viewí•¨ìˆ˜ë¡œ í•©ë‹ˆë‹¤
x = torch.randn(4,4)
print(x.size()) 
y = x.view(16)
print(y.size())
z = x.view(-1,8)
print(z.size())
k = x.view(1,4,4)
print(k.size())

# GPUë¡œ ê³„ì‚°í•˜ëŠ” ë²•
if torch.cuda.is_available():
  device = torch.device('cuda')
else:
  device - torch.device('cpu')

# ë‹¤ì–‘í•œ í•¨ìˆ˜
print(a.mean())
print(a.sum())
print(a.argmax(dim=1))
```

---

## Autograd

- ìë™ê¸°ìš¸ê¸° ê³„ì‚° ê¸°ëŠ¥ - ìë™ìœ¼ë¡œ ì´ì „ ê³„ì‚° ê²°ê³¼ë¥¼ ì €ì¥í•©ë‹ˆë‹¤.

```Python
x = torch.tensor(2.,requires_grad=True) # íŠ¸ë˜í‚¹ í™œì„±í™”
print(x)

out = x*5
print(out)

out.backward() # ê¸°ìš¸ê¸° ê³„ì‚°
print(x.grad) # gradì— ê¸°ìš¸ê¸°ê°€ ì €ì¥ë¨

#íŠ¸ë˜í‚¹ ì •ì§€ ì‹œí‚¤ê¸° = requires grad = Falseë¡œ ë§Œë“¤ê¸°
with torch.no_grad(): # ì´ êµ¬ê°„ì—ì„œëŠ” íŠ¸ë˜í‚¹ì„ í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤
  print((x*2).requires_grad)
```

- ì´ë¥¼ ì´ìš©í•´ ë„¤íŠ¸ì›Œí¬ë¥¼ ë§Œë“¤ì–´ ë´…ì‹œë‹¤

```Python
import torch

dtype = torch.float
device = torch.device("cpu")
# device = torch.device("cuda:0") # GPU

# N: ë°°ì¹˜ì‚¬ì´ì¦ˆ; D_in: ì…ë ¥ ì‚¬ì´ì¦ˆ; H: ì€ë‹‰ì¸µ ì‚¬ì´ì¦ˆ; D_out: ì¶œë ¥ ì‚¬ì´ì¦ˆ
N, D_in, H, D_out = 64, 1000, 100, 10

# ì…ë ¥ê³¼ ì¶œë ¥ ìœ„í•œ ëœë¤ í…ì„œ
x = torch.randn(N, D_in).to(device)
y = torch.randn(N, D_out).to(device)

# ê°€ì¤‘ì¹˜ í…ì„œ
# ì—­ì „íŒŒ ë•Œ ê¸°ìš¸ê¸° ê³„ì‚°ì„ ìœ„í•´ ê°€ì¤‘ì¹˜ í…ì„œì—ëŠ” requires_grad=False ì¶”ê°€
w1 = torch.randn(D_in, H, requires_grad=True).to(device)
w2 = torch.randn(H, D_out, requires_grad=True).to(device)

learning_rate = 1e-6

for t in range(500):
  y_pred = x.mm(w1).clamp(min=0).mm(w2) # ReLUí•¨ìˆ˜ë¥¼ ì¼ë‚˜ë´ìš”

	# loss í•¨ìˆ˜ëŠ” MSEë„¤ìš”
  loss = (y_pred - y).pow(2).sum()

  if t % 100 == 99:
    print(t, loss.item()) # ë‹¨ì¼ ê°’ì„ ì½ì–´ì˜¤ëŠ” .item()

	# ì—°ì‚°ì— ì´ìš©ëœ ëª¨ë“  ë³€ìˆ˜ì˜ ê¸°ìš¸ê¸° êµ¬í•˜ê¸°
  loss.backward()
	
	# SGD
  with torch.no_grad(): \#False ì•ˆí•˜ë©´ ê¸°ìš¸ê¸°ë¡œ ê°€ì¤‘ì¹˜ë¥¼ ì—…ë°ì´íŠ¸ í•´ì•¼í•˜ëŠ”ë° gradê°€ ì—…ë°ì´íŠ¸ ë˜ì–´ë²„ë¦¼
    w1 -= learning_rate * w1.grad
    w2 -= learning_rate * w2.grad

  w1.grad.zero_()
  w2.grad.zero_()
```

---

## NN Package

- ì‹ ê²½ë§ì„ layer ë‹¨ìœ„ë¡œ í‘œí˜„í•˜ëŠ” high-level ë°©ì‹ ì œê³µ
    
    - Modules : layer ë‹¨ìœ„ë¡œ tensor ì…ë ¥ í›„ tensor ì¶œë ¥
    - loss Functions : ì†ì‹¤í•¨ìˆ˜ë“¤
    
    ```Python
    # nn packageë¥¼ ì´ìš©í•˜ì—¬ ì—¬ëŸ¬ ì¸µìœ¼ë¡œ ì •ì˜ëœ ëª¨ë¸ ìƒì„±
    # nn.Sequentialì€ ë‹¤ë¥¸ ëª¨ë“ˆì„ ë‹´ì„ ìˆ˜ ìˆëŠ” ëª¨ë“ˆì´ë©° ë‹´ê²¨ì§„ ëª¨ë“ˆì€ ìˆœì„œëŒ€ë¡œ ì—°ê²°
    # Linear ëª¨ë“ˆì€ ê³§ Affine ëª¨ë“ˆ
    model = torch.nn.Sequential(
        torch.nn.Linear(D_in, H),
        torch.nn.ReLU(),
        torch.nn.Linear(H,H),
        torch.nn.ReLU(),
        torch.nn.Linear(H,D_out),
    )
    
    loss_fn = torch.nn.MSELoss() # ì†ì‹¤í•¨ìˆ˜ëŠ” MSE ì‚¬ìš©
    
    learning_rate = 1e-2
    
    for t in range(500):
    
      y_pred = model(x)
    
      loss = loss_fn(y_pred,y)
    
      if t%100 == 99:
        print(t,loss.item())
    
      model.zero_grad()
      loss.backward()
    
      with torch.no_grad():
        for param in model.parameters():
          param -= learning_rate * param.grad
    ```
    

### torch.optim

- ë‹¤ì–‘í•œ ìµœì í™” ì•Œê³ ë¦¬ì¦˜ - Adam, AdaGradâ€¦

```undefined
learning_rate = 1e-4

optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)

for t in range(500):

  y_pred = model(x)

  loss = loss_fn(y_pred,y)

  if t%100 == 99:
    print(t,loss.item())

  model.zero_grad()
  loss.backward()

  optimizer.step()
```

### Custom nn Modules

- ìƒì†ë°›ì•„ì„œ ë” ë³µì¡ë€ ì»¤ìŠ¤í…€ ëª¨ë“ˆ ë§Œë“¤ê¸°

```Python
# 2ì¸µ ì‹ ê²½ë§
# torch.nn.Moduleì„ ìƒì†ë°›ì•„ TwoLayerNet í´ë˜ìŠ¤ ì •ì˜
class TwoLayerNet(torch.nn.Module):
  def __init__(self, D_in, H, D_out):
    super().__init__()
    self.fc1 = torch.nn.Linear(D_in,H)
    self.fc2 = torch.nn.Linear(H,D_out)
  
  def forward(self,x): 
    x = torch.nn. functional.relu(self.fc1(x))
    return self.fc2(x)
```

---

## MNIST

### í™”ì´íŒ…

## 10ì£¼ì°¨ : í•©ì„±ê³± ì‹ ê²½ë§(CNN)

## ì „ì²´ êµ¬ì¡°

- ì§€ê¸ˆê¹Œì§€ì˜ ê³„ì¸µ - ì™„ì „ì—°ê²° ê³„ì¸µ (FC) - ëª¨ë“  ë‰´ëŸ°ì´ ì—°ê²°ë˜ì–´ ìˆìŒ
    
    ![[Media/Screenshot_2023-05-05_at_10.21.04_PM 2.png|Screenshot_2023-05-05_at_10.21.04_PM 2.png]]
    

---

## í•©ì„±ê³± ì‹ ê²½ë§

- CNN ê³„ì¸µì—ì„œ Conv - ReLU - (Pooling)
- ì¶œë ¥ì¸µì— ê°€ê¹Œì›Œ ì§ˆ ë•Œ FC - ReLU
- ë§ˆì§€ë§‰ ì¶œë ¥ ê³„ì¸µì—ì„œ FC - Softmax
    
    ![[Media/Screenshot_2023-05-05_at_10.25.13_PM 2.png|Screenshot_2023-05-05_at_10.25.13_PM 2.png]]
    

### ì™„ì „ì—°ê²° ê³„ì¸µì˜ ë¬¸ì œì (FC)

- ì…ë ¥ ë°ì´í„°ê°€ 1ì°¨ì› ë°°ì—´ í˜•íƒœë¡œ í•œì •ë¨
    - íŠ¹íˆë‚˜ nì°¨ì› ë°ì´í„°ë¥¼ 1ì°¨ì› ë°ì´í„°ë¡œ ë°”ê¾¸ëŠ” ê³¼ì •ì—ì„œ ì •ë³´ê°€ ì†ì‹¤ë¨
        - í•™ìŠµì˜ íš¨ìœ¨ì´ ë–¨ì–´ì§
        - ì •í™•ë„ë¥¼ ë†’ì´ëŠ” ë° í•œê³„ê°€ ìˆìŒ
- í•©ì„±ê³± ê³„ì¸µì€ í˜•ìƒì„ ìœ ì§€
    - ë“¤ì–´ì˜¤ëŠ” ë°ì´í„°ë„ nì°¨ì›
- í•©ì„±ê³± ê³„ì¸µì˜ ì…ì¶œë ¥ ë°ì´í„° - íŠ¹ì§•ë§µ
    - ì…ë ¥ íŠ¹ì§• ë§µ
    - ì¶œë ¥ íŠ¹ì§• ë§µ

### í•©ì„±ê³± ê³„ì¸µ

- ì…ì¶œë ¥ ë°ì´í„°ì˜ í˜•ìƒ ìœ ì§€
- ì´ë¯¸ì§€ì˜ íŠ¹ì§•ì„ íš¨ê³¼ì ìœ¼ë¡œ ìœ ì§€

- ì—¬ëŸ¬ê°œì˜ í•„í„°ë¡œ íŠ¹ì§• ì¶”ì¶œ ë° í•™ìŠµ â† ?

- í•„í„°ë¥¼ íŒŒë¼ë¯¸í„°ë¡œ ê³µìœ í•˜ê¸° ë–„ë¬¸ì— í•™ìŠµ íŒŒë¼ë¯¸í„° ê°¯ìˆ˜ê°€ ì ìŒ
- ì´ë¯¸ì§€ í•„í„° ì—°ì‚°ì— ì‚¬ìš©
    
    ![[Media/Screenshot_2023-05-05_at_10.32.59_PM 2.png|Screenshot_2023-05-05_at_10.32.59_PM 2.png]]
    
- ìœ„ì²˜ëŸ¼ ìœˆë„ìš°ë¥¼ ì¼ì • ê°„ê²©ìœ¼ë¡œ ì´ë™í•˜ë©° ì…ë ¥ ë°ì´í„°ì— í•„í„°ë¥¼ ê³±í•´ì„œ ë”í•¨ â†’ ì¶œë ¥ ë°ì´í„°
- CNN ì—ì„œëŠ” í•„í„°ì˜ ë§¤ê°œë³€ìˆ˜ê°€ ê³§ ê°€ì¤‘ì¹˜, í¸í–¥ì€ í•­ìƒ í•˜ë‚˜ë§Œ ì¡´ì¬

### íŒ¨ë”©

- í•©ì„±ê³± ì—°ì‚°ì„ ìˆ˜í–‰í•˜ê¸° ì „, ì…ë ¥ ë°ì´í„° ì£¼ë³€ ê°’ì„ íŠ¹ì • ê°’(ì£¼ë¡œ 0)ìœ¼ë¡œ ì±„ìš°ëŠ” ê²ƒ
    
    ![[Media/Screenshot_2023-05-05_at_10.35.45_PM 2.png|Screenshot_2023-05-05_at_10.35.45_PM 2.png]]
    
    - ìœ„ì—ì„œëŠ” ì…ë ¥ ë°ì´í„° (4,4) â†’ íŒ¨ë”© (5,5) â†’ í•©ì„±ê³± (3,3) â†’ ì¶œë ¥ ë°ì´í„° (4,4)
- ì…ë ¥ ë°ì´í„°ì˜ í¬ê¸°ê°€ ê³ ì •ë©ë‹ˆë‹¤

### ìŠ¤íŠ¸ë¼ì´ë“œ

- í•„í„°ë¥¼ ì ìš©í•˜ëŠ” ê°„ê²©
    - í•œì¹¸ ì”© ì´ë™ - stride 1
    - ë‘ì¹¸ ì”© ì´ë™ - stride 2

### íŒ¨ë”©, ìŠ¤íŠ¸ë¼ì´ë“œ, ì¶œë ¥ í¬ê¸°ì˜ ê´€ê³„

![[Media/Screenshot_2023-05-05_at_10.42.24_PM 2.png|Screenshot_2023-05-05_at_10.42.24_PM 2.png]]

- ì˜ˆì‹œ ë¬¸ì œ
    
    ![[Media/Screenshot_2023-05-05_at_10.44.11_PM 2.png|Screenshot_2023-05-05_at_10.44.11_PM 2.png]]
    

### 3ì°¨ì› ë°ì´í„°ì˜ í•©ì„±ê³± ì—°ì‚°

- 2ì°¨ì› + ì±„ë„ = 3ì°¨ì› ë°ì´í„° (ì£¼ë¡œ RGB)
- ì…ë ¥ ì±„ë„ ìˆ˜ = í•„í„°ì˜ ì±„ë„ìˆ˜
- ì¶œë ¥ì€ 2ì°¨ì›ì´ ë˜ëŠ”ê°€

### ì§ìœ¡ë©´ì²´ ë¸”ë¡

- 3ì°¨ì› í•©ì„±ê³± ì—°ì‚°ì˜ ë°ì´í„° ì°¨ì› í‘œê¸°ë²•
    
    ![[Media/Screenshot_2023-05-05_at_10.47.48_PM 2.png|Screenshot_2023-05-05_at_10.47.48_PM 2.png]]
    
- ì¶œë ¥ ë°ì´í„°ë¥¼ ì—¬ëŸ¬ê°œì˜ ì±„ë„ë¡œ ë§Œë“¤ê³  ì‹¶ì–´ìš”
    - í•„í„°ë¥¼ ì—¬ëŸ¬ ê°œ ì‚¬ìš©í•©ë‹ˆë‹¤ â†’ í•„í„°ì˜ ê°œìˆ˜ = ì¶œë ¥ ë°ì´í„°ì˜ ì±„ë„ ìˆ˜
    - ê·¸ëŸ¬ë©´ í•„í„°ëŠ” 4ì°¨ì› ë°ì´í„°ê°€ ë˜ë„¤ìš”â€¦.
        
        ![[Media/Screenshot_2023-05-05_at_10.50.49_PM 2.png|Screenshot_2023-05-05_at_10.50.49_PM 2.png]]
        
- í¸í–¥ì„ ì¶”ê°€í•œ í•©ì„±ê³± ì—°ì‚°ì˜ íë¦„
    
    ![[Media/Screenshot_2023-05-05_at_10.52.47_PM 2.png|Screenshot_2023-05-05_at_10.52.47_PM 2.png]]
    

### ë°°ì¹˜ ì²˜ë¦¬

- í•©ì„±ê³± ì—°ì‚°ì—ë„ ë°°ì¹˜ ì²˜ë¦¬ë¥¼ í†µí•´ íš¨ìœ¨ì„ ë†’ì¼ ìˆ˜ ìˆìŒ
    
    ![[Media/Screenshot_2023-05-05_at_10.56.31_PM 2.png|Screenshot_2023-05-05_at_10.56.31_PM 2.png]]
    

---

## í’€ë§ ê³„ì¸µ

### í’€ë§ ê³„ì¸µ

- ì„¸ë¡œ, ê°€ë¡œ ë°©í–¥ì˜ ì¼ì • ì˜ì—­ì„ ì›ì†Œ í•˜ë‚˜ë¡œ ì§‘ì•½
    - ìµœëŒ€ í’€ë§ Max Pooling
        - í’€ë§ ì˜ì—­ì—ì„œ ìµœëŒ“ê°’ì„ ì·¨í•¨
- ìŠ¤íŠ¸ë¼ì´ë“œëŠ” í•„í„°ì˜â€¦ +1?
- íŠ¹ì§•
    - í•™ìŠµí•´ì•¼í•  ë§¤ê°œë³€ìˆ˜ê°€ ì—†ë‹¤
    - ì±„ë„ì˜ ìˆ˜ê°€ ë³€í•˜ì§€ ì•ŠëŠ”ë‹¤
    - ì…ë ¥ì˜ ë³€í™”ì— ì˜í–¥ì„ ì ê²Œ ë°›ìŒ

---

## ì™„ì „ì—°ê²° ê³„ì¸µ

### ì™„ì „ì—°ê²° ê³„ì¸µ

- ë§ˆì§€ë§‰ Conv ê³„ì¸µì—ì„œì˜ ì¶œë ¥ ë°ì´í„°ë¥¼ ì²«ë²ˆì§¸ FC ê³„ì¸µì— ë„£ì–´ì„œâ€¦ ë„£ì„ ë•ŒëŠ” ì´ì°¨ì› ë°ì´í„°ë¡œâ€¦

---

## CNN êµ¬í˜„

- ConvNet1 : Conv1 + FC1 â† ìš°ë¦¬ê°€ ë§Œë“¤ ê²ƒ
    
    ![[Media/Screenshot_2023-05-05_at_11.06.54_PM 2.png|Screenshot_2023-05-05_at_11.06.54_PM 2.png]]
    
- Conv2d ëª¨ë“ˆì„ ì‚¬ìš©í•  ìˆ˜ ìˆì–´ìš”
    
    - Conv2D(in_channels=C, out_channels=FN, kernel_size=FH)
    
    - Convê°€ ë­”ë°ìš©
    
    ```Python
    class ConvNet1(nn.Module):
        def __init__(self, num_classes=10):
            super().__init__()
    
            # nn.Sequential ì •ì˜ - ì—¬ëŸ¬ ë ˆì´ì–´ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ê·¸ë£¹í™”
            self.layer1 = nn.Sequential(
                # Conv2D - conv layer (ì…ë ¥ ì±„ë„ 1, ì¶œë ¥ ì±„ë„ 30, í•„í„°ì‚¬ì´ì¦ˆ 5x5, ìŠ¤íŠ¸ë¼ì´ë“œ 1....)
                nn.Conv2d(1, 30, kernel_size=5, stride=1, padding=0)
    
                # BatchNorm2d - ë°°ì¹˜ì •ê·œí™” layer, ì±„ë„ì€ 30
                nn.BaychNorm2d(30),
                nn.ReLU(),
    
                # MaxPool2d - 2x2 max pool
                nn.MaxPool2d(kernel_size=2, stride=2)
            )
    
            # fc layer - ì…ë ¥ : 12*12*30, ì¶œë ¥ : num_classes
            self.fc = nn.Linear(12*12*30, num_classes)
            
        def forward(self, x):
            # layer1 í†µê³¼
            out = self.layer1(x)
            # fc layer ì…ë ¥ì „ reshape
            out = out.reshape(out.size(0), -1)
            # fc layer í†µê³¼
            out = self.fc(out)
            return out
    ```
    
    ```Python
    # train í•¨ìˆ˜
    def train():
      # í›ˆë ¨ ëª¨ë“œ
      model.train()
    
      batch_loss_list = []
      progress = ProgressMonitor(length=len(train_dataset))
    
      for batch, target in train_loader:
        # train batchë¥¼ GPUë¡œ
        batch, target = batch.to(device), target.to(device)
    
        # ìˆœì „íŒŒ, ì†ì‹¤ê³„ì‚°
        output = model(batch)
        loss = loss_func( output, target)
    
        optimizer.zero_grad()
    
        loss.backward()
        optimizer.step()
    
        batch_loss_list.append(loss.item())
        progress.update(batch.shape[0], sum(batch_loss_list)/len(batch_loss_list))
    
    def test():
      model.eval()
    
      correct = 0
    
      with torch.no_grad():
        for batch, target in test_loader:
          batch, target = batch.to(device), target.to(device)
    
          output = model(batch)
          output = torch.argmax(output,1)
    
          correct += (output == target).sum().item()
    
        acc = 100 * float(correct) / len(test_dataset)
        print('Test accracy: {}/{} ({:.2f}%'.format(correct, len(test_dataset), acc))
    ```
    

---

## CNN ì‹œê°í™”

---

## CNN ì˜ ì˜ˆì‹œ

- LeNet
- AlexNet
    - LRN ì‚¬ìš©
    - ë“œë¡­ì•„ì›ƒ ì‚¬ìš©

  

## Project

- dataset2
    
    Epoch 1/50  
    Train loss : 433.8795620575547  
    Test accuracy 331/660 (50.151515%)  
    Epoch 2/50  
    Train loss : 367.97383131086826  
    Test accuracy 331/660 (50.151515%)  
    Epoch 3/50  
    Train loss : 355.1996685862541  
    Test accuracy 335/660 (50.757576%)  
    Epoch 4/50  
    Train loss : 364.3521046489477  
    Test accuracy 365/660 (55.303030%)  
    Epoch 5/50  
    Train loss : 370.6498119086027  
    Test accuracy 356/660 (53.939394%)  
    Epoch 6/50  
    Train loss : 331.0747189670801  
    Test accuracy 368/660 (55.757576%)  
    Epoch 7/50  
    Train loss : 319.66992551088333  
    Test accuracy 373/660 (56.515152%)  
    Epoch 8/50  
    Train loss : 286.49186509661376  
    Test accuracy 368/660 (55.757576%)  
    Epoch 9/50  
    Train loss : 272.48600285872817  
    Test accuracy 377/660 (57.121212%)  
    Epoch 10/50  
    Train loss : 226.85299442708492  
    Test accuracy 347/660 (52.575758%)  
    Epoch 11/50  
    Train loss : 202.22799311461858  
    Test accuracy 379/660 (57.424242%)  
    Epoch 12/50  
    Train loss : 188.85355841135606  
    Test accuracy 362/660 (54.848485%)  
    Epoch 13/50  
    Train loss : 157.0219695288688  
    Test accuracy 380/660 (57.575758%)  
    Epoch 14/50  
    Train loss : 100.96259290885064  
    Test accuracy 391/660 (59.242424%)  
    Epoch 15/50  
    Train loss : 69.29816795409715  
    Test accuracy 373/660 (56.515152%)  
    Epoch 16/50  
    Train loss : 51.87082632185775  
    Test accuracy 383/660 (58.030303%)  
    Epoch 17/50  
    Train loss : 33.96104884392116  
    Test accuracy 388/660 (58.787879%)  
    Epoch 18/50  
    Train loss : 22.588687931129243  
    Test accuracy 385/660 (58.333333%)  
    Epoch 19/50  
    Train loss : 9.89001023027231  
    Test accuracy 394/660 (59.696970%)  
    Epoch 20/50  
    Train loss : 4.152478828684252  
    Test accuracy 392/660 (59.393939%)  
    Epoch 21/50  
    Train loss : 8.707331527628412  
    Test accuracy 386/660 (58.484848%)  
    Epoch 22/50  
    Train loss : 8.972744699516625  
    Test accuracy 390/660 (59.090909%)  
    Epoch 23/50  
    Train loss : 4.701703791059117  
    Test accuracy 387/660 (58.636364%)  
    Epoch 24/50  
    Train loss : 4.740196201084473  
    Test accuracy 404/660 (61.212121%)  
    Epoch 25/50  
    Train loss : 5.618677305177698  
    Test accuracy 407/660 (61.666667%)  
    Epoch 26/50  
    Train loss : 1.335235112461305  
    Test accuracy 407/660 (61.666667%)  
    Epoch 27/50  
    Train loss : 1.422667796814494  
    Test accuracy 416/660 (63.030303%)  
    Epoch 28/50  
    Train loss : 0.6701745056761865  
    Test accuracy 402/660 (60.909091%)  
    Epoch 29/50  
    Train loss : 0.854424114277208  
    Test accuracy 396/660 (60.000000%)  
    Epoch 30/50  
    Train loss : 0.5669067905482734  
    Test accuracy 408/660 (61.818182%)  
    Epoch 31/50  
    Train loss : 0.4687967082418254  
    Test accuracy 402/660 (60.909091%)  
    Epoch 32/50  
    Train loss : 1.251823444123147  
    Test accuracy 407/660 (61.666667%)  
    Epoch 33/50  
    Train loss : 0.7797148492327324  
    Test accuracy 404/660 (61.212121%)  
    Epoch 34/50  
    Train loss : 0.4607491051974648  
    Test accuracy 401/660 (60.757576%)  
    Epoch 35/50  
    Train loss : 0.45757791616779286  
    Test accuracy 405/660 (61.363636%)  
    Epoch 36/50  
    Train loss : 0.4125841020504595  
    Test accuracy 415/660 (62.878788%)  
    Epoch 37/50  
    Train loss : 1.424494672074161  
    Test accuracy 395/660 (59.848485%)  
    Epoch 38/50  
    Train loss : 0.894138605626722  
    Test accuracy 393/660 (59.545455%)  
    Epoch 39/50  
    Train loss : 0.5815037472120821  
    Test accuracy 402/660 (60.909091%)  
    Epoch 40/50  
    Train loss : 0.433586412385921  
    Test accuracy 391/660 (59.242424%)  
    Epoch 41/50  
    Train loss : 0.4140952911129716  
    Test accuracy 395/660 (59.848485%)  
    Epoch 42/50  
    Train loss : 0.5287650238778951  
    Test accuracy 406/660 (61.515152%)  
    Epoch 43/50  
    Train loss : 0.2785112618275889  
    Test accuracy 406/660 (61.515152%)  
    Epoch 44/50  
    Train loss : 0.32402577356060647  
    Test accuracy 409/660 (61.969697%)  
    Epoch 45/50  
    Train loss : 0.19914430985409126  
    Test accuracy 406/660 (61.515152%)  
    Epoch 46/50  
    Train loss : 0.19978674412959663  
    Test accuracy 406/660 (61.515152%)  
    Epoch 47/50  
    Train loss : 0.10707507464394439  
    Test accuracy 409/660 (61.969697%)  
    Epoch 48/50  
    Train loss : 0.15685593246507779  
    Test accuracy 408/660 (61.818182%)  
    Epoch 49/50  
    Train loss : 0.15535948423723767  
    Test accuracy 408/660 (61.818182%)  
    Epoch 50/50  
    Train loss : 0.12640929552412672  
    Test accuracy 405/660 (61.363636%)  
    
- hyperparameter-first - resnet18 - 4,50,0.001
    
    Epoch 1/50  
    Train loss : 1709.847081899643  
    Test accuracy 78/660 (11.818182%)  
    Epoch 2/50  
    Train loss : 1457.2198804616928  
    Test accuracy 113/660 (17.121212%)  
    Epoch 3/50  
    Train loss : 1362.1738313436508  
    Test accuracy 159/660 (24.090909%)  
    Epoch 4/50  
    Train loss : 1240.6358853578568  
    Test accuracy 198/660 (30.000000%)  
    Epoch 5/50  
    Train loss : 1130.9488495588303  
    Test accuracy 208/660 (31.515152%)  
    Epoch 6/50  
    Train loss : 1004.7225704193115  
    Test accuracy 230/660 (34.848485%)  
    Epoch 7/50  
    Train loss : 851.3898586928844  
    Test accuracy 221/660 (33.484848%)  
    Epoch 8/50  
    Train loss : 678.6424172967672  
    Test accuracy 235/660 (35.606061%)  
    Epoch 9/50  
    Train loss : 559.0368413478136  
    Test accuracy 217/660 (32.878788%)  
    Epoch 10/50  
    Train loss : 394.96942318975925  
    Test accuracy 232/660 (35.151515%)  
    Epoch 11/50  
    Train loss : 304.5309368185699  
    Test accuracy 224/660 (33.939394%)  
    Epoch 12/50  
    Train loss : 188.42359252972528  
    Test accuracy 232/660 (35.151515%)  
    Epoch 13/50  
    Train loss : 161.48146144486964  
    Test accuracy 233/660 (35.303030%)  
    Epoch 14/50  
    Train loss : 132.40016750921495  
    Test accuracy 222/660 (33.636364%)  
    Epoch 15/50  
    Train loss : 104.06399890175089  
    Test accuracy 268/660 (40.606061%)  
    Epoch 16/50  
    Train loss : 84.98558356903959  
    Test accuracy 238/660 (36.060606%)  
    Epoch 17/50  
    Train loss : 81.60152271628613  
    Test accuracy 237/660 (35.909091%)  
    Epoch 18/50  
    Train loss : 61.01945226744283  
    Test accuracy 253/660 (38.333333%)  
    Epoch 19/50  
    Train loss : 51.69554940453963  
    Test accuracy 223/660 (33.787879%)  
    Epoch 20/50  
    Train loss : 60.296447047338006  
    Test accuracy 268/660 (40.606061%)  
    Epoch 21/50  
    Train loss : 42.545033527349005  
    Test accuracy 259/660 (39.242424%)  
    Epoch 22/50  
    Train loss : 57.146607084272546  
    Test accuracy 246/660 (37.272727%)  
    Epoch 23/50  
    Train loss : 38.560218845683266  
    Test accuracy 270/660 (40.909091%)  
    Epoch 24/50  
    Train loss : 38.47810690211918  
    Test accuracy 278/660 (42.121212%)  
    Epoch 25/50  
    Train loss : 40.49320298132079  
    Test accuracy 255/660 (38.636364%)  
    Epoch 26/50  
    Train loss : 35.753498859441606  
    Test accuracy 233/660 (35.303030%)  
    Epoch 27/50  
    Train loss : 29.116213314475317  
    Test accuracy 261/660 (39.545455%)  
    Epoch 28/50  
    Train loss : 28.91523071397387  
    Test accuracy 258/660 (39.090909%)  
    Epoch 29/50  
    Train loss : 34.80598680575349  
    Test accuracy 234/660 (35.454545%)  
    Epoch 30/50  
    Train loss : 34.47565404661145  
    Test accuracy 249/660 (37.727273%)  
    Epoch 31/50  
    Train loss : 25.280880429636454  
    Test accuracy 251/660 (38.030303%)  
    Epoch 32/50  
    Train loss : 28.80112268704397  
    Test accuracy 262/660 (39.696970%)  
    Epoch 33/50  
    Train loss : 27.923733881405497  
    Test accuracy 247/660 (37.424242%)  
    Epoch 34/50  
    Train loss : 26.8798210816567  
    Test accuracy 255/660 (38.636364%)  
    Epoch 35/50  
    Train loss : 33.21862790205341  
    Test accuracy 257/660 (38.939394%)  
    Epoch 36/50  
    Train loss : 24.804143441142514  
    Test accuracy 246/660 (37.272727%)  
    Epoch 37/50  
    Train loss : 20.179840346732817  
    Test accuracy 251/660 (38.030303%)  
    Epoch 38/50  
    Train loss : 16.24657160982315  
    Test accuracy 257/660 (38.939394%)  
    Epoch 39/50  
    Train loss : 19.402670546944137  
    Test accuracy 259/660 (39.242424%)  
    Epoch 40/50  
    Train loss : 27.055525137235236  
    Test accuracy 264/660 (40.000000%)  
    Epoch 41/50  
    Train loss : 14.399680379399797  
    Test accuracy 250/660 (37.878788%)  
    Epoch 42/50  
    Train loss : 14.137002646581095  
    Test accuracy 251/660 (38.030303%)  
    Epoch 43/50  
    Train loss : 23.35528039847304  
    Test accuracy 255/660 (38.636364%)  
    Epoch 44/50  
    Train loss : 21.659646429732675  
    Test accuracy 265/660 (40.151515%)  
    Epoch 45/50  
    Train loss : 12.748209141995176  
    Test accuracy 264/660 (40.000000%)  
    Epoch 46/50  
    Train loss : 25.736757634277637  
    Test accuracy 239/660 (36.212121%)  
    Epoch 47/50  
    Train loss : 16.548289263377228  
    Test accuracy 273/660 (41.363636%)  
    Epoch 48/50  
    Train loss : 17.787550484527856  
    Test accuracy 270/660 (40.909091%)  
    Epoch 49/50  
    Train loss : 19.65188252058215  
    Test accuracy 268/660 (40.606061%)  
    Epoch 50/50  
    Train loss : 15.553027820926218  
    Test accuracy 262/660 (39.696970%)  
    
- hyperparameter - second - resnet18 - 4,30,0.005
    
    Epoch 1/30  
    Train loss : 1849.9632233381271  
    Test accuracy 81/660 (12.272727%)  
    Epoch 2/30  
    Train loss : 1479.5314617156982  
    Test accuracy 130/660 (19.696970%)  
    Epoch 3/30  
    Train loss : 1338.2574572563171  
    Test accuracy 170/660 (25.757576%)  
    Epoch 4/30  
    Train loss : 1221.3547901511192  
    Test accuracy 160/660 (24.242424%)  
    Epoch 5/30  
    Train loss : 1111.760188639164  
    Test accuracy 193/660 (29.242424%)  
    Epoch 6/30  
    Train loss : 999.8287369012833  
    Test accuracy 218/660 (33.030303%)  
    Epoch 7/30  
    Train loss : 854.5187346935272  
    Test accuracy 213/660 (32.272727%)  
    Epoch 8/30  
    Train loss : 742.3231260031462  
    Test accuracy 225/660 (34.090909%)  
    Epoch 9/30  
    Train loss : 570.8776232227683  
    Test accuracy 225/660 (34.090909%)  
    Epoch 10/30  
    Train loss : 474.9924735929817  
    Test accuracy 237/660 (35.909091%)  
    Epoch 11/30  
    Train loss : 333.37734261900187  
    Test accuracy 241/660 (36.515152%)  
    Epoch 12/30  
    Train loss : 302.74999504722655  
    Test accuracy 256/660 (38.787879%)  
    Epoch 13/30  
    Train loss : 197.20883095590398  
    Test accuracy 216/660 (32.727273%)  
    Epoch 14/30  
    Train loss : 139.65518539608456  
    Test accuracy 238/660 (36.060606%)  
    Epoch 15/30  
    Train loss : 113.755049755855  
    Test accuracy 242/660 (36.666667%)  
    Epoch 16/30  
    Train loss : 71.42039379494963  
    Test accuracy 251/660 (38.030303%)  
    Epoch 17/30  
    Train loss : 69.48051309611765  
    Test accuracy 249/660 (37.727273%)  
    Epoch 18/30  
    Train loss : 50.4735193032393  
    Test accuracy 274/660 (41.515152%)  
    Epoch 19/30  
    Train loss : 40.34680068130547  
    Test accuracy 240/660 (36.363636%)  
    Epoch 20/30  
    Train loss : 45.04781518915843  
    Test accuracy 236/660 (35.757576%)  
    Epoch 21/30  
    Train loss : 39.148906511924  
    Test accuracy 253/660 (38.333333%)  
    Epoch 22/30  
    Train loss : 23.605487315100618  
    Test accuracy 252/660 (38.181818%)  
    Epoch 23/30  
    Train loss : 21.199476290272287  
    Test accuracy 252/660 (38.181818%)  
    Epoch 24/30  
    Train loss : 34.25592986674019  
    Test accuracy 265/660 (40.151515%)  
    Epoch 25/30  
    Train loss : 22.37580777496987  
    Test accuracy 261/660 (39.545455%)  
    Epoch 26/30  
    Train loss : 17.42484427744239  
    Test accuracy 272/660 (41.212121%)  
    Epoch 27/30  
    Train loss : 32.63602443760465  
    Test accuracy 247/660 (37.424242%)  
    Epoch 28/30  
    Train loss : 29.996329996582062  
    Test accuracy 262/660 (39.696970%)  
    Epoch 29/30  
    Train loss : 17.41629193332119  
    Test accuracy 252/660 (38.181818%)  
    Epoch 30/30  
    Train loss : 18.41998718889954  
    Test accuracy 263/660 (39.848485%)  
    
- 2.0 (hyperparameter 4,30,0.01 ë°ì´í„° ì „ì²˜ë¦¬
    
    Epoch 1/30  
    Train loss : 1863.2882692813873  
    Test accuracy 43/660 (6.515152%)  
    Epoch 2/30  
    Train loss : 1572.0648002624512  
    Test accuracy 46/660 (6.969697%)  
    Epoch 3/30  
    Train loss : 1501.6109511852264  
    Test accuracy 63/660 (9.545455%)  
    Epoch 4/30  
    Train loss : 1464.4967592954636  
    Test accuracy 105/660 (15.909091%)  
    Epoch 5/30  
    Train loss : 1432.4523386955261  
    Test accuracy 127/660 (19.242424%)  
    Epoch 6/30  
    Train loss : 1414.1929708719254  
    Test accuracy 99/660 (15.000000%)  
    Epoch 7/30  
    Train loss : 1391.9070833921432  
    Test accuracy 143/660 (21.666667%)  
    Epoch 8/30  
    Train loss : 1348.4420142173767  
    Test accuracy 128/660 (19.393939%)  
    Epoch 9/30  
    Train loss : 1315.5391248464584  
    Test accuracy 146/660 (22.121212%)  
    Epoch 10/30  
    Train loss : 1296.9601438045502  
    Test accuracy 149/660 (22.575758%)  
    Epoch 11/30  
    Train loss : 1294.992491722107  
    Test accuracy 152/660 (23.030303%)  
    Epoch 12/30  
    Train loss : 1263.5051754713058  
    Test accuracy 167/660 (25.303030%)  
    Epoch 13/30  
    Train loss : 1241.1774116754532  
    Test accuracy 187/660 (28.333333%)  
    Epoch 14/30  
    Train loss : 1203.5502259731293  
    Test accuracy 156/660 (23.636364%)  
    Epoch 15/30  
    Train loss : 1190.58535695076  
    Test accuracy 165/660 (25.000000%)  
    Epoch 16/30  
    Train loss : 1170.852182149887  
    Test accuracy 207/660 (31.363636%)  
    Epoch 17/30  
    Train loss : 1135.143336892128  
    Test accuracy 203/660 (30.757576%)  
    Epoch 18/30  
    Train loss : 1133.9858967065811  
    Test accuracy 234/660 (35.454545%)  
    Epoch 19/30  
    Train loss : 1109.5767900943756  
    Test accuracy 203/660 (30.757576%)  
    Epoch 20/30  
    Train loss : 1103.0872507095337  
    Test accuracy 206/660 (31.212121%)  
    Epoch 21/30  
    Train loss : 1059.646414399147  
    Test accuracy 190/660 (28.787879%)  
    Epoch 22/30  
    Train loss : 1049.0566716194153  
    Test accuracy 250/660 (37.878788%)  
    Epoch 23/30  
    Train loss : 1026.284754216671  
    Test accuracy 242/660 (36.666667%)  
    Epoch 24/30  
    Train loss : 1002.0206743478775  
    Test accuracy 250/660 (37.878788%)  
    Epoch 25/30  
    Train loss : 1000.1578423082829  
    Test accuracy 254/660 (38.484848%)  
    Epoch 26/30  
    Train loss : 964.2373849749565  
    Test accuracy 261/660 (39.545455%)  
    Epoch 27/30  
    Train loss : 968.1672407388687  
    Test accuracy 247/660 (37.424242%)  
    Epoch 28/30  
    Train loss : 953.3905048966408  
    Test accuracy 247/660 (37.424242%)  
    Epoch 29/30  
    Train loss : 930.5266073942184  
    Test accuracy 289/660 (43.787879%)  
    Epoch 30/30  
    Train loss : 914.8646373748779  
    Test accuracy 298/660 (45.151515%)  
    
- 2.1 (model - resnet50)
    
    Epoch 1/30  
    Train loss : 2235.8202731609344  
    Test accuracy 53/660 (8.030303%)  
    Epoch 2/30  
    Train loss : 1617.0080437660217  
    Test accuracy 61/660 (9.242424%)  
    Epoch 3/30  
    Train loss : 1564.0348501205444  
    Test accuracy 65/660 (9.848485%)  
    Epoch 4/30  
    Train loss : 1545.6281716823578  
    Test accuracy 59/660 (8.939394%)  
    Epoch 5/30  
    Train loss : 1511.2544466257095  
    Test accuracy 78/660 (11.818182%)  
    Epoch 6/30  
    Train loss : 1483.153357744217  
    Test accuracy 88/660 (13.333333%)  
    Epoch 7/30  
    Train loss : 1459.7515054941177  
    Test accuracy 98/660 (14.848485%)  
    Epoch 8/30  
    Train loss : 1438.2500838041306  
    Test accuracy 98/660 (14.848485%)  
    Epoch 9/30  
    Train loss : 1422.3219151496887  
    Test accuracy 94/660 (14.242424%)  
    Epoch 10/30  
    Train loss : 1400.4271770715714  
    Test accuracy 95/660 (14.393939%)  
    Epoch 11/30  
    Train loss : 1386.8792620897293  
    Test accuracy 107/660 (16.212121%)  
    Epoch 12/30  
    Train loss : 1369.3411258459091  
    Test accuracy 103/660 (15.606061%)  
    Epoch 13/30  
    Train loss : 1348.5258370637894  
    Test accuracy 125/660 (18.939394%)  
    Epoch 14/30  
    Train loss : 1335.2068976163864  
    Test accuracy 129/660 (19.545455%)  
    Epoch 15/30  
    Train loss : 1321.1347044706345  
    Test accuracy 87/660 (13.181818%)  
    Epoch 16/30  
    Train loss : 1312.4720759391785  
    Test accuracy 120/660 (18.181818%)  
    Epoch 17/30  
    Train loss : 1296.539174079895  
    Test accuracy 105/660 (15.909091%)  
    Epoch 18/30  
    Train loss : 1293.357898235321  
    Test accuracy 147/660 (22.272727%)  
    Epoch 19/30  
    Train loss : 1269.5970216989517  
    Test accuracy 158/660 (23.939394%)  
    Epoch 20/30  
    Train loss : 1254.2561793327332  
    Test accuracy 116/660 (17.575758%)  
    Epoch 21/30  
    Train loss : 1228.313757777214  
    Test accuracy 145/660 (21.969697%)  
    Epoch 22/30  
    Train loss : 1195.190467953682  
    Test accuracy 171/660 (25.909091%)  
    Epoch 23/30  
    Train loss : 1202.648944735527  
    Test accuracy 177/660 (26.818182%)  
    Epoch 24/30  
    Train loss : 1183.2876206636429  
    Test accuracy 138/660 (20.909091%)  
    Epoch 25/30  
    Train loss : 1153.555959045887  
    Test accuracy 168/660 (25.454545%)  
    Epoch 26/30  
    Train loss : 1156.2759185433388  
    Test accuracy 184/660 (27.878788%)  
    Epoch 27/30  
    Train loss : 1136.1671670079231  
    Test accuracy 162/660 (24.545455%)  
    Epoch 28/30  
    Train loss : 1138.1718531847  
    Test accuracy 185/660 (28.030303%)  
    Epoch 29/30  
    Train loss : 1115.3888199925423  
    Test accuracy 191/660 (28.939394%)  
    Epoch 30/30  
    Train loss : 1087.6902663111687  
    Test accuracy 198/660 (30.000000%)  
    
- 2.2 (model - DenseNet)
    
    Epoch 1/30  
    Train loss : 1818.805026292801  
    Test accuracy 70/660 (10.606061%)  
    Epoch 2/30  
    Train loss : 1531.8445568084717  
    Test accuracy 86/660 (13.030303%)  
    Epoch 3/30  
    Train loss : 1491.5209988355637  
    Test accuracy 91/660 (13.787879%)  
    Epoch 4/30  
    Train loss : 1471.8161920309067  
    Test accuracy 71/660 (10.757576%)  
    Epoch 5/30  
    Train loss : 1421.1587158441544  
    Test accuracy 91/660 (13.787879%)  
    Epoch 6/30  
    Train loss : 1400.169784784317  
    Test accuracy 107/660 (16.212121%)  
    Epoch 7/30  
    Train loss : 1391.210727095604  
    Test accuracy 129/660 (19.545455%)  
    Epoch 8/30  
    Train loss : 1360.1707566976547  
    Test accuracy 107/660 (16.212121%)  
    Epoch 9/30  
    Train loss : 1354.5485212802887  
    Test accuracy 129/660 (19.545455%)  
    Epoch 10/30  
    Train loss : 1311.3734602928162  
    Test accuracy 128/660 (19.393939%)  
    Epoch 11/30  
    Train loss : 1301.185721039772  
    Test accuracy 133/660 (20.151515%)  
    Epoch 12/30  
    Train loss : 1257.6844409704208  
    Test accuracy 130/660 (19.696970%)  
    Epoch 13/30  
    Train loss : 1248.2922520637512  
    Test accuracy 153/660 (23.181818%)  
    Epoch 14/30  
    Train loss : 1237.014399588108  
    Test accuracy 154/660 (23.333333%)  
    Epoch 15/30  
    Train loss : 1227.6517922878265  
    Test accuracy 152/660 (23.030303%)  
    Epoch 16/30  
    Train loss : 1195.908163189888  
    Test accuracy 174/660 (26.363636%)  
    Epoch 17/30  
    Train loss : 1157.0713956356049  
    Test accuracy 191/660 (28.939394%)  
    Epoch 18/30  
    Train loss : 1136.7711527943611  
    Test accuracy 221/660 (33.484848%)  
    Epoch 19/30  
    Train loss : 1149.2795425057411  
    Test accuracy 198/660 (30.000000%)  
    Epoch 20/30  
    Train loss : 1109.3303876519203  
    Test accuracy 188/660 (28.484848%)  
    Epoch 21/30  
    Train loss : 1090.05551725626  
    Test accuracy 204/660 (30.909091%)  
    Epoch 22/30  
    Train loss : 1079.6727448105812  
    Test accuracy 203/660 (30.757576%)  
    Epoch 23/30  
    Train loss : 1066.7630581855774  
    Test accuracy 189/660 (28.636364%)  
    Epoch 24/30  
    Train loss : 1059.8549606204033  
    Test accuracy 208/660 (31.515152%)  
    Epoch 25/30  
    Train loss : 1059.3921986222267  
    Test accuracy 242/660 (36.666667%)  
    Epoch 26/30  
    Train loss : 1056.7434205114841  
    Test accuracy 264/660 (40.000000%)  
    Epoch 27/30  
    Train loss : 1051.563884794712  
    Test accuracy 244/660 (36.969697%)  
    Epoch 28/30  
    Train loss : 1009.0424494743347  
    Test accuracy 275/660 (41.666667%)  
    Epoch 29/30  
    Train loss : 962.385627746582  
    Test accuracy 215/660 (32.575758%)  
    Epoch 30/30  
    Train loss : 992.5944796800613  
    Test accuracy 288/660 (43.636364%)  
    
- 2.3 (densenet121)
    
    Epoch 1/30  
    Train loss : 1757.6924502849579  
    Test accuracy 95/660 (14.393939%)  
    Epoch 2/30  
    Train loss : 1542.097622036934  
    Test accuracy 90/660 (13.636364%)  
    Epoch 3/30  
    Train loss : 1496.8621106147766  
    Test accuracy 82/660 (12.424242%)  
    Epoch 4/30  
    Train loss : 1442.8013805150986  
    Test accuracy 83/660 (12.575758%)  
    Epoch 5/30  
    Train loss : 1432.0028762817383  
    Test accuracy 109/660 (16.515152%)  
    Epoch 6/30  
    Train loss : 1412.9116492271423  
    Test accuracy 115/660 (17.424242%)  
    Epoch 7/30  
    Train loss : 1380.3279049396515  
    Test accuracy 118/660 (17.878788%)  
    Epoch 8/30  
    Train loss : 1352.683423280716  
    Test accuracy 125/660 (18.939394%)  
    Epoch 9/30  
    Train loss : 1331.2695035934448  
    Test accuracy 111/660 (16.818182%)  
    Epoch 10/30  
    Train loss : 1300.0142563581467  
    Test accuracy 179/660 (27.121212%)  
    Epoch 11/30  
    Train loss : 1285.3112505674362  
    Test accuracy 152/660 (23.030303%)  
    Epoch 12/30  
    Train loss : 1278.9220252037048  
    Test accuracy 163/660 (24.696970%)  
    Epoch 13/30  
    Train loss : 1247.9215015172958  
    Test accuracy 145/660 (21.969697%)  
    Epoch 14/30  
    Train loss : 1221.3149000406265  
    Test accuracy 188/660 (28.484848%)  
    Epoch 15/30  
    Train loss : 1271.290284872055  
    Test accuracy 164/660 (24.848485%)  
    Epoch 16/30  
    Train loss : 1213.7111866474152  
    Test accuracy 200/660 (30.303030%)  
    Epoch 17/30  
    Train loss : 1170.9882282614708  
    Test accuracy 214/660 (32.424242%)  
    Epoch 18/30  
    Train loss : 1156.5821832418442  
    Test accuracy 212/660 (32.121212%)  
    Epoch 19/30  
    Train loss : 1138.4767770767212  
    Test accuracy 183/660 (27.727273%)  
    Epoch 20/30  
    Train loss : 1152.917645931244  
    Test accuracy 237/660 (35.909091%)  
    Epoch 21/30  
    Train loss : 1120.9327235221863  
    Test accuracy 220/660 (33.333333%)  
    Epoch 22/30  
    Train loss : 1120.5798943638802  
    Test accuracy 249/660 (37.727273%)  
    Epoch 23/30  
    Train loss : 1115.1758430600166  
    Test accuracy 214/660 (32.424242%)  
    Epoch 24/30  
    Train loss : 1078.6732615232468  
    Test accuracy 236/660 (35.757576%)  
    Epoch 25/30  
    Train loss : 1082.560262799263  
    Test accuracy 258/660 (39.090909%)  
    Epoch 26/30  
    Train loss : 1062.7868010997772  
    Test accuracy 275/660 (41.666667%)  
    Epoch 27/30  
    Train loss : 1050.5425499379635  
    Test accuracy 270/660 (40.909091%)  
    Epoch 28/30  
    Train loss : 1008.3743104934692  
    Test accuracy 267/660 (40.454545%)  
    Epoch 29/30  
    Train loss : 1037.8094412088394  
    Test accuracy 242/660 (36.666667%)  
    Epoch 30/30  
    Train loss : 1008.6187039017677  
    Test accuracy 257/660 (38.939394%)  
    
- 2.4 (custom)
    
    Epoch 1/30  
    Train loss : 1684.4103461503983  
    Test accuracy 74/660 (11.212121%)  
    Epoch 2/30  
    Train loss : 1528.6197370290756  
    Test accuracy 85/660 (12.878788%)  
    Epoch 3/30  
    Train loss : 1471.6831514835358  
    Test accuracy 87/660 (13.181818%)  
    Epoch 4/30  
    Train loss : 1431.2139061689377  
    Test accuracy 90/660 (13.636364%)  
    Epoch 5/30  
    Train loss : 1437.8142901659012  
    Test accuracy 112/660 (16.969697%)  
    Epoch 6/30  
    Train loss : 1415.9858133792877  
    Test accuracy 115/660 (17.424242%)  
    Epoch 7/30  
    Train loss : 1408.876769900322  
    Test accuracy 112/660 (16.969697%)  
    Epoch 8/30  
    Train loss : 1382.3290008306503  
    Test accuracy 126/660 (19.090909%)  
    Epoch 9/30  
    Train loss : 1359.0357863903046  
    Test accuracy 122/660 (18.484848%)  
    Epoch 10/30  
    Train loss : 1335.406890153885  
    Test accuracy 131/660 (19.848485%)  
    Epoch 11/30  
    Train loss : 1369.4550833702087  
    Test accuracy 132/660 (20.000000%)  
    Epoch 12/30  
    Train loss : 1349.266711473465  
    Test accuracy 116/660 (17.575758%)  
    Epoch 13/30  
    Train loss : 1318.537847518921  
    Test accuracy 173/660 (26.212121%)  
    Epoch 14/30  
    Train loss : 1297.0214554071426  
    Test accuracy 143/660 (21.666667%)  
    Epoch 15/30  
    Train loss : 1289.5315008163452  
    Test accuracy 159/660 (24.090909%)  
    Epoch 16/30  
    Train loss : 1266.0718561410904  
    Test accuracy 149/660 (22.575758%)  
    Epoch 17/30  
    Train loss : 1262.1636182069778  
    Test accuracy 158/660 (23.939394%)  
    Epoch 18/30  
    Train loss : 1216.138608455658  
    Test accuracy 169/660 (25.606061%)  
    Epoch 19/30  
    Train loss : 1234.7398719191551  
    Test accuracy 123/660 (18.636364%)  
    Epoch 20/30  
    Train loss : 1192.7351569533348  
    Test accuracy 167/660 (25.303030%)  
    Epoch 21/30  
    Train loss : 1196.6281063556671  
    Test accuracy 183/660 (27.727273%)  
    Epoch 22/30  
    Train loss : 1173.637413084507  
    Test accuracy 187/660 (28.333333%)  
    Epoch 23/30  
    Train loss : 1146.6372238397598  
    Test accuracy 182/660 (27.575758%)  
    Epoch 24/30  
    Train loss : 1133.4806493520737  
    Test accuracy 186/660 (28.181818%)  
    Epoch 25/30  
    Train loss : 1154.2448362708092  
    Test accuracy 178/660 (26.969697%)  
    Epoch 26/30  
    Train loss : 1127.3913524150848  
    Test accuracy 173/660 (26.212121%)  
    Epoch 27/30  
    Train loss : 1112.3012298941612  
    Test accuracy 213/660 (32.272727%)  
    Epoch 28/30  
    Train loss : 1112.5307074189186  
    Test accuracy 213/660 (32.272727%)  
    Epoch 29/30  
    Train loss : 1100.754581451416  
    Test accuracy 214/660 (32.424242%)  
    Epoch 30/30  
    Train loss : 1068.6898882985115  
    Test accuracy 237/660 (35.909091%)  
    
- final : 3.0 - hyperparameter, model,
    
    Epoch 1/50  
    Train loss : 1793.363133072853  
    Test accuracy 70/660 (10.606061%)  
    Epoch 2/50  
    Train loss : 1551.2929408550262  
    Test accuracy 49/660 (7.424242%)  
    Epoch 3/50  
    Train loss : 1504.8062771558762  
    Test accuracy 100/660 (15.151515%)  
    Epoch 4/50  
    Train loss : 1460.146014213562  
    Test accuracy 103/660 (15.606061%)  
    Epoch 5/50  
    Train loss : 1437.745771765709  
    Test accuracy 95/660 (14.393939%)  
    Epoch 6/50  
    Train loss : 1410.1092879772186  
    Test accuracy 112/660 (16.969697%)  
    Epoch 7/50  
    Train loss : 1397.6455438137054  
    Test accuracy 127/660 (19.242424%)  
    Epoch 8/50  
    Train loss : 1371.1415835618973  
    Test accuracy 150/660 (22.727273%)  
    Epoch 9/50  
    Train loss : 1333.0799448490143  
    Test accuracy 152/660 (23.030303%)  
    Epoch 10/50  
    Train loss : 1320.4952311515808  
    Test accuracy 148/660 (22.424242%)  
    Epoch 11/50  
    Train loss : 1308.4253058433533  
    Test accuracy 151/660 (22.878788%)  
    Epoch 12/50  
    Train loss : 1280.7437851428986  
    Test accuracy 177/660 (26.818182%)  
    Epoch 13/50  
    Train loss : 1255.5697748064995  
    Test accuracy 176/660 (26.666667%)  
    Epoch 14/50  
    Train loss : 1225.0958684682846  
    Test accuracy 216/660 (32.727273%)  
    Epoch 15/50  
    Train loss : 1206.9239443540573  
    Test accuracy 200/660 (30.303030%)  
    Epoch 16/50  
    Train loss : 1213.9880170822144  
    Test accuracy 219/660 (33.181818%)  
    Epoch 17/50  
    Train loss : 1181.6942682266235  
    Test accuracy 221/660 (33.484848%)  
    Epoch 18/50  
    Train loss : 1138.7970014810562  
    Test accuracy 241/660 (36.515152%)  
    Epoch 19/50  
    Train loss : 1113.7802930474281  
    Test accuracy 235/660 (35.606061%)  
    Epoch 20/50  
    Train loss : 1118.654246687889  
    Test accuracy 214/660 (32.424242%)  
    Epoch 21/50  
    Train loss : 1100.40710324049  
    Test accuracy 225/660 (34.090909%)  
    Epoch 22/50  
    Train loss : 1051.9472254514694  
    Test accuracy 233/660 (35.303030%)  
    Epoch 23/50  
    Train loss : 1042.4470603466034  
    Test accuracy 249/660 (37.727273%)  
    Epoch 24/50  
    Train loss : 1012.4233772754669  
    Test accuracy 291/660 (44.090909%)  
    Epoch 25/50  
    Train loss : 1000.8358233571053  
    Test accuracy 310/660 (46.969697%)  
    Epoch 26/50  
    Train loss : 991.0700577795506  
    Test accuracy 295/660 (44.696970%)  
    Epoch 27/50  
    Train loss : 964.8318086862564  
    Test accuracy 249/660 (37.727273%)  
    Epoch 28/50  
    Train loss : 941.2342943847179  
    Test accuracy 264/660 (40.000000%)  
    Epoch 29/50  
    Train loss : 936.9816718995571  
    Test accuracy 276/660 (41.818182%)  
    Epoch 30/50  
    Train loss : 936.9653038084507  
    Test accuracy 288/660 (43.636364%)  
    Epoch 31/50  
    Train loss : 904.3021990954876  
    Test accuracy 297/660 (45.000000%)  
    Epoch 32/50  
    Train loss : 900.0194394290447  
    Test accuracy 266/660 (40.303030%)  
    Epoch 33/50  
    Train loss : 866.0663493275642  
    Test accuracy 277/660 (41.969697%)  
    Epoch 34/50  
    Train loss : 847.7931573092937  
    Test accuracy 293/660 (44.393939%)  
    Epoch 35/50  
    Train loss : 830.5360671877861  
    Test accuracy 326/660 (49.393939%)  
    Epoch 36/50  
    Train loss : 839.0527000427246  
    Test accuracy 325/660 (49.242424%)  
    Epoch 37/50  
    Train loss : 825.6711771786213  
    Test accuracy 324/660 (49.090909%)  
    Epoch 38/50  
    Train loss : 803.4922357797623  
    Test accuracy 331/660 (50.151515%)  
    Epoch 39/50  
    Train loss : 791.5872900784016  
    Test accuracy 352/660 (53.333333%)  
    Epoch 40/50  
    Train loss : 769.6625090166926  
    Test accuracy 319/660 (48.333333%)  
    Epoch 41/50  
    Train loss : 775.2774451971054  
    Test accuracy 322/660 (48.787879%)  
    Epoch 42/50  
    Train loss : 757.9739295393229  
    Test accuracy 338/660 (51.212121%)  
    Epoch 43/50  
    Train loss : 746.7388896644115  
    Test accuracy 342/660 (51.818182%)  
    Epoch 44/50  
    Train loss : 743.3831364661455  
    Test accuracy 341/660 (51.666667%)  
    Epoch 45/50  
    Train loss : 719.4649023413658  
    Test accuracy 361/660 (54.696970%)  
    Epoch 46/50  
    Train loss : 711.2443339377642  
    Test accuracy 350/660 (53.030303%)  
    Epoch 47/50  
    Train loss : 683.6624288931489  
    Test accuracy 319/660 (48.333333%)  
    Epoch 48/50  
    Train loss : 685.6579523980618  
    Test accuracy 353/660 (53.484848%)  
    Epoch 49/50  
    Train loss : 661.1975513920188  
    Test accuracy 354/660 (53.636364%)  
    Epoch 50/50  
    Train loss : 651.4748446829617  
    Test accuracy 340/660 (51.515152%)  
    

â†’ epoch 100.

- Epoch 1/100  
    Train loss : 1824.9245611429214  
    Test accuracy 64/660 (9.696970%)  
    Epoch 2/100  
    Train loss : 1558.438700079918  
    Test accuracy 75/660 (11.363636%)  
    Epoch 3/100  
    Train loss : 1505.2105588912964  
    Test accuracy 92/660 (13.939394%)  
    Epoch 4/100  
    Train loss : 1451.9469701051712  
    Test accuracy 111/660 (16.818182%)  
    Epoch 5/100  
    Train loss : 1439.1715704202652  
    Test accuracy 99/660 (15.000000%)  
    Epoch 6/100  
    Train loss : 1416.9947715997696  
    Test accuracy 95/660 (14.393939%)  
    Epoch 7/100  
    Train loss : 1390.8373898267746  
    Test accuracy 130/660 (19.696970%)  
    Epoch 8/100  
    Train loss : 1370.6020830869675  
    Test accuracy 111/660 (16.818182%)  
    Epoch 9/100  
    Train loss : 1346.0591424703598  
    Test accuracy 146/660 (22.121212%)  
    Epoch 10/100  
    Train loss : 1313.2320946455002  
    Test accuracy 145/660 (21.969697%)  
    Epoch 11/100  
    Train loss : 1311.0344195365906  
    Test accuracy 166/660 (25.151515%)  
    Epoch 12/100  
    Train loss : 1284.1567192077637  
    Test accuracy 150/660 (22.727273%)  
    Epoch 13/100  
    Train loss : 1259.0329936742783  
    Test accuracy 185/660 (28.030303%)  
    Epoch 14/100  
    Train loss : 1217.1180562973022  
    Test accuracy 183/660 (27.727273%)  
    Epoch 15/100  
    Train loss : 1200.7038453817368  
    Test accuracy 209/660 (31.666667%)  
    Epoch 16/100  
    Train loss : 1187.8286690711975  
    Test accuracy 223/660 (33.787879%)  
    Epoch 17/100  
    Train loss : 1149.2340105772018  
    Test accuracy 213/660 (32.272727%)  
    Epoch 18/100  
    Train loss : 1129.8083550333977  
    Test accuracy 215/660 (32.575758%)  
    Epoch 19/100  
    Train loss : 1116.8163486123085  
    Test accuracy 229/660 (34.696970%)  
    Epoch 20/100  
    Train loss : 1092.547711968422  
    Test accuracy 240/660 (36.363636%)  
    Epoch 21/100  
    Train loss : 1064.2384468317032  
    Test accuracy 229/660 (34.696970%)  
    Epoch 22/100  
    Train loss : 1071.9822427034378  
    Test accuracy 266/660 (40.303030%)  
    Epoch 23/100  
    Train loss : 1033.000556230545  
    Test accuracy 234/660 (35.454545%)  
    Epoch 24/100  
    Train loss : 1007.7808876037598  
    Test accuracy 228/660 (34.545455%)  
    Epoch 25/100  
    Train loss : 989.3699949383736  
    Test accuracy 264/660 (40.000000%)  
    Epoch 26/100  
    Train loss : 983.7640237808228  
    Test accuracy 253/660 (38.333333%)  
    Epoch 27/100  
    Train loss : 974.8145852386951  
    Test accuracy 251/660 (38.030303%)  
    Epoch 28/100  
    Train loss : 950.4126779139042  
    Test accuracy 251/660 (38.030303%)  
    Epoch 29/100  
    Train loss : 931.176960170269  
    Test accuracy 266/660 (40.303030%)  
    Epoch 30/100  
    Train loss : 937.330546438694  
    Test accuracy 307/660 (46.515152%)  
    Epoch 31/100  
    Train loss : 899.4397295117378  
    Test accuracy 267/660 (40.454545%)  
    Epoch 32/100  
    Train loss : 874.2076793313026  
    Test accuracy 318/660 (48.181818%)  
    Epoch 33/100  
    Train loss : 877.0160283148289  
    Test accuracy 293/660 (44.393939%)  
    Epoch 34/100  
    Train loss : 870.6642246246338  
    Test accuracy 313/660 (47.424242%)  
    Epoch 35/100  
    Train loss : 846.6755894422531  
    Test accuracy 326/660 (49.393939%)  
    Epoch 36/100  
    Train loss : 826.9032298326492  
    Test accuracy 284/660 (43.030303%)  
    Epoch 37/100  
    Train loss : 830.9779167175293  
    Test accuracy 280/660 (42.424242%)  
    Epoch 38/100  
    Train loss : 809.2221790552139  
    Test accuracy 298/660 (45.151515%)  
    Epoch 39/100  
    Train loss : 791.8859551548958  
    Test accuracy 302/660 (45.757576%)  
    Epoch 40/100  
    Train loss : 790.843866750598  
    Test accuracy 334/660 (50.606061%)  
    Epoch 41/100  
    Train loss : 763.3826350569725  
    Test accuracy 318/660 (48.181818%)  
    Epoch 42/100  
    Train loss : 780.3598033487797  
    Test accuracy 321/660 (48.636364%)  
    Epoch 43/100  
    Train loss : 740.0392558425665  
    Test accuracy 322/660 (48.787879%)  
    Epoch 44/100  
    Train loss : 730.7030105739832  
    Test accuracy 315/660 (47.727273%)  
    Epoch 45/100  
    Train loss : 713.504777610302  
    Test accuracy 327/660 (49.545455%)  
    Epoch 46/100  
    Train loss : 698.9630656987429  
    Test accuracy 346/660 (52.424242%)  
    Epoch 47/100  
    Train loss : 692.5018476545811  
    Test accuracy 373/660 (56.515152%)  
    Epoch 48/100  
    Train loss : 696.0330566465855  
    Test accuracy 353/660 (53.484848%)  
    Epoch 49/100  
    Train loss : 673.8017378151417  
    Test accuracy 346/660 (52.424242%)  
    Epoch 50/100  
    Train loss : 676.9622166454792  
    Test accuracy 341/660 (51.666667%)  
    Epoch 51/100  
    Train loss : 650.1986406967044  
    Test accuracy 375/660 (56.818182%)  
    Epoch 52/100  
    Train loss : 655.1659116074443  
    Test accuracy 343/660 (51.969697%)  
    Epoch 53/100  
    Train loss : 632.6462698355317  
    Test accuracy 373/660 (56.515152%)  
    Epoch 54/100  
    Train loss : 618.1452964320779  
    Test accuracy 338/660 (51.212121%)  
    Epoch 55/100  
    Train loss : 612.8296367228031  
    Test accuracy 362/660 (54.848485%)  
    Epoch 56/100  
    Train loss : 601.4944356903434  
    Test accuracy 346/660 (52.424242%)  
    Epoch 57/100  
    Train loss : 612.7612150087953  
    Test accuracy 383/660 (58.030303%)  
    Epoch 58/100  
    Train loss : 581.8777996003628  
    Test accuracy 368/660 (55.757576%)  
    Epoch 59/100  
    Train loss : 584.1199950650334  
    Test accuracy 363/660 (55.000000%)  
    Epoch 60/100  
    Train loss : 556.5121412687004  
    Test accuracy 359/660 (54.393939%)  
    Epoch 61/100  
    Train loss : 531.7213869988918  
    Test accuracy 376/660 (56.969697%)  
    Epoch 62/100  
    Train loss : 530.67882392928  
    Test accuracy 387/660 (58.636364%)  
    Epoch 63/100  
    Train loss : 515.9044560901821  
    Test accuracy 367/660 (55.606061%)  
    Epoch 64/100  
    Train loss : 524.2822845932096  
    Test accuracy 386/660 (58.484848%)  
    Epoch 65/100  
    Train loss : 514.5531164370477  
    Test accuracy 372/660 (56.363636%)  
    Epoch 66/100  
    Train loss : 501.54947379790246  
    Test accuracy 391/660 (59.242424%)  
    Epoch 67/100  
    Train loss : 517.2050592005253  
    Test accuracy 369/660 (55.909091%)  
    Epoch 68/100  
    Train loss : 503.16262786462903  
    Test accuracy 388/660 (58.787879%)  
    Epoch 69/100  
    Train loss : 479.8545928746462  
    Test accuracy 360/660 (54.545455%)  
    Epoch 70/100  
    Train loss : 521.8097349852324  
    Test accuracy 394/660 (59.696970%)  
    Epoch 71/100  
    Train loss : 485.53263778425753  
    Test accuracy 369/660 (55.909091%)  
    Epoch 72/100  
    Train loss : 469.34399265795946  
    Test accuracy 413/660 (62.575758%)  
    Epoch 73/100  
    Train loss : 471.3291343264282  
    Test accuracy 389/660 (58.939394%)  
    Epoch 74/100  
    Train loss : 471.89770578499883  
    Test accuracy 404/660 (61.212121%)  
    Epoch 75/100  
    Train loss : 444.6286405194551  
    Test accuracy 385/660 (58.333333%)  
    Epoch 76/100  
    Train loss : 441.5125313065946  
    Test accuracy 376/660 (56.969697%)  
    Epoch 77/100  
    Train loss : 446.41625471413136  
    Test accuracy 400/660 (60.606061%)  
    Epoch 78/100  
    Train loss : 429.35336108272895  
    Test accuracy 410/660 (62.121212%)  
    Epoch 79/100  
    Train loss : 417.8156395731494  
    Test accuracy 401/660 (60.757576%)  
    Epoch 80/100  
    Train loss : 418.8426562445238  
    Test accuracy 393/660 (59.545455%)  
    Epoch 81/100  
    Train loss : 411.91742643807083  
    Test accuracy 404/660 (61.212121%)  
    Epoch 82/100  
    Train loss : 407.93418421410024  
    Test accuracy 381/660 (57.727273%)  
    Epoch 83/100  
    Train loss : 404.67410549893975  
    Test accuracy 403/660 (61.060606%)  
    Epoch 84/100  
    Train loss : 394.75862002233043  
    Test accuracy 384/660 (58.181818%)  
    Epoch 85/100  
    Train loss : 381.82506050914526  
    Test accuracy 412/660 (62.424242%)  
    Epoch 86/100  
    Train loss : 404.1513839121908  
    Test accuracy 406/660 (61.515152%)  
    Epoch 87/100  
    Train loss : 410.08552837884054  
    Test accuracy 401/660 (60.757576%)  
    Epoch 88/100  
    Train loss : 355.4564240947366  
    Test accuracy 414/660 (62.727273%)  
    Epoch 89/100  
    Train loss : 352.66036033420824  
    Test accuracy 403/660 (61.060606%)  
    Epoch 90/100  
    Train loss : 387.6534295864403  
    Test accuracy 398/660 (60.303030%)  
    Epoch 91/100  
    Train loss : 361.65959601663053  
    Test accuracy 401/660 (60.757576%)  
    Epoch 92/100  
    Train loss : 354.9988845640328  
    Test accuracy 408/660 (61.818182%)  
    Epoch 93/100  
    Train loss : 348.1276288558729  
    Test accuracy 416/660 (63.030303%)  
    Epoch 94/100  
    Train loss : 347.3392227618024  
    Test accuracy 426/660 (64.545455%)  
    Epoch 95/100  
    Train loss : 332.6423339792527  
    Test accuracy 416/660 (63.030303%)  
    Epoch 96/100  
    Train loss : 330.5023876866326  
    Test accuracy 404/660 (61.212121%)  
    Epoch 97/100  
    Train loss : 315.10936888307333  
    Test accuracy 412/660 (62.424242%)  
    Epoch 98/100  
    Train loss : 328.2401503538713  
    Test accuracy 411/660 (62.272727%)  
    Epoch 99/100  
    Train loss : 328.8844599018339  
    Test accuracy 399/660 (60.454545%)  
    Epoch 100/100  
    Train loss : 317.85233451100066  
    Test accuracy 431/660 (65.303030%)