## 1주차

![[Media/ai00-2023-1 2.pdf|ai00-2023-1 2.pdf]]

- 강의실이 좁아서 옮길 수도
- 대면강의는 실습 위주
- 목요일 ~ 수요일이 출석인정기간인듯요
- Google Colab
    - 마크다운 셀
    - 코드 셀
        - 서버에서 실행하는 것이기 때문에 어쩌구…
- 본인 아직도 딕셔너리 잘 쓸줄 모름
- 자동으로 auto 가 적용되는 갓-파이썬
- 나중에 시퀀스랑 딕셔너리랑 클래스만 볼래….

>> google colab : [https://colab.research.google.com/](https://colab.research.google.com/)

## 3주차 - Numpy, Matplotlib

## Numpy

- 파이썬의 수치계산 라이브러리

### 다차원 배열

- n 차원 배열 = n-dimension array = ndarray
    - 1차원 배열 : `a[5] = {1,2,3,4,5};` 같은 거,
    - 2차원 배열 : a[5][3] = {{1,2,3,4,5},{1,2,3,4,5},{1,2,3,4,5}} 같은 거

### Import

```Python
import numpy as np
```

### ndarray

- Numpy 의 자료형
- c언어의 array를 사용하는 것이기 때문에, python의 list 보다 연산이 빠르고 메모리 공간을 절약한다.

```Python
x = np.ndarray([1,2,3])
print(type(x)) # numpy.ndarray
print(x) # [1,2,3]
```

- np.array()
    
    - 2차원 배열 생성 : [[ 1, 2],[3,4]] 과 같이 생성
    
    ```Python
    # 1a.3
    x = np.array([[1, 2],[3, 4]])
    
    print(x)
    ```
    
- dtype 속성
    
    - 알아서 지정하기는 한데
    
    ```Python
    # 1a.4
    x = np.array([1,2,3,4], dtype=np.int64)
    print(x)
    print(x.dtype)
    
    x = np.array([1.0,2.0,3.0,4.0], dtype = np.float64)
    print(x)
    print(x.dtype)
    ```
    
- ndarray.shape
    
    - 배열의 모양
    - ex) [[1,2,3],[4,5,6]] 일 경우 (2,3)
    
    ```Python
    # 1a.5
    x = np.array([1,2,3,4])
    x.shape
    ```
    
- 다양한 생성 함수
    
    ```Python
    a = np.zeros((2,2)) # Create an array of all zeros
    print(a)
    
    b = np.ones((1,2))  # Create an array of all ones
    print(b)
    
    c = np.full((2,2),7) # Create a constant array
    print(c)
    
    d = np.eye(2) # Create a 2x2 identity matrix
    print(d)
    
    e = np.random.random((2,2)) # Create an array filled with random values
    print(e)
    ```
    

### arange()

- np.arange( start, stop, step )
    
    - start ~ stop 을 step 만큼 띄어서 aray 생성
    
    ```Python
    # 1a.8
    
    a = np.arange(10)
    b = np.arange(5,10)
    c = np.arange(3,10,2)
    
    print(a)
    print(b) # [5,6,7,8,9]
    print(c) # [3,5,7,9]
    
    print(type(a))
    ```
    

### Array Math

- 형상이 같으면 같은 위치의 element 끼리 연산을 수행
    
    ```Python
    # 1a.9
    x = np.array([1.0,2.0,3.0])
    y = np.array([2.0, 4.0, 6.0])
    
    print(x + y)
    print(x - y)
    print(x * y)
    print(x / y)
    ```
    
- scalar 연산은 모든 elements 와 연산 수행
    
    ```Python
    # 1a.11
    x = np.array([1.0, 2.0, 3.0])
    
    print(x + 1.0)
    print( x / 2.0)
    ```
    
- dot() : 1차원은 내적, 2차원은 행렬곱을 수행한다.
    
    ```Python
    # 1a.13
    
    x = np.array([[1,2],
                  [3,4]])
    y = np. array([[5,6],[7,8]])
    
    v = np.array([9,10])
    w = np.array([11,12])
    
    print(np.dot(v,w)) # 219 = 내적
    print(np.dot(x,v))
    print(np.dot(x,y))
    ```
    

### Shape and Reshape

- 이미 존재하는 nparray를 지정한 shape으로 변경

```Python
# 1a.14

# np.reshape(newshape) << np : ndarray

y = np.arange(24)
print(y)

y = y.reshape(3,8) # newshape : int or int tuple
print(y)
```

```Python
# 1a.15

# unknown dimension = -1
# 알아서 추정하라는 뜻

print(y)
print( y.reshape(2,-1))   # -1 -> 12
print( y.reshape(-1,6))   # -1 -> 4
```

### Broadcasting

```Python
# 1a.16

# 형상이 다른 배열끼리의 계산
  # 작은 배열을 큰 배열만큼 확대한 후 연산

A = np.array([[1,2]
              ,[3,4]])
B = np.array([10,20])

print(A * B) # B -> [[10,20],[10,20]]
```

### Indexing

```Python
# 1a.18

# indexing 방법은 C언어와 같다

X = np.array([[51,55],
              [14,19],
              [0,4]])

print(X, '\n')
print(X[0],'\n')
print(X[0][1],'\n')
```

### Slicing

```Python
# 1a.19

# list 와 비슷한 방식, 차원의 길이를 나누는 방식

a = np.array([[1,2,3,4],
              [5,6,7,8],
              [9,10,11,12]])
print(a, '\n')

b = a[:2,1:3]
print(b)
```

```Python
# 1a.20

# 슬라이싱은 같은 주소를 공유하는 것에 가깝다

print(a, '\n')

b[0,0] = 77 \#b[0,0] = a[0,1], 같은 데이터 저장
print(a)
```

### Loops

```Python
# 1a.21

import numpy as np

a = np.array([ [1,2,3,4], [5,6,7,8], [9,10,11,12]])

for row in a:
    print('-', row) # 각 row를 출력

# - [1,2,3,4]
# - [5,6,7,8]
# - [9,10,11,12]
```

### Advanced Indexing

```Python
# 1a.22

# integer array indexing

X = np.array([[51,55],
              [14,19],
              [0,4]])

Y = X.flatten() # X를 1차원으로 만들어 Y에 넣는다

print(Y)
print(Y[np.array([0,2,4])]) # index 가 0,2,4인 값을 가져오기
```

```Python
# 1a.23

# Boolran array indexing
print(X,'\n')

z = X > 15 # 각 element 에 대해 연산 수행
print(z, '\n') 

print( X[z], '\n') # z = True 인 경우만

[[51 55]
 [14 19]
 [ 0  4]] 

[[ True  True]
 [False  True]
 [False False]] 

[51 55 19]
```

### Data type

```Python
# 1a.24

x = np.array([1,2]) # 내부가 정수이면
print(x.dtype) # -> np.int64

x = np.array([1.0, 2,0]) # 내부가 실수이면
print(x.dtype) # -> np.float64

x = np.array([1, 2], dtype = np.int64) # Force a particular datatype
print(x.dtype)
```

## Matplotlib

- 이거 진짜 객체 생성 안해도 됨?????

### import

```Python
# 1a.25

import matplotlib.pyplot as plt
```

### 단순한 그래프 그리기

```Python
# 1a.26

# 직선 그래프

plt.plot([1,2,3,4]) # (x,y) -> (index,list)
plt.show()

# 데이터 추가

plt.plot([1,2,3,4,5,2,8,3,7]) # (x,y) -> (index,list)
plt.show()

# x축 값 추가

plt.plot( [10, 20, 30, 40, 50, 60],[1,2,3,4,3,3]) # X -> Y
plt.show()

# sin 함수 그래프

x = np.arange(0, 6, 0.1) # 0~5.9 까지 0.1만큼
y = np.sin(x)

plt.plot(x,y)
plt.show()

# clip으로 y값 범위 제한하기

x = np.arange(0, 6, 0.1)
y = np.log(x)

y = np.clip( y, -0.5, 0.5) # -0.5 ~ 0.5 로 제한

plt.plot(x,y)
plt.show()

# sin() 외에도 exp(), log(), tanh(), fabs(), clip()...
# fabs() - 부동 소수점 절대값 연산
# clip() - (arr, min, max) - arr의 min보다 작은 값을 min으로 바꾸고, max보다 큰 값을 max로 바꾼다.
```

### 다른 기능들

```Python
# 1a.31

# 다른 기능들

x = np.arange(0,6,0.1)
y1 = np.sin(x)
y2 = np.cos(x)

plt.plot(x, y1, label='sin') # line label
plt.plot(x, y2, linestyle='--', label = 'cos') # linestyle - '--', '-.', ':','-'

plt.xlabel('x') # x-axis label
plt.ylabel('y') # y-axis label

plt.title('sin & cos') # graph title

plt.legend()

plt.show()
```

[[🔗 LINK]](https://zephyrus1111.tistory.com/17) — [기타] Matplotlib 선 종류(Line style) 지정

### 이미지 표시

```Python
# 이미지 -> ndarray : imread( URL )
# ndarray -> 이미지 : imshow( ndarray )
# 이미지 주소: 'http://esohn.be/images/yonsei.png'

img = plt.imread('http://esohn.be/images/yonsei.png')

plt.imshow(img)
plt.show()

plt.imshow(img, interpolation = 'bilinear')
plt.axis('off')
plt.show()
```

  

## 4주차 : 퍼셉트론

## 1. 퍼셉트론

- 뉴런의 동작 원리를 모델링
- 다수의 신호를 입력받아 하나의 신호를 출력
    - 흐른다 : 1
    - 안 흐른다 : 0
- 입력으로 2개의 신호를 받는 퍼셉트론
    
    - Inputs : x1, x2
    - Output : y
    - Weights : w1, w2
    - x1w1 + x2w2 > threshold = 1
    
    ![[Media/Untitled 26.png|Untitled 26.png]]
    

## 2. 단순 논리회로

### AND gate

- 입력 2개, 출력 1개
- truth table -> 모두 1일 때만 1 출력
- 퍼셉트론?

### NAND gate

- not AND gate

### OR gate

- 둘 중 하나라도 1이면 1 출력

### Rosenblatt's Algorithm

- weight를 랜덤으로 잡음
- 예측이 1인데 값이 0이 나왔으면 weight를 증가
- 예측이 0인데 값이 1이 나왔으면 weight를 감소
- 에러가 줄어들 때까지 계속 반복

```Python
# 2.1
# AND 구현

def AND(x1,x2):
  w1, w2, theta = 1.0, 1.0, 1.0

  tmp = w1*x1 + w2*x2 # sigma(가중치 * input)

  if tmp <= theta:
    return 0
  elif tmp > theta:
    return 1

print(AND(0, 0)) # 0
print(AND(1, 0)) # 0
print(AND(0, 1)) # 0
print(AND(1, 1)) # 1
```

### Bias 도입

- 나중을 위해 다른 방식으로 수정
- (-theta) = bias
- 0보다 크거나 작음을 판단의 기준으로 삼을 수 있습니다.

```Python
# 2.2
import numpy as np

x = np.array([0,1])
w = np.array([0.5,0.5])
b = -0.7

print(w*x) # 0, 0.5
print(np.sum(w*x)) # 0.5
print(np.sum(w*x)+b) # -0.2
```

- bias를 사용해서 AND, NAND, OR gate를 구현해봅시다

```Python
# 2.3
# AND gate
import numpy as np

def AND(x1,x2):
  x = np.array([x1,x2])
  w = np.array([0.5,0.5])
  b = -0.7

  tmp = np.sum(w*x) + b
  if tmp <= 0:
    return 0
  else:
    return 1

for xs in [(0,0),(1,0),(0,1),(1,1)]:
  y = AND(xs[0],xs[1])
  print(str(xs) + " -> " + str(y))
```

```Python
# 2.4
# NAND gate

import numpy as np

def NAND(x1,x2):
  x = np.array([x1,x2])
  w = np.array([-0.5,-0.5])
  b = 0.7
  tmp = np.sum(w*x) + b

  if tmp  <= 0:
    return 0
  else:
    return 1

for xs in [(0,0),(1,0),(0,1),(1,1)]:
  y = NAND(xs[0],xs[1])
  print(str(xs) + " -> " + str(y))
```

```Python
# 2.5
# OR gate

import numpy as np

def OR(x1,x2):
  x = np.array([x1,x2])
  w = np.array([0.5,0.5])
  b = -0.3
  tmp = np.sum(w*x) + b

  if tmp  <= 0:
    return 0
  else:
    return 1

for xs in [(0,0),(1,0),(0,1),(1,1)]:
  y = OR(xs[0],xs[1])
  print(str(xs) + " -> " + str(y))
```

### 가중치와 편향

- weights : 입력신호가 결과에 주는 영향력 - 해당 요소가? 얼마나? 영향을 주는가?
- Bias : 퍼셉트론이 얼마나 쉽게 활성화되는가

## 4. 퍼셉트론의 한계

### 퍼셉트론의 시각화

- 0과 1을 출력하는 영역을 구분
- OR 게이트는 원과 삼각형을 직선으로 나눠야 함
    - 0 - 원 1 - 삼각형일 때 직선으로 나눠
    - XOR은 될까요?
        
        ![[Media/Untitled 1 18.png|Untitled 1 18.png]]
        

### XOR gate

- = Exclusive-OR
- 둘이 달라야 1 출력
- => 직선으로 그래프를 그리는 퍼셉트론은 시각화가 어려움
- Nonlinear 라는 것을 알 수 있다

---

## 5. 다층 퍼셉트론

- 퍼셉트론을 여러 층으로 쌓은 모델
- = Neural Network
    
    ### XOR problem
    
- 여러개의 층을 사용하면 된다 (하나로는 안 되지만)
- NAND + OR -> AND

```Python
# 2.6
# XOR gate

import numpy as np

def XOR(x1,x2):
  s1 = NAND(x1,x2) # 버블을 다는 것과 비슷함
  s2 = OR(x1,x2)
  y = AND(s1,s2)

  return y

for xs in [(0,0),(1,0),(0,1),(1,1)]:
  y = XOR(xs[0],xs[1])
  print(str(xs) + " -> " + str(y))
```

  

## 5주차 : 신경망

## 신경망

- 퍼셉트론의 문제점
    - 1층 퍼셉트론은 선형분리 외의 문제를 해결할 수 없음
    - MLP : 층을 늘리면 해결

## 신경망의 예

- 2층 신경망
    - 입력층 : input layer
    - 은닉층 : hidden layer
    - 출력층 : output layer
- 총 3층이지만 가중치를 갖는 층은 2층이다 (입력층 제외)

## 활성화 함수

- 입력신호의 총합을 출력신호로 변환하는 함수
- 입력 신호의 총합이 활성화를 일으키는지 판단
- 가중치의 합을 변수로 받아 0,1을 출력하는 함수
- $a = b + x_1w_1 + x_2w_2$﻿ 일 때, $h(a)$﻿ = $0$﻿ or $1$﻿ = $y$﻿

### 계단 함수

- 임계값을 경계로 출력이 바뀜
- 퍼셉트론에서의 활성화 함수

### 시그모이드 함수

- 0과 1사이의 연속적인 output
- 시그모이드 : S자 모양
- 신경망부터는 활성화 함수로 이를 사용
    
    ![[Media/Screenshot_2023-05-04_at_5.13.39_PM 2.png|Screenshot_2023-05-04_at_5.13.39_PM 2.png]]
    
- 계단 함수 구현
    
    ```Python
    # 3.1
    # 단순한 형태의 계단 함수
    
    def step_function(x):
      if x > 0 :
        return 1
      else:
        return 0
    
    # 3.2
    # numpy 배열입력 가능 
    
    def step_function(x):
      y = x > 0 # True / False 로 y 에 저장
      return y.astype(np.int64) # 이를 1과 0로 변환
    
      # 아래 과정으로 자세히 설명
    
    import numpy as np
    
    x = np.array([-1.0 , 1.0, 2.0])
    
    print(x) # -1.0, 1.0, 2.0
    
    y = x > 0
    print(y) \#Boolean -> [False, True, True]
    y.astype(np.int64)
    print(y) # int -> [0,1,1]
    
    # 3.5
    
    import matplotlib.pyplot as plt
    
    X = np.arange(-5.0, 5.0, 0.1)
    Y = step_function(X) # 계단 함수 구현
    plt.plot(X,Y)
    plt.ylim(-0.1, 1.1)
    plt.show
    ```
    

### 시그모이드와 계단 함수 비교

차이점

1. 시그모이드는 계단에 비해 부드러운 곡선, 출력이 연속적으로 변화
2. 시그모이드는 0~1, 계단은 0 or 1

공통점

1. 입력이 작을 때는 0, 클 때는 1출력
2. 출력은 항상 0~1
3. 비선형 함수 -> 층을 깊게 할 수 있다
    - 활성화 함수가 선형 함수, cx같은 거라면 3층으로 통과시켜도 결국 c^3x으로 선형 함수와 같아진다
    - 시그모이드, 계단 함수 같은 경우는 비선형이기 때문에 통과시킬 수록 값이 달라진다.

- 시그모이드 함수 구현
    
    ```Python
    # 3.6
    # 시그모이드 함수 구현
    
    def sigmoid(x):
      return 1 / (1 + np.exp(-x))
    ```
    

### ReLU 함수

- 입력이 0을 넘으면 그대로 출력
- 음수이면 0 출력
- ReLU 함수 구현
    
    ```Python
    # 3.9
    # ReLU 함수
    
    def relu(x):
      return np.maximum(0,x) # 0벡터와 x array 에서 위치마다 큰 값을 고르는 거니까...
    ```
    
    +) np.max 와 np.maximum의 차이
    
    [[🔗 LINK]](https://jimmy-ai.tistory.com/70) — [Numpy] 최대값, 최소값 함수 np.max vs np.maximum 차이 (np.min과 np.minimum)
    

## 다차원 배열의 계산

### 다차원 배열

- 신경망을 구성하기 위해 사용, numpy로 구현이 가능
    
    ```Python
    # 3.10
    import numpy as np
    
    A = np.array([1,2,3,4])
    print(A)
    
    print(np.ndim(A)) # 1
    print( A.shape ) #(4,) <- 이건 왜 이렇게 나올까 일차원 배열은 그냥 이렇게 나온다
    print( A.shape[0]) # 4
    
    # 3.11
    B = np.array([[1,2],[3,4],[5,6]])
    print(B)
    
    print( np.ndim(B)) 
    print( B.shape)
    ```
    

### 신경망에서의 행렬곱

![[Media/Screenshot_2023-05-04_at_5.39.03_PM 2.png|Screenshot_2023-05-04_at_5.39.03_PM 2.png]]

```Python
# 3.14

X = np.array([1,2]) # input
print(X.shape)

W = np.array([[1,3,5],[2,4,6]]) # 가중치
print(W.shape)

Y = np.dot(X,W)
print(Y)
```

## 3층 신경망 구현

### 3층 신경망

- 입력층 - 2개
- 첫번째 은닉층 - 3개
- 두번째 은닉충 - 2개
- 출력층 - 2개

### 표기법 설명

- 가중치 오른쪽 위 : n층의 가중치
- 가중치 오른쪽 아래 : 다음층 인덱스, 앞 층 인덱스
    
    ![[Media/Screenshot_2023-05-04_at_5.46.55_PM 2.png|Screenshot_2023-05-04_at_5.46.55_PM 2.png]]
    

```Python
# 3.15
# 첫번째 - 입력층 -> 은닉층1
X = np.array([1.0, 0.5])
W1 = np.array([[0.1,0.3,0.5],[0.2,0.4,0.6]]) # 2->3 이니까 3x2
B1 = np.array([0.1,0.2,0.3])

A1 = np.dot(X,W1) + B1
print(A1)

Z1 = sigmoid(A1)
print(Z1)

# 두번재 - 은닉층1 -> 은닉층2
W2 = np.array([[0.1,0.4],[0.2,0.5],[0.3,0.6]]) # 3->2 이니까 2x3 
B2 = np.array([0.1,0.2])

print(Z1.shape)
print(W2.shape)
print(B2.shape)

A2 = np.dot(Z1,W2) + B2
Z2 = sigmoid(A2)

print(A2)
print(Z2)

# 은닉층 -> 출력층도 똑같이 하시면 됩니다
```

- 하나로 구현해봅시다
    
    ```Python
    # 3.20
    
    # 신경망 초기화 (가중치와 편향)
    def init_network():
      network = {}
      network['W1'] = np.array([[0.1, 0.3, 0.5], [0.2, 0.4, 0.6]])
      network['b1'] = np.array([0.1, 0.2, 0.3])
      network['W2'] = np.array([[0.1, 0.4], [0.2, 0.4], [0.3, 0.6]])
      network['b2'] = np.array([0.1, 0.2])
      network['W3'] = np.array([[0.1, 0.3], [0.2, 0.4]])
      network['b3'] = np.array([0.1, 0.2])
      return network
    
    # 네트워크를 열심히 돌린다
    def forward( network, x ):
      W1, W2, W3 = network['W1'], network['W2'], network['W3']
      b1, b2, b3 = network['b1'], network['b2'], network['b3']
      a1 = np.dot(x, W1) + b1
      z1 = sigmoid(a1)
      a2 = np.dot(z1, W2) + b2
      z2 = sigmoid(a2)
      a3 = np.dot(z2, W3) + b3
      y = identity_function(a3)
      return y
      
    network = init_network()
    x = np.array([1.0, 0.5])
    y = forward( network, x )
    print(y)
    ```
    

### 출력층 설계

---

### 지도학습

- 분류
    - 미리 정의된 클래스 중 하나를 예측하는 것
        - 스팸메일, 정상메일 분류
        - 손글씨 분류
        - 소프트맥스 함수 예측
- 회귀
    - 연속적인 수를 예측하는 것
        - 기상데이터로 내일 기온 예측
        - 공부시간으로 성적 예측
        - 항등 함수

### 항등 함수

- 입력을 그대로 출력

### 소프트맥스 함수

![[Media/Screenshot_2023-05-04_at_5.54.11_PM 2.png|Screenshot_2023-05-04_at_5.54.11_PM 2.png]]

- 클래스 분류를 위해 마지막 출력값을 정규화 하는 함수
- 모든 값의 합이 1이기 때문에 확률 수치화에 유리
- 식에서 알 수 있듯이 모든 입력값에 영향을 받음

```Python
# 1D 입력을 처리할 수 있는 소프트맥스 함수

def softmax(a): # array를 받는다
	c = np.max(a) # 오버플로를 막기 위해 array에서 제일 큰 값을 빼줍니다
  exp_a = np.exp(a-c)
  sum_exp_a = np.sum(exp_a)
  y = exp_a / sum_exp_a

  return y

# 2D 입력을 처리할 수 있는 소프트맥스 함수

def softmax(a):
    if a.ndim >= 2:
        c = np.max( a, axis=1, keepdims=True ) # 첫번째 축을 기준으로 하는게 무슨 말일까
        exp_a = np.exp(a - c)
        sum_exp_a = np.sum( exp_a, axis=1, keepdims=True )
    else:
        c = np.max(a)
        exp_a = np.exp(a - c)
        sum_exp_a = np.sum(exp_a)

    return exp_a / sum_exp_a
```

### 출력층의 뉴런수 정하기

- 주로 클래스 수 = 출력층의 뉴런 수

- ex) 0~9로 분류하고 싶으면 출력층의 뉴런은 10개

## 손글씨 분류하기

- 우선 데이터셋을 받고, test용 데이터셋과 train용 데이터 셋을 나눕니다.
- 각 이미지 파일은 28x28=784 크기로 이루어져있으며, 각 비트는 흑백을 표현합니다.

```Python
from sklearn.datasets import fetch_openml
X, y = fetch_openml('mnist_784', version=1, return_X_y=True)

X = X.values.astype(np.uint8)
y = y.values.astype(np.uint8)

x_train = X[:60000] # 훈련용 60000개
t_train = y[:60000] # 훈련용 정답 60000개
x_test = X[60000:]
t_test = y[60000:]

print(x_train.shape) # 각 이미지 파일 정보
print(t_train.shape) # 정답
print(x_test.shape) # test할 것
print(t_test.shape)

#(60000, 784)
#(60000,)
#(10000, 784)
#(10000,)
```

- 예시용으로 이미지를 출력해봅시다.

```Python
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.image as mpimg

img = x_train[0] # 훈련용 이미지 중 한 장을 가져옵니다.
label = t_train[0] # 훈련용 첫번째 이미지의 정답을 가져옵니다.

print(img.shape) # (784,)
img = img.reshape(28,28) # 28x28 으로 reshape
print(img.shape) # (28,28)

plt.imshow(img,cmap='gray')
plt.show()
```

- 여러 개 이미지를 출력해봅시다
    - np.random.choice :

```Python
img_count = 5 * 4

fig = plt.figure(figsize=(10,10)) # 10x10인치의 figure를 생성합니다.
indices = np.random.choice(x_train.shape[0],img_count) # x.train의 이미지 60000개 중 cnt만큼 이미지를 선택합니다.

imgs = x_train[indices] # array안에 index array를 넣어도 되나요 되네 신기하다
labels = t_train[indices] # 진짜 신기하네 <- link

for i in range(img_count):
  ax = fig.add_subplot(5,4,i+1)
  ax.set_title(labels[i])
  ax.set_xticks([])
  ax.set_yticks([])
  ax.imshow(imgs[i].reshape(28,28), cmap = 'gray')

plt.show
```

- 신경망을 구현합시다

```Python
import pickle

def get_data():
    
    return x_test / 255, t_test

def init_network():
    with open('sample_weights.pkl', 'rb') as f:
        network = pickle.load(f)
        
    return network

def predict(network, x): # 은닉층이 3개인 5층 신경망입니다
  W1,W2,W3 = network['W1'], network['W2'], network['W3']
  b1,b2,b3 = network['b1'], network['b2'], network['b3']

  a1 = np.dot(x,W1) + b1
  z1 = sigmoid(a1)
  a2 = np.dot(z1,W2) + b2
  z2 = sigmoid(a2)
  a3 = np.dot(z2,W3) + b3

  y = softmax(a3)
  return y

x, t = get_data() # x_test의 채도 정보, 정답을 가져옵니다.
network = init_network() # 네트워크 초기화

accuracy_count = 0 # 정확도 초기화

for i in range(len(x)):
  y = predict(network, x[i]) # 예측값을 네트워크로 계산합니다
  p = np.argmax(y) # 가장 큰 인덱스를 정답으로 예측합니다

  if p == t[i]: # 예측값이 정답과 맞았다면
    accuracy_count += 1 # 정확도 증가

print( 'Accuracy: {}'.format(accuracy_count / len(x)))

print(x.shape) # 10000, 784
print(x[0].shape) # 784,
print(W1.shape) # 784, 50
print(W2.shape) # 50, 100
print(W3.shape) # 50, 10
```

- 배치 처리로 더욱 효율적으로 계산해봅시다
    - 배치 처리는 set을 원하는 간격만큼 잘라 계산하는 것을 말합니다.

```Python
x, t = get_data()
network = init_network()

batch_size = 100
accuracy_count = 0

for i in range(0, len(x), batch_size): # 100만큼 잘라서 
  x_batch = x[i:i+batch_size]
  y_batch = predict(network, x_batch) # 계산을 한 다음에
  p = np.argmax(y_batch, axis=1) # axis는 어떤 축을 기준으로 할건지 (axis=1은 두번째 차원을 기준으로) <- link
  accuracy_count += np.sum(p == t[i:i+batch_size]) # 배열 인덱싱이 사용되엇습니다

print('Accuracy: {}'.format(accuracy_count / len(x)))
```

---

### argmax, range, indexing

```Python
print( list( range(0, 10) ) )
print( list( range(0, 10, 3) ) )

x = np.array([[0.1, 0.8, 0.1],
              [0.3, 0.1, 0.6],
              [0.2, 0.5, 0.3],
              [0.8, 0.1, 0.1]])
print( np.argmax(x, axis=1) )
print( np.argmax(x, axis=0) )

y = np.array([1, 2, 1, 0])
t = np.array([1, 2, 0, 0])
print( y == t )
print( np.sum(y == t) )
```

## 6주차 : 신경망 학습 & 미분

### 데이터 주도 학습

- End-to-end learning
    - 복잡한 학습시스템을 하나의 신경망으로 표현
    - 사람의 두뇌와 비슷한 방식

### Splitting Data

- training set : 훈련용 데이터
- Test set : 성능을 평가하기 위한 데이터 (객관적)
- 나누는 이유는 범용적으로 사용하기 위함
- Overfitting : 특정 데이터셋에 지나치게 최적화된 상태

---

### 손실 함수

- 손실함수 : 신경망이 학습할 수 있도록 해주는 지표, 작아져야 함
- MSE : 회귀 문제에서 손실 함수로 사용
    
    - 예측값과 목표값의 차이의 제곱에 대한 평균
    
    ```Python
    def mse_loss(y_hat, y):
      return np.mean( (y- y_hat)**2) # 예측값과 목표값의 차이의 제곱에 대한 평균
    ```
    
- Cross-Entropy
    
    - 분류 문제의 성능을 측정
    - 예측의 정확도에 따라 값이 달라짐 (가까울 수 록 loss 값이 낮아짐)
    - 정답 * 정답 클래스 prediction의 로그값
    
    ```Python
    def ce_loss(y_hat,y):
      delta = 1e-7 # 로그에 0 들어가는 경우를 방지하기 위해서 작은 델타를 더하기
      return -np.sum(y*np.log(y_hat+delta)) # (정답)*(정답 예측의 로그값)
    ```
    
- one-hot encoding
    - 정답 인덱스는 1, 나머지는 0인 인코딩 방식
    - 위의 ce_loss 에서 사용

### 미니배치 학습

- 훈련 데이터 모두에 대한 손실 함수 값을 구해야 함
- 모든 훈련 데이터에 대한 손실 함수의 합을 구하는 법 : 각각의 출력값에 ce_loss 적용후 평균내기
- 하지만 데이터가 많으면 몹시 느려진다
- 훈련 데이터 일부를 골라서 전체의 근사치로 이용
    - 일부를 미니배치라고 합니다.
- 데이터셋 가져오기

```Python
X = X.values.astype(np.uint8)
y = y.values.astype(np.uint8)

# one-hot encoding
y = np.eye(10)[y] # one-hot encoding 이 들어간 2차원 배열을 만든다. 이제 각각의 이미지에 one-hot encoding이 적용됐네요!

x_train = X[:60000]
x_test = X[60000:]
y_train = y[:60000]
y_test = y[60000:]

print(x_train.shape) # 60000, 784
print(y_train.shape) # 60000, 10
print(x_test.shape) # 10000, 784
print(y_test.shape) # 10000, 10

train_size = x_train.shape[0] # 훈련할 데이터의 사이즈는 60000
batch_size = 10 # 미니배치의 사이즈는 10
batch_mask = np.random.choice(train_size, batch_size) \#60000개 중에서 배치 사이즈 만큼 뽑기
x_batch = x_train[batch_mask] # 훈련 데이터에서 해당 인덱스의 데이터를 불러와 미니배치를 만든다
y_batch = y_train[batch_mask]
```

- 미니배치용 ce_loss

```Python
def ce_loss( y_hat, y ):
    if y_hat.ndim == 1:
        y = y.reshape(1, y.size)
        y_hat = y_hat.reshape(1, y_hat.size)
    
    batch_size = y_hat.shape[0]
    return - np.sum( y * np.log(y_hat + 1e-7)) / batch_size
```

### 왜 손실함수를 사용할까?

- 높은 정확도를 위해 사용합니다.
    - 정확도를 기준으로 사용할 경우, 정확도는 연속적이지 않으며 가중치의 변화에 따라 정확도의 변화는 미미한 경우가 많습니다.
    - 손실함수는 반대로 연속적이며, 미미한 가중치의 변화에도 즉각적으로 반응하기 때문에, 기준으로 삼기 좋습니다.

---

## 미분

- 변수의 변화량에 따른 함숫값의 변화량

```Python
def num_diff(f,x): # 함수, x값을 받아 미분을 진행
  h = 1e-4
  return (f(x-h)-f(x))/ h
```

- 해석적 미분 (진짜 접선에서의 기울기) ≠ 수치미분 (대충 h에 값 때려넣은 미분)

### 기울기

- 기울기는 모든 변수의 편미분으로 이루어진 벡터입니다.
- 기울기를 구하는 함수

```Python
def num_grad(f, x):
    h = 1e-4
    grad = np.zeros_like(x)  # x와 형상이 같고 원소가 모두 0인 배열 생성
    
    for idx in range(x.size):

        # 기존 x 값을 보관
        temp = x[idx]

        # f(x+h) 계산
        x[idx] = temp + h
        fxh1 = f(x)

        # f(x-h) 계산
        x[idx] = temp - h
        fxh2 = f(x)
        
        # 그리고 기울기 계산
        grad[idx] = (fxh1 - fxh2) / (2*h)

        # 다시 x를 원래대로
        x[idx] = temp
        
    return grad # 함수 f에 대해서 x에 대한 기울기 값이 저장된 배열입니다 
```

### Gradient Decent

- 최적의 가중치를 찾기 위해 기울기 방향으로 이동
- Global minimum : 가장 작은값
- Local minimum : 극소값
- Saddle Point : 어느 방향에서 maximum, 다른 방향에서는 minimum이 되는 값
- Plateau : 평평한 곳
    
    ![[Media/Screenshot_2023-05-05_at_3.25.37_PM 2.png|Screenshot_2023-05-05_at_3.25.37_PM 2.png]]
    
- 학습률 : 한 번의 학습으로 얼마나 갱신하는가
    - 너무 작은 경우 갱신되지 않음
    - 너무 큰 경우 발산함
    - 적당한 경우 최소값을 찾을 수 있다

```Python
def gd( f, init_x, lr=0.01, step_num=100 ):
    x = init_x # x 배열 받기
    for i in range(step_num):
        grad = num_grad(f, x)
        x -= lr * grad
        plt.scatter(x[0], x[1]) # 산점도 함수
    return x
```

### Hyperparamater

- 신경망이 학습해 바뀌는 값이 아닌, 사용자가 직접 수정하는 값
    - 노드의 개수, 학습률, 가중치 초기값, 미니배치 사이즈 등
    - 가장 잘 학습이 되는 값을 찾아야 함

### 신경망에서의 기울기

- 가중치에 대한 손실 함수의 기울기 - 손실함수의 최솟값을 찾기 위함

```Python
class SimpleNet:
    def __init__(self):
        self.W = np.random.randn(2, 3)
    
    def predict(self, x):
        return softmax( np.dot(x, self.W) )
    
    def loss(self, x, y):
        y_hat = self.predict(x)

        loss = ce_loss(y_hat, y)
        
        return loss

    def num_grad(self, x, y):
        h = 1e-4
        grad = np.zeros_like( self.W )

        for idx, val in np.ndenumerate( self.W ):

            temp = self.W[idx]

            self.W[idx] = temp + h
            fxh1 = self.loss(x, y)

            self.W[idx] = temp - h
            fxh2 = self.loss(x, y)
            
            grad[idx] = (fxh1 - fxh2) / (2*h) # loss 함수의 기울기

            self.W[idx] = temp
            
        return grad
```

---

## 신경망 학습의 절차 - SGD

1. 미니배치
    1. 데이터에서 무작위로 가져옴
2. 기울기 산출
    1. 각 가중치의 기울기를 구함
3. 매개변수 갱신
    1. 가중치를 기울기 방향으로 조금 갱신
4. 반복
    1. 원하는 결과가 나올때까지 갱신

```Python
class TwoLayerNet:
    def __init__(self, i_size, h_size, o_size, init_std=0.01):
        self.params = {}
        self.params['W1'] = init_std * np.random.randn(i_size, h_size)
        self.params['b1'] = np.zeros(h_size)
        self.params['W2'] = init_std * np.random.randn(h_size, o_size)
        self.params['b2'] = np.zeros(o_size)

    def predict(self, x):
        W1, W2 = self.params['W1'], self.params['W2']
        b1, b2 = self.params['b1'], self.params['b2']
    
        a1 = np.dot(x, W1) + b1
        z1 = sigmoid(a1)
        a2 = np.dot(z1, W2) + b2
        return softmax(a2)
        
    def loss(self, x, y):
        y_hat = self.predict(x)
        return ce_loss( y_hat, y )
    
    def accuracy(self, x, y):
        y_hat = self.predict(x)
        y_hat = np.argmax(y_hat, axis=1)
        y = np.argmax(y, axis=1)
        accuracy = np.sum(y_hat == y) / float(x.shape[0])
        return accuracy
        
    def num_grad(self, x, y):
        h = 1e-4
        grads = {}

        for key in ('W1', 'b1', 'W2', 'b2'):
            w = self.params[key]
            grad = np.zeros_like( w )
        
            for idx, val in np.ndenumerate( w ):
                temp = w[idx]

                w[idx] = temp + h
                fxh1 = self.loss(x, y)
                w[idx] = temp - h
                fxh2 = self.loss(x, y)
                grad[idx] = (fxh1 - fxh2) / (2*h)

                w[idx] = temp
            
            grads[key] = grad

        return grads

...
# 갱신
for i in range(iters_num):
  
    # 미니배치 획득
    batch_mask = np.random.choice(train_size, batch_size)
    x_batch = x_train[batch_mask]
    y_batch = y_train[batch_mask]

    # 기울기 계산
    grad = net.num_grad(x_batch, y_batch)

    # 매개변수 갱신
    for key in ('W1', 'b1', 'W2', 'b2'):
        net.params[key] -= learning_rate * grad[key]

    # 학습 경과 기록
    loss = net.loss(x_batch, y_batch)
    train_loss_list.append(loss)
    print('loss: {}'.format(loss))
```

- epoch : 훈련 데이터를 모두 소진한 횟수
- 위의 결과를 epoch 만큼 반복

## 7주차 : 오차역전파법

## 계산 그래프

### 계산 그래프

- 그래프 = 여러 개의 노드 + 노드를 연결하는 에지
- 계산 그래프 : 계산 과정을 노드와 화살표로 표현, 복잡한 계산에서 유용
- 순전파 : 왼쪽 -> 오른쪽으로 계산 진행
- 역전파 : 계산 그래프의 종착점에서 출발점으로 전파 (미분계산)
    
    ![[Media/Screenshot_2023-05-05_at_3.55.22_PM 2.png|Screenshot_2023-05-05_at_3.55.22_PM 2.png]]
    

---

## 연쇄법칙

### 합성함수

- 여러 함수의 중첩으로 구성된 함수
- 계산 그래프는 일종의 합성함수로 생각할 수 있다
- 함성 함수의 미분은 합성 함수를 구성하는 각 함수의 미분의 곱 (chain rule)
    
    ![[Media/Screenshot_2023-05-05_at_3.57.45_PM 2.png|Screenshot_2023-05-05_at_3.57.45_PM 2.png]]
    

---

## 역전파

- 덧셈 노드의 역전파는 하류의 값을 그대로 올려보낸다. (z = x+y의 경우)
- 곱셈 노드의 역전파는 하류의 값에 순전파의 입력신호를 바꾼 것을 곱해서 올라간다. -> 입력 신호를 저장해 두어야 함
    
    ![[Media/Screenshot_2023-05-05_at_3.59.28_PM 2.png|Screenshot_2023-05-05_at_3.59.28_PM 2.png]]
    

---

## 단순한 계층

### 단순 연산 계층

- 곱셈, 덧셈과 같은 단순 노드의 역전파를 테스트
- 신경망을 구성하는 layer 단위의 클래스로 구현
    - multilayer, addlayer
    - forward(), backward()

---

### **단순한 계층 구현 - 곱셈 노드**

```Python
class MultLayer:
    def __init__(self):
      self.x = None
      self.y = None

    def forward(self,x,y): # 순전파의 값 저장
      self.x = x
      self.y = y

      return x*y # 곱셈 노드군

    def backward(self, dout): # 역전파 값 저장 = 편미분값
      dx = dout * self.y # 바로 앞의 미분값 x 다른 노드의 입력값
      dy = dout * self.x

      return dx,dy
```

### 단순한 계층 구현 - 덧셈 노드

```Python
class AddLayer:
    def forward(self,x,y):
      return x+y

    def backward(self,dout):
      return dout,dout
```

### 활성화 함수 계층

- 순전파 입력이 0보다 크면 역전파는 상류의 값을 그대로 하류로 흘림
- 0보다 작다면 역전파는 하류로 신호를 보내지 않음

```Python
class ReLU:
    def __init__(self):
        self.mask = None
    
    def forward(self, x):
        self.mask = (x <= 0)
        out = x.copy()
        out[self.mask] = 0

        return out
    
    def backward(self, dout):
        dout[self.mask] = 0
        dx = dout
        
        return dx
```

### 시그모이드 계층

```Python
class Sigmoid:
    def __init__(self):
      self.out = None

    def forward(self,x):
      out = 1 / (1 + np.exp(-x))
      self.out = out1
      return out

    def backward(self,dout):
      dx = dout * (1.0 - self.out) * self.out # 놀랍게도 시그모이드 편미분값이다

      return dx
```

### Affine / softmax 계층

- affine 계층 - 행렬 곱

```Python
class Affine:
    def __init__(self, W, b):
        self.W = W
        self.b = b
        self.x = None
        self.dW = None
        self.db = None
    
    def forward(self, x):
        self.x = x
        out = np.dot(x, self.W) + self.b # 여기서는 기존과 다르게 b가 각 노드에 정해져 있는 것이 아닌, 한꺼번에 더해집니다
        
        return out
    
    def backward(self, dout):
        dx = np.dot(dout, self.W.T) # 행렬 곱 같은 경우는 이전의 미분 값에 다른 입력 행렬의 transpose를 곱한 값이란다!
        self.dW = np.dot(self.x.T, dout)
        self.db = np.sum(dout, axis=0) # 왜 더하나요? -> 입력층의 사이즈가 N일 경우에는 미분값이 (N,x) 의 사이즈를 가지지만, 편향은 일차원 배열이다 -> sum으로 일차원으로 만듦
        
        return dx
```

- softmax

```Python
class SoftmaxWithLoss:
    def __init__(self):
        self.loss = None
        self.o = None
        self.t = None
        
    def forward(self, x, t):
        self.t = t
        self.o = softmax(x)
        self.loss = ce_loss(self.o, self.t)
    
        return self.loss
    
    def backward(self, dout=1):
        batch_size = self.t.shape[0]
        dx = (self.o - self.t) / batch_size
        
        return dx
```

---

## 오차역전파법

```Python
class TwoLayerNet:
    def __init__(self, i_size, h_size, o_size, init_std = 0.01):
        self.params = {}
        self.params['W1'] = init_std * np.random.randn(i_size, h_size)
        self.params['b1'] = np.zeros(h_size)
        self.params['W2'] = init_std * np.random.randn(h_size, o_size) 
        self.params['b2'] = np.zeros(o_size)

        self.layers = {} # 레이어 딕셔너리 만들기
        self.layers['Affine1'] = Affine(self.params['W1'], self.params['b1'])
        self.layers['Relu1'] = ReLU()
        self.layers['Affine2'] = Affine(self.params['W2'], self.params['b2'])
        self.layers['SoftmaxWithLoss'] = SoftmaxWithLoss()
        
    def predict(self, x):
        for key in ('Affine1', 'Relu1', 'Affine2'): # 순전파로 세 층을 통과시키기
            x = self.layers[key].forward(x)
        
        return x
        
    def loss(self, x, t):
        o = self.predict(x) # 세 층을 통과한 값을 가지고
        return self.layers['SoftmaxWithLoss'].forward(o, t) # 오차값 구하기
    
    def accuracy(self, x, t):
        o = self.predict(x)
        o = np.argmax(o, axis=1) # 가장 높은 값 = 정답예측을 진짜 정답과 비교하기
        if t.ndim != 1: # 정답이 일차원 배열이 아닐 경우
            t = np.argmax(t, axis=1) # 제일 높은 값을 정답으로 정하기
        
        accuracy = np.sum(o == t) / float(x.shape[0])
        return accuracy
        
    def gradient(self, x, t): # 역전파법으로 미분값 구하기
        
        # forward
        self.loss(x, t)

        # backward
        dout = 1
        for key in ('SoftmaxWithLoss', 'Affine2', 'Relu1', 'Affine1'):
            dout = self.layers[key].backward(dout)

        grads = {}
        grads['W1'] = self.layers['Affine1'].dW
        grads['b1'] = self.layers['Affine1'].db
        grads['W2'] = self.layers['Affine2'].dW
        grads['b2'] = self.layers['Affine2'].db
        
        return grads
```

## 8주차 : 최적화

## 최적화

### 확률적 경사 하강법(SGD)

- 가중치에 대한 기울기를 구해 가중치를 기울기 방향으로 조금씩 변화시키는 방법
- 문제에 따라서 비효율적인 때가 있음 - 비등방성 함수에서 탐색 경로가 비효율적

```Python
class SGD:
    def __init__(self, lr=0.01):
        self.lr = lr
        
    def update(self, params, grads):
        for key in params.keys():
            params[key] -= self.lr * grads[key]
```

### 모멘텀

- 관성을 이용하는 방법
    
    ![[Media/Screenshot_2023-05-05_at_8.24.38_PM 2.png|Screenshot_2023-05-05_at_8.24.38_PM 2.png]]
    
- V는 속도, 알파는 속도가 차츰 감소되도록 하는 역할 (0.9)

```Python
class Momentum:
    def __init__(self, lr=0.01, momentum=0.9):
        self.lr = lr
        self.momentum = momentum
        self.v = None
        
    def update(self, param, grad):
        if self.v is None:
            self.v = {}
            for key, val in params.items():
                self.v[key] = np.zeros_like(val)

        for key in params.keys():
            self.v[key] = self.momentum * self.v[key] - self.lr * grads[key]
            params[key] += self.v[key]
```

### AdaGrad

- 학습률을 조정하는 학습법
- 각각의 매개변수의 원소별로 적응적인 학습률을 갖도록 합니다
    
    ![[Media/Screenshot_2023-05-05_at_8.26.42_PM 2.png|Screenshot_2023-05-05_at_8.26.42_PM 2.png]]
    
- h는 기존 기울기값을 제곱하여 계속 더해줌
- 저 기호는 행렬의 원소별 곱을 말함
- 기울기 루트의 역수를 곱해서 학습률의 정도를 업데이트
- Adam = AdaGrad + momentum

```Python
class AdaGrad:

    def __init__(self, lr=0.01):
        self.lr = lr
        self.h = None
        
    def update(self, params, grads):
        if self.h is None:
            self.h = {}
            for key, val in params.items():
                self.h[key] = np.zeros_like(val)
            
        for key in params.keys():
            self.h[key] += grads[key] * grads[key]
            params[key] -= self.lr * grads[key] / (np.sqrt(self.h[key]) + 1e-7) # 0으로 나누기 방지
```

---

## 가중치의 초기값

- 가중치가 크면 오버피팅이 일어나는 경우가 많음
    - 가능한 작은 값으로 시작하자
- 가중치를 모두 같거나 모두 0으로 설정하지 않는 이유

### 은닉층의 활성화값 분포

- 은닉층에서의 활성화값 분포를 알기 위한 실험 = 5층 신경망을 구현해보자
- 가중치의 표준편차가1 일 떄 각층의 활성화값들은 0과 1에 치우쳐서 분포 → 미분값 구할 때 문제

### 기울기 소실

- 역전파의 기울기 값이 점점 작아지다가 사라지는 것
    - 층을 깊게 할 수록 심각한 문제가 된다
- 그래서 표준편차를 작게 놓으면? → 모든 뉴런의 출력이 중간에 치우침

### Xavier 초기값

- 앞 계층의 노드가 n새라면 표준편차가 $1/ \sqrt{n}$﻿인 정규분포로 퍼뜨리는 것

### tanh()의 사용

- 활성화 함수로 사용하면 시그모이드보다 좀더 괜찮아진 분포를 보임

### He 초깃값

- ReLU에 특화된 초깃값으로, ReLU는 음의 영역이 없는 셈(0)이므로 분포가 두배는 넓어야 함
- 표준편차가 $2/ \sqrt{n}$﻿배

---

## 배치 정규화

- 데이터 분포를 정규화하는 과정
- 학습 시 미니배치를 단위로 정규화함 = 데이터 분포가 평균이 0, 분산이 1이 되도록 정규화

![[Media/Screenshot_2023-05-05_at_8.41.31_PM 2.png|Screenshot_2023-05-05_at_8.41.31_PM 2.png]]

- 아무튼 사용했을 때 학습이 잘 된다

```Python
class BatchNormalization:

    def __init__(self, gamma, beta, momentum=0.9, running_mean=None, running_var=None):
        self.gamma = gamma
        self.beta = beta
        self.momentum = momentum
        self.input_shape = None 

        self.running_mean = running_mean
        self.running_var = running_var  
        
        self.batch_size = None
        self.xc = None
        self.std = None
        self.dgamma = None
        self.dbeta = None

    def forward(self, x, train_flg=True):
        self.input_shape = x.shape
        if x.ndim != 2:
            N, C, H, W = x.shape
            x = x.reshape(N, -1)

        out = self.__forward(x, train_flg)
        
        return out.reshape(*self.input_shape)
            
    def __forward(self, x, train_flg):
        if self.running_mean is None:
            N, D = x.shape
            self.running_mean = np.zeros(D)
            self.running_var = np.zeros(D)
                        
        if train_flg:
            mu = x.mean(axis=0)
            xc = x - mu
            var = np.mean(xc**2, axis=0)
            std = np.sqrt(var + 10e-7)
            xn = xc / std
            
            self.batch_size = x.shape[0]
            self.xc = xc
            self.xn = xn
            self.std = std
            self.running_mean = self.momentum * self.running_mean + (1-self.momentum) * mu
            self.running_var = self.momentum * self.running_var + (1-self.momentum) * var            
        else:
            xc = x - self.running_mean
            xn = xc / ((np.sqrt(self.running_var + 10e-7)))
            
        out = self.gamma * xn + self.beta 
        return out

    def backward(self, dout):
        if dout.ndim != 2:
            N, C, H, W = dout.shape
            dout = dout.reshape(N, -1)

        dx = self.__backward(dout)

        dx = dx.reshape(*self.input_shape)
        return dx

    def __backward(self, dout):
        dbeta = dout.sum(axis=0)
        dgamma = np.sum(self.xn * dout, axis=0)
        dxn = self.gamma * dout
        dxc = dxn / self.std
        dstd = -np.sum((dxn * self.xc) / (self.std * self.std), axis=0)
        dvar = 0.5 * dstd / self.std
        dxc += (2.0 / self.batch_size) * self.xc * dvar
        dmu = np.sum(dxc, axis=0)
        dx = dxc - dmu / self.batch_size
        
        self.dgamma = dgamma
        self.dbeta = dbeta
        
        return dx
```

---

## 바른 학습을 위해

### 오버피팅

- 신경망이 특정 데이터셋에 대해서 치우치게 학습된 경우
    - 가중치가 많고 표현력이 떨어지는 모델
    - 훈련 데이터가 적음

### 가중치 감소

- 오버피팅을 억제하기 위해 큰 가중치에 대해 상응하는 큰 패널티를 부과하는 방법
    - 가중치의 각 원소의 제곱들을 더한 것(L2 norm)을 손실함수에 더해 가중치가 커지는 것을 억제한다
    - $L = \gamma W$﻿

### 드롭아웃

- 뉴런을 임의적으로 삭제하면서 학습하는 방법
- 훈련 때는 은닉층의 뉴런을 무작위로 골라 삭제
- 예측 때는 모든 뉴럭을 사용하되 출력에 삭제 비율을 곱해서 출력

---

## 하이퍼파라미터

### 검증 데이터

- 지금까지는 데이터셋을 test set, train set으로 나누어서 test set에 대해 하이퍼파라미터를 조정했지만, 그러면 오버피팅될 가능성이 높음
- 하이퍼파라미터 전용 데이터 - Validition set

### 하이퍼파라미터 최적화

- 우선 범위를 대략적으로 지정하고, 무작위로 값을 골라낸 후 정확도를 평가

1. 파라미터의 값의 범위를 설정
2. 설정된 범위에서 무작위로 추출
3. 하이퍼파라미터 값을 사용해 검증 데이터로 평가
4. 정확도의 결과로 범위를 점차 좁혀나감

## 9주차 : pyTorch

## pyTorch

- python 기반 과학 계산 패키지, GPU 계산을 지원합니다.

### Tensor

- ndarray와 비슷합니다

```Python
x = torch.empty(3,2)

x = torch.rand(3,2)

x = torch.zeros(3,2, dtype=torch.int) # or float

a = torch.ones(5)
print(a)
b = a.numpy()
print(b)
a.add_(1)
print(a)
print(b)
# numpy와 tensor의 메모리 공유

a = np.ones(5)
b = torch.from_numpy(a)
np.add(a, 1, out=a)
print(a) # [2. 2. 2. 2. 2.]
print(b) # tensor([2. 2. 2. 2. 2.])

# reshape은 view함수로 합니다
x = torch.randn(4,4)
print(x.size()) 
y = x.view(16)
print(y.size())
z = x.view(-1,8)
print(z.size())
k = x.view(1,4,4)
print(k.size())

# GPU로 계산하는 법
if torch.cuda.is_available():
  device = torch.device('cuda')
else:
  device - torch.device('cpu')

# 다양한 함수
print(a.mean())
print(a.sum())
print(a.argmax(dim=1))
```

---

## Autograd

- 자동기울기 계산 기능 - 자동으로 이전 계산 결과를 저장합니다.

```Python
x = torch.tensor(2.,requires_grad=True) # 트래킹 활성화
print(x)

out = x*5
print(out)

out.backward() # 기울기 계산
print(x.grad) # grad에 기울기가 저장됨

#트래킹 정지 시키기 = requires grad = False로 만들기
with torch.no_grad(): # 이 구간에서는 트래킹을 하지 않습니다
  print((x*2).requires_grad)
```

- 이를 이용해 네트워크를 만들어 봅시다

```Python
import torch

dtype = torch.float
device = torch.device("cpu")
# device = torch.device("cuda:0") # GPU

# N: 배치사이즈; D_in: 입력 사이즈; H: 은닉층 사이즈; D_out: 출력 사이즈
N, D_in, H, D_out = 64, 1000, 100, 10

# 입력과 출력 위한 랜덤 텐서
x = torch.randn(N, D_in).to(device)
y = torch.randn(N, D_out).to(device)

# 가중치 텐서
# 역전파 때 기울기 계산을 위해 가중치 텐서에는 requires_grad=False 추가
w1 = torch.randn(D_in, H, requires_grad=True).to(device)
w2 = torch.randn(H, D_out, requires_grad=True).to(device)

learning_rate = 1e-6

for t in range(500):
  y_pred = x.mm(w1).clamp(min=0).mm(w2) # ReLU함수를 썼나봐요

	# loss 함수는 MSE네요
  loss = (y_pred - y).pow(2).sum()

  if t % 100 == 99:
    print(t, loss.item()) # 단일 값을 읽어오는 .item()

	# 연산에 이용된 모든 변수의 기울기 구하기
  loss.backward()
	
	# SGD
  with torch.no_grad(): \#False 안하면 기울기로 가중치를 업데이트 해야하는데 grad가 업데이트 되어버림
    w1 -= learning_rate * w1.grad
    w2 -= learning_rate * w2.grad

  w1.grad.zero_()
  w2.grad.zero_()
```

---

## NN Package

- 신경망을 layer 단위로 표현하는 high-level 방식 제공
    
    - Modules : layer 단위로 tensor 입력 후 tensor 출력
    - loss Functions : 손실함수들
    
    ```Python
    # nn package를 이용하여 여러 층으로 정의된 모델 생성
    # nn.Sequential은 다른 모듈을 담을 수 있는 모듈이며 담겨진 모듈은 순서대로 연결
    # Linear 모듈은 곧 Affine 모듈
    model = torch.nn.Sequential(
        torch.nn.Linear(D_in, H),
        torch.nn.ReLU(),
        torch.nn.Linear(H,H),
        torch.nn.ReLU(),
        torch.nn.Linear(H,D_out),
    )
    
    loss_fn = torch.nn.MSELoss() # 손실함수는 MSE 사용
    
    learning_rate = 1e-2
    
    for t in range(500):
    
      y_pred = model(x)
    
      loss = loss_fn(y_pred,y)
    
      if t%100 == 99:
        print(t,loss.item())
    
      model.zero_grad()
      loss.backward()
    
      with torch.no_grad():
        for param in model.parameters():
          param -= learning_rate * param.grad
    ```
    

### torch.optim

- 다양한 최적화 알고리즘 - Adam, AdaGrad…

```undefined
learning_rate = 1e-4

optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)

for t in range(500):

  y_pred = model(x)

  loss = loss_fn(y_pred,y)

  if t%100 == 99:
    print(t,loss.item())

  model.zero_grad()
  loss.backward()

  optimizer.step()
```

### Custom nn Modules

- 상속받아서 더 복잡란 커스텀 모듈 만들기

```Python
# 2층 신경망
# torch.nn.Module을 상속받아 TwoLayerNet 클래스 정의
class TwoLayerNet(torch.nn.Module):
  def __init__(self, D_in, H, D_out):
    super().__init__()
    self.fc1 = torch.nn.Linear(D_in,H)
    self.fc2 = torch.nn.Linear(H,D_out)
  
  def forward(self,x): 
    x = torch.nn. functional.relu(self.fc1(x))
    return self.fc2(x)
```

---

## MNIST

### 화이팅

## 10주차 : 합성곱 신경망(CNN)

## 전체 구조

- 지금까지의 계층 - 완전연결 계층 (FC) - 모든 뉴런이 연결되어 있음
    
    ![[Media/Screenshot_2023-05-05_at_10.21.04_PM 2.png|Screenshot_2023-05-05_at_10.21.04_PM 2.png]]
    

---

## 합성곱 신경망

- CNN 계층에서 Conv - ReLU - (Pooling)
- 출력층에 가까워 질 때 FC - ReLU
- 마지막 출력 계층에서 FC - Softmax
    
    ![[Media/Screenshot_2023-05-05_at_10.25.13_PM 2.png|Screenshot_2023-05-05_at_10.25.13_PM 2.png]]
    

### 완전연결 계층의 문제점(FC)

- 입력 데이터가 1차원 배열 형태로 한정됨
    - 특히나 n차원 데이터를 1차원 데이터로 바꾸는 과정에서 정보가 손실됨
        - 학습의 효율이 떨어짐
        - 정확도를 높이는 데 한계가 있음
- 합성곱 계층은 형상을 유지
    - 들어오는 데이터도 n차원
- 합성곱 계층의 입출력 데이터 - 특징맵
    - 입력 특징 맵
    - 출력 특징 맵

### 합성곱 계층

- 입출력 데이터의 형상 유지
- 이미지의 특징을 효과적으로 유지

- 여러개의 필터로 특징 추출 및 학습 ← ?

- 필터를 파라미터로 공유하기 떄문에 학습 파라미터 갯수가 적음
- 이미지 필터 연산에 사용
    
    ![[Media/Screenshot_2023-05-05_at_10.32.59_PM 2.png|Screenshot_2023-05-05_at_10.32.59_PM 2.png]]
    
- 위처럼 윈도우를 일정 간격으로 이동하며 입력 데이터에 필터를 곱해서 더함 → 출력 데이터
- CNN 에서는 필터의 매개변수가 곧 가중치, 편향은 항상 하나만 존재

### 패딩

- 합성곱 연산을 수행하기 전, 입력 데이터 주변 값을 특정 값(주로 0)으로 채우는 것
    
    ![[Media/Screenshot_2023-05-05_at_10.35.45_PM 2.png|Screenshot_2023-05-05_at_10.35.45_PM 2.png]]
    
    - 위에서는 입력 데이터 (4,4) → 패딩 (5,5) → 합성곱 (3,3) → 출력 데이터 (4,4)
- 입력 데이터의 크기가 고정됩니다

### 스트라이드

- 필터를 적용하는 간격
    - 한칸 씩 이동 - stride 1
    - 두칸 씩 이동 - stride 2

### 패딩, 스트라이드, 출력 크기의 관계

![[Media/Screenshot_2023-05-05_at_10.42.24_PM 2.png|Screenshot_2023-05-05_at_10.42.24_PM 2.png]]

- 예시 문제
    
    ![[Media/Screenshot_2023-05-05_at_10.44.11_PM 2.png|Screenshot_2023-05-05_at_10.44.11_PM 2.png]]
    

### 3차원 데이터의 합성곱 연산

- 2차원 + 채널 = 3차원 데이터 (주로 RGB)
- 입력 채널 수 = 필터의 채널수
- 출력은 2차원이 되는가

### 직육면체 블록

- 3차원 합성곱 연산의 데이터 차원 표기법
    
    ![[Media/Screenshot_2023-05-05_at_10.47.48_PM 2.png|Screenshot_2023-05-05_at_10.47.48_PM 2.png]]
    
- 출력 데이터를 여러개의 채널로 만들고 싶어요
    - 필터를 여러 개 사용합니다 → 필터의 개수 = 출력 데이터의 채널 수
    - 그러면 필터는 4차원 데이터가 되네요….
        
        ![[Media/Screenshot_2023-05-05_at_10.50.49_PM 2.png|Screenshot_2023-05-05_at_10.50.49_PM 2.png]]
        
- 편향을 추가한 합성곱 연산의 흐름
    
    ![[Media/Screenshot_2023-05-05_at_10.52.47_PM 2.png|Screenshot_2023-05-05_at_10.52.47_PM 2.png]]
    

### 배치 처리

- 합성곱 연산에도 배치 처리를 통해 효율을 높일 수 있음
    
    ![[Media/Screenshot_2023-05-05_at_10.56.31_PM 2.png|Screenshot_2023-05-05_at_10.56.31_PM 2.png]]
    

---

## 풀링 계층

### 풀링 계층

- 세로, 가로 방향의 일정 영역을 원소 하나로 집약
    - 최대 풀링 Max Pooling
        - 풀링 영역에서 최댓값을 취함
- 스트라이드는 필터의… +1?
- 특징
    - 학습해야할 매개변수가 없다
    - 채널의 수가 변하지 않는다
    - 입력의 변화에 영향을 적게 받음

---

## 완전연결 계층

### 완전연결 계층

- 마지막 Conv 계층에서의 출력 데이터를 첫번째 FC 계층에 넣어서… 넣을 때는 이차원 데이터로…

---

## CNN 구현

- ConvNet1 : Conv1 + FC1 ← 우리가 만들 것
    
    ![[Media/Screenshot_2023-05-05_at_11.06.54_PM 2.png|Screenshot_2023-05-05_at_11.06.54_PM 2.png]]
    
- Conv2d 모듈을 사용할 수 있어요
    
    - Conv2D(in_channels=C, out_channels=FN, kernel_size=FH)
    
    - Conv가 뭔데용
    
    ```Python
    class ConvNet1(nn.Module):
        def __init__(self, num_classes=10):
            super().__init__()
    
            # nn.Sequential 정의 - 여러 레이어를 순차적으로 그룹화
            self.layer1 = nn.Sequential(
                # Conv2D - conv layer (입력 채널 1, 출력 채널 30, 필터사이즈 5x5, 스트라이드 1....)
                nn.Conv2d(1, 30, kernel_size=5, stride=1, padding=0)
    
                # BatchNorm2d - 배치정규화 layer, 채널은 30
                nn.BaychNorm2d(30),
                nn.ReLU(),
    
                # MaxPool2d - 2x2 max pool
                nn.MaxPool2d(kernel_size=2, stride=2)
            )
    
            # fc layer - 입력 : 12*12*30, 출력 : num_classes
            self.fc = nn.Linear(12*12*30, num_classes)
            
        def forward(self, x):
            # layer1 통과
            out = self.layer1(x)
            # fc layer 입력전 reshape
            out = out.reshape(out.size(0), -1)
            # fc layer 통과
            out = self.fc(out)
            return out
    ```
    
    ```Python
    # train 함수
    def train():
      # 훈련 모드
      model.train()
    
      batch_loss_list = []
      progress = ProgressMonitor(length=len(train_dataset))
    
      for batch, target in train_loader:
        # train batch를 GPU로
        batch, target = batch.to(device), target.to(device)
    
        # 순전파, 손실계산
        output = model(batch)
        loss = loss_func( output, target)
    
        optimizer.zero_grad()
    
        loss.backward()
        optimizer.step()
    
        batch_loss_list.append(loss.item())
        progress.update(batch.shape[0], sum(batch_loss_list)/len(batch_loss_list))
    
    def test():
      model.eval()
    
      correct = 0
    
      with torch.no_grad():
        for batch, target in test_loader:
          batch, target = batch.to(device), target.to(device)
    
          output = model(batch)
          output = torch.argmax(output,1)
    
          correct += (output == target).sum().item()
    
        acc = 100 * float(correct) / len(test_dataset)
        print('Test accracy: {}/{} ({:.2f}%'.format(correct, len(test_dataset), acc))
    ```
    

---

## CNN 시각화

---

## CNN 의 예시

- LeNet
- AlexNet
    - LRN 사용
    - 드롭아웃 사용

  

## Project

- dataset2
    
    Epoch 1/50  
    Train loss : 433.8795620575547  
    Test accuracy 331/660 (50.151515%)  
    Epoch 2/50  
    Train loss : 367.97383131086826  
    Test accuracy 331/660 (50.151515%)  
    Epoch 3/50  
    Train loss : 355.1996685862541  
    Test accuracy 335/660 (50.757576%)  
    Epoch 4/50  
    Train loss : 364.3521046489477  
    Test accuracy 365/660 (55.303030%)  
    Epoch 5/50  
    Train loss : 370.6498119086027  
    Test accuracy 356/660 (53.939394%)  
    Epoch 6/50  
    Train loss : 331.0747189670801  
    Test accuracy 368/660 (55.757576%)  
    Epoch 7/50  
    Train loss : 319.66992551088333  
    Test accuracy 373/660 (56.515152%)  
    Epoch 8/50  
    Train loss : 286.49186509661376  
    Test accuracy 368/660 (55.757576%)  
    Epoch 9/50  
    Train loss : 272.48600285872817  
    Test accuracy 377/660 (57.121212%)  
    Epoch 10/50  
    Train loss : 226.85299442708492  
    Test accuracy 347/660 (52.575758%)  
    Epoch 11/50  
    Train loss : 202.22799311461858  
    Test accuracy 379/660 (57.424242%)  
    Epoch 12/50  
    Train loss : 188.85355841135606  
    Test accuracy 362/660 (54.848485%)  
    Epoch 13/50  
    Train loss : 157.0219695288688  
    Test accuracy 380/660 (57.575758%)  
    Epoch 14/50  
    Train loss : 100.96259290885064  
    Test accuracy 391/660 (59.242424%)  
    Epoch 15/50  
    Train loss : 69.29816795409715  
    Test accuracy 373/660 (56.515152%)  
    Epoch 16/50  
    Train loss : 51.87082632185775  
    Test accuracy 383/660 (58.030303%)  
    Epoch 17/50  
    Train loss : 33.96104884392116  
    Test accuracy 388/660 (58.787879%)  
    Epoch 18/50  
    Train loss : 22.588687931129243  
    Test accuracy 385/660 (58.333333%)  
    Epoch 19/50  
    Train loss : 9.89001023027231  
    Test accuracy 394/660 (59.696970%)  
    Epoch 20/50  
    Train loss : 4.152478828684252  
    Test accuracy 392/660 (59.393939%)  
    Epoch 21/50  
    Train loss : 8.707331527628412  
    Test accuracy 386/660 (58.484848%)  
    Epoch 22/50  
    Train loss : 8.972744699516625  
    Test accuracy 390/660 (59.090909%)  
    Epoch 23/50  
    Train loss : 4.701703791059117  
    Test accuracy 387/660 (58.636364%)  
    Epoch 24/50  
    Train loss : 4.740196201084473  
    Test accuracy 404/660 (61.212121%)  
    Epoch 25/50  
    Train loss : 5.618677305177698  
    Test accuracy 407/660 (61.666667%)  
    Epoch 26/50  
    Train loss : 1.335235112461305  
    Test accuracy 407/660 (61.666667%)  
    Epoch 27/50  
    Train loss : 1.422667796814494  
    Test accuracy 416/660 (63.030303%)  
    Epoch 28/50  
    Train loss : 0.6701745056761865  
    Test accuracy 402/660 (60.909091%)  
    Epoch 29/50  
    Train loss : 0.854424114277208  
    Test accuracy 396/660 (60.000000%)  
    Epoch 30/50  
    Train loss : 0.5669067905482734  
    Test accuracy 408/660 (61.818182%)  
    Epoch 31/50  
    Train loss : 0.4687967082418254  
    Test accuracy 402/660 (60.909091%)  
    Epoch 32/50  
    Train loss : 1.251823444123147  
    Test accuracy 407/660 (61.666667%)  
    Epoch 33/50  
    Train loss : 0.7797148492327324  
    Test accuracy 404/660 (61.212121%)  
    Epoch 34/50  
    Train loss : 0.4607491051974648  
    Test accuracy 401/660 (60.757576%)  
    Epoch 35/50  
    Train loss : 0.45757791616779286  
    Test accuracy 405/660 (61.363636%)  
    Epoch 36/50  
    Train loss : 0.4125841020504595  
    Test accuracy 415/660 (62.878788%)  
    Epoch 37/50  
    Train loss : 1.424494672074161  
    Test accuracy 395/660 (59.848485%)  
    Epoch 38/50  
    Train loss : 0.894138605626722  
    Test accuracy 393/660 (59.545455%)  
    Epoch 39/50  
    Train loss : 0.5815037472120821  
    Test accuracy 402/660 (60.909091%)  
    Epoch 40/50  
    Train loss : 0.433586412385921  
    Test accuracy 391/660 (59.242424%)  
    Epoch 41/50  
    Train loss : 0.4140952911129716  
    Test accuracy 395/660 (59.848485%)  
    Epoch 42/50  
    Train loss : 0.5287650238778951  
    Test accuracy 406/660 (61.515152%)  
    Epoch 43/50  
    Train loss : 0.2785112618275889  
    Test accuracy 406/660 (61.515152%)  
    Epoch 44/50  
    Train loss : 0.32402577356060647  
    Test accuracy 409/660 (61.969697%)  
    Epoch 45/50  
    Train loss : 0.19914430985409126  
    Test accuracy 406/660 (61.515152%)  
    Epoch 46/50  
    Train loss : 0.19978674412959663  
    Test accuracy 406/660 (61.515152%)  
    Epoch 47/50  
    Train loss : 0.10707507464394439  
    Test accuracy 409/660 (61.969697%)  
    Epoch 48/50  
    Train loss : 0.15685593246507779  
    Test accuracy 408/660 (61.818182%)  
    Epoch 49/50  
    Train loss : 0.15535948423723767  
    Test accuracy 408/660 (61.818182%)  
    Epoch 50/50  
    Train loss : 0.12640929552412672  
    Test accuracy 405/660 (61.363636%)  
    
- hyperparameter-first - resnet18 - 4,50,0.001
    
    Epoch 1/50  
    Train loss : 1709.847081899643  
    Test accuracy 78/660 (11.818182%)  
    Epoch 2/50  
    Train loss : 1457.2198804616928  
    Test accuracy 113/660 (17.121212%)  
    Epoch 3/50  
    Train loss : 1362.1738313436508  
    Test accuracy 159/660 (24.090909%)  
    Epoch 4/50  
    Train loss : 1240.6358853578568  
    Test accuracy 198/660 (30.000000%)  
    Epoch 5/50  
    Train loss : 1130.9488495588303  
    Test accuracy 208/660 (31.515152%)  
    Epoch 6/50  
    Train loss : 1004.7225704193115  
    Test accuracy 230/660 (34.848485%)  
    Epoch 7/50  
    Train loss : 851.3898586928844  
    Test accuracy 221/660 (33.484848%)  
    Epoch 8/50  
    Train loss : 678.6424172967672  
    Test accuracy 235/660 (35.606061%)  
    Epoch 9/50  
    Train loss : 559.0368413478136  
    Test accuracy 217/660 (32.878788%)  
    Epoch 10/50  
    Train loss : 394.96942318975925  
    Test accuracy 232/660 (35.151515%)  
    Epoch 11/50  
    Train loss : 304.5309368185699  
    Test accuracy 224/660 (33.939394%)  
    Epoch 12/50  
    Train loss : 188.42359252972528  
    Test accuracy 232/660 (35.151515%)  
    Epoch 13/50  
    Train loss : 161.48146144486964  
    Test accuracy 233/660 (35.303030%)  
    Epoch 14/50  
    Train loss : 132.40016750921495  
    Test accuracy 222/660 (33.636364%)  
    Epoch 15/50  
    Train loss : 104.06399890175089  
    Test accuracy 268/660 (40.606061%)  
    Epoch 16/50  
    Train loss : 84.98558356903959  
    Test accuracy 238/660 (36.060606%)  
    Epoch 17/50  
    Train loss : 81.60152271628613  
    Test accuracy 237/660 (35.909091%)  
    Epoch 18/50  
    Train loss : 61.01945226744283  
    Test accuracy 253/660 (38.333333%)  
    Epoch 19/50  
    Train loss : 51.69554940453963  
    Test accuracy 223/660 (33.787879%)  
    Epoch 20/50  
    Train loss : 60.296447047338006  
    Test accuracy 268/660 (40.606061%)  
    Epoch 21/50  
    Train loss : 42.545033527349005  
    Test accuracy 259/660 (39.242424%)  
    Epoch 22/50  
    Train loss : 57.146607084272546  
    Test accuracy 246/660 (37.272727%)  
    Epoch 23/50  
    Train loss : 38.560218845683266  
    Test accuracy 270/660 (40.909091%)  
    Epoch 24/50  
    Train loss : 38.47810690211918  
    Test accuracy 278/660 (42.121212%)  
    Epoch 25/50  
    Train loss : 40.49320298132079  
    Test accuracy 255/660 (38.636364%)  
    Epoch 26/50  
    Train loss : 35.753498859441606  
    Test accuracy 233/660 (35.303030%)  
    Epoch 27/50  
    Train loss : 29.116213314475317  
    Test accuracy 261/660 (39.545455%)  
    Epoch 28/50  
    Train loss : 28.91523071397387  
    Test accuracy 258/660 (39.090909%)  
    Epoch 29/50  
    Train loss : 34.80598680575349  
    Test accuracy 234/660 (35.454545%)  
    Epoch 30/50  
    Train loss : 34.47565404661145  
    Test accuracy 249/660 (37.727273%)  
    Epoch 31/50  
    Train loss : 25.280880429636454  
    Test accuracy 251/660 (38.030303%)  
    Epoch 32/50  
    Train loss : 28.80112268704397  
    Test accuracy 262/660 (39.696970%)  
    Epoch 33/50  
    Train loss : 27.923733881405497  
    Test accuracy 247/660 (37.424242%)  
    Epoch 34/50  
    Train loss : 26.8798210816567  
    Test accuracy 255/660 (38.636364%)  
    Epoch 35/50  
    Train loss : 33.21862790205341  
    Test accuracy 257/660 (38.939394%)  
    Epoch 36/50  
    Train loss : 24.804143441142514  
    Test accuracy 246/660 (37.272727%)  
    Epoch 37/50  
    Train loss : 20.179840346732817  
    Test accuracy 251/660 (38.030303%)  
    Epoch 38/50  
    Train loss : 16.24657160982315  
    Test accuracy 257/660 (38.939394%)  
    Epoch 39/50  
    Train loss : 19.402670546944137  
    Test accuracy 259/660 (39.242424%)  
    Epoch 40/50  
    Train loss : 27.055525137235236  
    Test accuracy 264/660 (40.000000%)  
    Epoch 41/50  
    Train loss : 14.399680379399797  
    Test accuracy 250/660 (37.878788%)  
    Epoch 42/50  
    Train loss : 14.137002646581095  
    Test accuracy 251/660 (38.030303%)  
    Epoch 43/50  
    Train loss : 23.35528039847304  
    Test accuracy 255/660 (38.636364%)  
    Epoch 44/50  
    Train loss : 21.659646429732675  
    Test accuracy 265/660 (40.151515%)  
    Epoch 45/50  
    Train loss : 12.748209141995176  
    Test accuracy 264/660 (40.000000%)  
    Epoch 46/50  
    Train loss : 25.736757634277637  
    Test accuracy 239/660 (36.212121%)  
    Epoch 47/50  
    Train loss : 16.548289263377228  
    Test accuracy 273/660 (41.363636%)  
    Epoch 48/50  
    Train loss : 17.787550484527856  
    Test accuracy 270/660 (40.909091%)  
    Epoch 49/50  
    Train loss : 19.65188252058215  
    Test accuracy 268/660 (40.606061%)  
    Epoch 50/50  
    Train loss : 15.553027820926218  
    Test accuracy 262/660 (39.696970%)  
    
- hyperparameter - second - resnet18 - 4,30,0.005
    
    Epoch 1/30  
    Train loss : 1849.9632233381271  
    Test accuracy 81/660 (12.272727%)  
    Epoch 2/30  
    Train loss : 1479.5314617156982  
    Test accuracy 130/660 (19.696970%)  
    Epoch 3/30  
    Train loss : 1338.2574572563171  
    Test accuracy 170/660 (25.757576%)  
    Epoch 4/30  
    Train loss : 1221.3547901511192  
    Test accuracy 160/660 (24.242424%)  
    Epoch 5/30  
    Train loss : 1111.760188639164  
    Test accuracy 193/660 (29.242424%)  
    Epoch 6/30  
    Train loss : 999.8287369012833  
    Test accuracy 218/660 (33.030303%)  
    Epoch 7/30  
    Train loss : 854.5187346935272  
    Test accuracy 213/660 (32.272727%)  
    Epoch 8/30  
    Train loss : 742.3231260031462  
    Test accuracy 225/660 (34.090909%)  
    Epoch 9/30  
    Train loss : 570.8776232227683  
    Test accuracy 225/660 (34.090909%)  
    Epoch 10/30  
    Train loss : 474.9924735929817  
    Test accuracy 237/660 (35.909091%)  
    Epoch 11/30  
    Train loss : 333.37734261900187  
    Test accuracy 241/660 (36.515152%)  
    Epoch 12/30  
    Train loss : 302.74999504722655  
    Test accuracy 256/660 (38.787879%)  
    Epoch 13/30  
    Train loss : 197.20883095590398  
    Test accuracy 216/660 (32.727273%)  
    Epoch 14/30  
    Train loss : 139.65518539608456  
    Test accuracy 238/660 (36.060606%)  
    Epoch 15/30  
    Train loss : 113.755049755855  
    Test accuracy 242/660 (36.666667%)  
    Epoch 16/30  
    Train loss : 71.42039379494963  
    Test accuracy 251/660 (38.030303%)  
    Epoch 17/30  
    Train loss : 69.48051309611765  
    Test accuracy 249/660 (37.727273%)  
    Epoch 18/30  
    Train loss : 50.4735193032393  
    Test accuracy 274/660 (41.515152%)  
    Epoch 19/30  
    Train loss : 40.34680068130547  
    Test accuracy 240/660 (36.363636%)  
    Epoch 20/30  
    Train loss : 45.04781518915843  
    Test accuracy 236/660 (35.757576%)  
    Epoch 21/30  
    Train loss : 39.148906511924  
    Test accuracy 253/660 (38.333333%)  
    Epoch 22/30  
    Train loss : 23.605487315100618  
    Test accuracy 252/660 (38.181818%)  
    Epoch 23/30  
    Train loss : 21.199476290272287  
    Test accuracy 252/660 (38.181818%)  
    Epoch 24/30  
    Train loss : 34.25592986674019  
    Test accuracy 265/660 (40.151515%)  
    Epoch 25/30  
    Train loss : 22.37580777496987  
    Test accuracy 261/660 (39.545455%)  
    Epoch 26/30  
    Train loss : 17.42484427744239  
    Test accuracy 272/660 (41.212121%)  
    Epoch 27/30  
    Train loss : 32.63602443760465  
    Test accuracy 247/660 (37.424242%)  
    Epoch 28/30  
    Train loss : 29.996329996582062  
    Test accuracy 262/660 (39.696970%)  
    Epoch 29/30  
    Train loss : 17.41629193332119  
    Test accuracy 252/660 (38.181818%)  
    Epoch 30/30  
    Train loss : 18.41998718889954  
    Test accuracy 263/660 (39.848485%)  
    
- 2.0 (hyperparameter 4,30,0.01 데이터 전처리
    
    Epoch 1/30  
    Train loss : 1863.2882692813873  
    Test accuracy 43/660 (6.515152%)  
    Epoch 2/30  
    Train loss : 1572.0648002624512  
    Test accuracy 46/660 (6.969697%)  
    Epoch 3/30  
    Train loss : 1501.6109511852264  
    Test accuracy 63/660 (9.545455%)  
    Epoch 4/30  
    Train loss : 1464.4967592954636  
    Test accuracy 105/660 (15.909091%)  
    Epoch 5/30  
    Train loss : 1432.4523386955261  
    Test accuracy 127/660 (19.242424%)  
    Epoch 6/30  
    Train loss : 1414.1929708719254  
    Test accuracy 99/660 (15.000000%)  
    Epoch 7/30  
    Train loss : 1391.9070833921432  
    Test accuracy 143/660 (21.666667%)  
    Epoch 8/30  
    Train loss : 1348.4420142173767  
    Test accuracy 128/660 (19.393939%)  
    Epoch 9/30  
    Train loss : 1315.5391248464584  
    Test accuracy 146/660 (22.121212%)  
    Epoch 10/30  
    Train loss : 1296.9601438045502  
    Test accuracy 149/660 (22.575758%)  
    Epoch 11/30  
    Train loss : 1294.992491722107  
    Test accuracy 152/660 (23.030303%)  
    Epoch 12/30  
    Train loss : 1263.5051754713058  
    Test accuracy 167/660 (25.303030%)  
    Epoch 13/30  
    Train loss : 1241.1774116754532  
    Test accuracy 187/660 (28.333333%)  
    Epoch 14/30  
    Train loss : 1203.5502259731293  
    Test accuracy 156/660 (23.636364%)  
    Epoch 15/30  
    Train loss : 1190.58535695076  
    Test accuracy 165/660 (25.000000%)  
    Epoch 16/30  
    Train loss : 1170.852182149887  
    Test accuracy 207/660 (31.363636%)  
    Epoch 17/30  
    Train loss : 1135.143336892128  
    Test accuracy 203/660 (30.757576%)  
    Epoch 18/30  
    Train loss : 1133.9858967065811  
    Test accuracy 234/660 (35.454545%)  
    Epoch 19/30  
    Train loss : 1109.5767900943756  
    Test accuracy 203/660 (30.757576%)  
    Epoch 20/30  
    Train loss : 1103.0872507095337  
    Test accuracy 206/660 (31.212121%)  
    Epoch 21/30  
    Train loss : 1059.646414399147  
    Test accuracy 190/660 (28.787879%)  
    Epoch 22/30  
    Train loss : 1049.0566716194153  
    Test accuracy 250/660 (37.878788%)  
    Epoch 23/30  
    Train loss : 1026.284754216671  
    Test accuracy 242/660 (36.666667%)  
    Epoch 24/30  
    Train loss : 1002.0206743478775  
    Test accuracy 250/660 (37.878788%)  
    Epoch 25/30  
    Train loss : 1000.1578423082829  
    Test accuracy 254/660 (38.484848%)  
    Epoch 26/30  
    Train loss : 964.2373849749565  
    Test accuracy 261/660 (39.545455%)  
    Epoch 27/30  
    Train loss : 968.1672407388687  
    Test accuracy 247/660 (37.424242%)  
    Epoch 28/30  
    Train loss : 953.3905048966408  
    Test accuracy 247/660 (37.424242%)  
    Epoch 29/30  
    Train loss : 930.5266073942184  
    Test accuracy 289/660 (43.787879%)  
    Epoch 30/30  
    Train loss : 914.8646373748779  
    Test accuracy 298/660 (45.151515%)  
    
- 2.1 (model - resnet50)
    
    Epoch 1/30  
    Train loss : 2235.8202731609344  
    Test accuracy 53/660 (8.030303%)  
    Epoch 2/30  
    Train loss : 1617.0080437660217  
    Test accuracy 61/660 (9.242424%)  
    Epoch 3/30  
    Train loss : 1564.0348501205444  
    Test accuracy 65/660 (9.848485%)  
    Epoch 4/30  
    Train loss : 1545.6281716823578  
    Test accuracy 59/660 (8.939394%)  
    Epoch 5/30  
    Train loss : 1511.2544466257095  
    Test accuracy 78/660 (11.818182%)  
    Epoch 6/30  
    Train loss : 1483.153357744217  
    Test accuracy 88/660 (13.333333%)  
    Epoch 7/30  
    Train loss : 1459.7515054941177  
    Test accuracy 98/660 (14.848485%)  
    Epoch 8/30  
    Train loss : 1438.2500838041306  
    Test accuracy 98/660 (14.848485%)  
    Epoch 9/30  
    Train loss : 1422.3219151496887  
    Test accuracy 94/660 (14.242424%)  
    Epoch 10/30  
    Train loss : 1400.4271770715714  
    Test accuracy 95/660 (14.393939%)  
    Epoch 11/30  
    Train loss : 1386.8792620897293  
    Test accuracy 107/660 (16.212121%)  
    Epoch 12/30  
    Train loss : 1369.3411258459091  
    Test accuracy 103/660 (15.606061%)  
    Epoch 13/30  
    Train loss : 1348.5258370637894  
    Test accuracy 125/660 (18.939394%)  
    Epoch 14/30  
    Train loss : 1335.2068976163864  
    Test accuracy 129/660 (19.545455%)  
    Epoch 15/30  
    Train loss : 1321.1347044706345  
    Test accuracy 87/660 (13.181818%)  
    Epoch 16/30  
    Train loss : 1312.4720759391785  
    Test accuracy 120/660 (18.181818%)  
    Epoch 17/30  
    Train loss : 1296.539174079895  
    Test accuracy 105/660 (15.909091%)  
    Epoch 18/30  
    Train loss : 1293.357898235321  
    Test accuracy 147/660 (22.272727%)  
    Epoch 19/30  
    Train loss : 1269.5970216989517  
    Test accuracy 158/660 (23.939394%)  
    Epoch 20/30  
    Train loss : 1254.2561793327332  
    Test accuracy 116/660 (17.575758%)  
    Epoch 21/30  
    Train loss : 1228.313757777214  
    Test accuracy 145/660 (21.969697%)  
    Epoch 22/30  
    Train loss : 1195.190467953682  
    Test accuracy 171/660 (25.909091%)  
    Epoch 23/30  
    Train loss : 1202.648944735527  
    Test accuracy 177/660 (26.818182%)  
    Epoch 24/30  
    Train loss : 1183.2876206636429  
    Test accuracy 138/660 (20.909091%)  
    Epoch 25/30  
    Train loss : 1153.555959045887  
    Test accuracy 168/660 (25.454545%)  
    Epoch 26/30  
    Train loss : 1156.2759185433388  
    Test accuracy 184/660 (27.878788%)  
    Epoch 27/30  
    Train loss : 1136.1671670079231  
    Test accuracy 162/660 (24.545455%)  
    Epoch 28/30  
    Train loss : 1138.1718531847  
    Test accuracy 185/660 (28.030303%)  
    Epoch 29/30  
    Train loss : 1115.3888199925423  
    Test accuracy 191/660 (28.939394%)  
    Epoch 30/30  
    Train loss : 1087.6902663111687  
    Test accuracy 198/660 (30.000000%)  
    
- 2.2 (model - DenseNet)
    
    Epoch 1/30  
    Train loss : 1818.805026292801  
    Test accuracy 70/660 (10.606061%)  
    Epoch 2/30  
    Train loss : 1531.8445568084717  
    Test accuracy 86/660 (13.030303%)  
    Epoch 3/30  
    Train loss : 1491.5209988355637  
    Test accuracy 91/660 (13.787879%)  
    Epoch 4/30  
    Train loss : 1471.8161920309067  
    Test accuracy 71/660 (10.757576%)  
    Epoch 5/30  
    Train loss : 1421.1587158441544  
    Test accuracy 91/660 (13.787879%)  
    Epoch 6/30  
    Train loss : 1400.169784784317  
    Test accuracy 107/660 (16.212121%)  
    Epoch 7/30  
    Train loss : 1391.210727095604  
    Test accuracy 129/660 (19.545455%)  
    Epoch 8/30  
    Train loss : 1360.1707566976547  
    Test accuracy 107/660 (16.212121%)  
    Epoch 9/30  
    Train loss : 1354.5485212802887  
    Test accuracy 129/660 (19.545455%)  
    Epoch 10/30  
    Train loss : 1311.3734602928162  
    Test accuracy 128/660 (19.393939%)  
    Epoch 11/30  
    Train loss : 1301.185721039772  
    Test accuracy 133/660 (20.151515%)  
    Epoch 12/30  
    Train loss : 1257.6844409704208  
    Test accuracy 130/660 (19.696970%)  
    Epoch 13/30  
    Train loss : 1248.2922520637512  
    Test accuracy 153/660 (23.181818%)  
    Epoch 14/30  
    Train loss : 1237.014399588108  
    Test accuracy 154/660 (23.333333%)  
    Epoch 15/30  
    Train loss : 1227.6517922878265  
    Test accuracy 152/660 (23.030303%)  
    Epoch 16/30  
    Train loss : 1195.908163189888  
    Test accuracy 174/660 (26.363636%)  
    Epoch 17/30  
    Train loss : 1157.0713956356049  
    Test accuracy 191/660 (28.939394%)  
    Epoch 18/30  
    Train loss : 1136.7711527943611  
    Test accuracy 221/660 (33.484848%)  
    Epoch 19/30  
    Train loss : 1149.2795425057411  
    Test accuracy 198/660 (30.000000%)  
    Epoch 20/30  
    Train loss : 1109.3303876519203  
    Test accuracy 188/660 (28.484848%)  
    Epoch 21/30  
    Train loss : 1090.05551725626  
    Test accuracy 204/660 (30.909091%)  
    Epoch 22/30  
    Train loss : 1079.6727448105812  
    Test accuracy 203/660 (30.757576%)  
    Epoch 23/30  
    Train loss : 1066.7630581855774  
    Test accuracy 189/660 (28.636364%)  
    Epoch 24/30  
    Train loss : 1059.8549606204033  
    Test accuracy 208/660 (31.515152%)  
    Epoch 25/30  
    Train loss : 1059.3921986222267  
    Test accuracy 242/660 (36.666667%)  
    Epoch 26/30  
    Train loss : 1056.7434205114841  
    Test accuracy 264/660 (40.000000%)  
    Epoch 27/30  
    Train loss : 1051.563884794712  
    Test accuracy 244/660 (36.969697%)  
    Epoch 28/30  
    Train loss : 1009.0424494743347  
    Test accuracy 275/660 (41.666667%)  
    Epoch 29/30  
    Train loss : 962.385627746582  
    Test accuracy 215/660 (32.575758%)  
    Epoch 30/30  
    Train loss : 992.5944796800613  
    Test accuracy 288/660 (43.636364%)  
    
- 2.3 (densenet121)
    
    Epoch 1/30  
    Train loss : 1757.6924502849579  
    Test accuracy 95/660 (14.393939%)  
    Epoch 2/30  
    Train loss : 1542.097622036934  
    Test accuracy 90/660 (13.636364%)  
    Epoch 3/30  
    Train loss : 1496.8621106147766  
    Test accuracy 82/660 (12.424242%)  
    Epoch 4/30  
    Train loss : 1442.8013805150986  
    Test accuracy 83/660 (12.575758%)  
    Epoch 5/30  
    Train loss : 1432.0028762817383  
    Test accuracy 109/660 (16.515152%)  
    Epoch 6/30  
    Train loss : 1412.9116492271423  
    Test accuracy 115/660 (17.424242%)  
    Epoch 7/30  
    Train loss : 1380.3279049396515  
    Test accuracy 118/660 (17.878788%)  
    Epoch 8/30  
    Train loss : 1352.683423280716  
    Test accuracy 125/660 (18.939394%)  
    Epoch 9/30  
    Train loss : 1331.2695035934448  
    Test accuracy 111/660 (16.818182%)  
    Epoch 10/30  
    Train loss : 1300.0142563581467  
    Test accuracy 179/660 (27.121212%)  
    Epoch 11/30  
    Train loss : 1285.3112505674362  
    Test accuracy 152/660 (23.030303%)  
    Epoch 12/30  
    Train loss : 1278.9220252037048  
    Test accuracy 163/660 (24.696970%)  
    Epoch 13/30  
    Train loss : 1247.9215015172958  
    Test accuracy 145/660 (21.969697%)  
    Epoch 14/30  
    Train loss : 1221.3149000406265  
    Test accuracy 188/660 (28.484848%)  
    Epoch 15/30  
    Train loss : 1271.290284872055  
    Test accuracy 164/660 (24.848485%)  
    Epoch 16/30  
    Train loss : 1213.7111866474152  
    Test accuracy 200/660 (30.303030%)  
    Epoch 17/30  
    Train loss : 1170.9882282614708  
    Test accuracy 214/660 (32.424242%)  
    Epoch 18/30  
    Train loss : 1156.5821832418442  
    Test accuracy 212/660 (32.121212%)  
    Epoch 19/30  
    Train loss : 1138.4767770767212  
    Test accuracy 183/660 (27.727273%)  
    Epoch 20/30  
    Train loss : 1152.917645931244  
    Test accuracy 237/660 (35.909091%)  
    Epoch 21/30  
    Train loss : 1120.9327235221863  
    Test accuracy 220/660 (33.333333%)  
    Epoch 22/30  
    Train loss : 1120.5798943638802  
    Test accuracy 249/660 (37.727273%)  
    Epoch 23/30  
    Train loss : 1115.1758430600166  
    Test accuracy 214/660 (32.424242%)  
    Epoch 24/30  
    Train loss : 1078.6732615232468  
    Test accuracy 236/660 (35.757576%)  
    Epoch 25/30  
    Train loss : 1082.560262799263  
    Test accuracy 258/660 (39.090909%)  
    Epoch 26/30  
    Train loss : 1062.7868010997772  
    Test accuracy 275/660 (41.666667%)  
    Epoch 27/30  
    Train loss : 1050.5425499379635  
    Test accuracy 270/660 (40.909091%)  
    Epoch 28/30  
    Train loss : 1008.3743104934692  
    Test accuracy 267/660 (40.454545%)  
    Epoch 29/30  
    Train loss : 1037.8094412088394  
    Test accuracy 242/660 (36.666667%)  
    Epoch 30/30  
    Train loss : 1008.6187039017677  
    Test accuracy 257/660 (38.939394%)  
    
- 2.4 (custom)
    
    Epoch 1/30  
    Train loss : 1684.4103461503983  
    Test accuracy 74/660 (11.212121%)  
    Epoch 2/30  
    Train loss : 1528.6197370290756  
    Test accuracy 85/660 (12.878788%)  
    Epoch 3/30  
    Train loss : 1471.6831514835358  
    Test accuracy 87/660 (13.181818%)  
    Epoch 4/30  
    Train loss : 1431.2139061689377  
    Test accuracy 90/660 (13.636364%)  
    Epoch 5/30  
    Train loss : 1437.8142901659012  
    Test accuracy 112/660 (16.969697%)  
    Epoch 6/30  
    Train loss : 1415.9858133792877  
    Test accuracy 115/660 (17.424242%)  
    Epoch 7/30  
    Train loss : 1408.876769900322  
    Test accuracy 112/660 (16.969697%)  
    Epoch 8/30  
    Train loss : 1382.3290008306503  
    Test accuracy 126/660 (19.090909%)  
    Epoch 9/30  
    Train loss : 1359.0357863903046  
    Test accuracy 122/660 (18.484848%)  
    Epoch 10/30  
    Train loss : 1335.406890153885  
    Test accuracy 131/660 (19.848485%)  
    Epoch 11/30  
    Train loss : 1369.4550833702087  
    Test accuracy 132/660 (20.000000%)  
    Epoch 12/30  
    Train loss : 1349.266711473465  
    Test accuracy 116/660 (17.575758%)  
    Epoch 13/30  
    Train loss : 1318.537847518921  
    Test accuracy 173/660 (26.212121%)  
    Epoch 14/30  
    Train loss : 1297.0214554071426  
    Test accuracy 143/660 (21.666667%)  
    Epoch 15/30  
    Train loss : 1289.5315008163452  
    Test accuracy 159/660 (24.090909%)  
    Epoch 16/30  
    Train loss : 1266.0718561410904  
    Test accuracy 149/660 (22.575758%)  
    Epoch 17/30  
    Train loss : 1262.1636182069778  
    Test accuracy 158/660 (23.939394%)  
    Epoch 18/30  
    Train loss : 1216.138608455658  
    Test accuracy 169/660 (25.606061%)  
    Epoch 19/30  
    Train loss : 1234.7398719191551  
    Test accuracy 123/660 (18.636364%)  
    Epoch 20/30  
    Train loss : 1192.7351569533348  
    Test accuracy 167/660 (25.303030%)  
    Epoch 21/30  
    Train loss : 1196.6281063556671  
    Test accuracy 183/660 (27.727273%)  
    Epoch 22/30  
    Train loss : 1173.637413084507  
    Test accuracy 187/660 (28.333333%)  
    Epoch 23/30  
    Train loss : 1146.6372238397598  
    Test accuracy 182/660 (27.575758%)  
    Epoch 24/30  
    Train loss : 1133.4806493520737  
    Test accuracy 186/660 (28.181818%)  
    Epoch 25/30  
    Train loss : 1154.2448362708092  
    Test accuracy 178/660 (26.969697%)  
    Epoch 26/30  
    Train loss : 1127.3913524150848  
    Test accuracy 173/660 (26.212121%)  
    Epoch 27/30  
    Train loss : 1112.3012298941612  
    Test accuracy 213/660 (32.272727%)  
    Epoch 28/30  
    Train loss : 1112.5307074189186  
    Test accuracy 213/660 (32.272727%)  
    Epoch 29/30  
    Train loss : 1100.754581451416  
    Test accuracy 214/660 (32.424242%)  
    Epoch 30/30  
    Train loss : 1068.6898882985115  
    Test accuracy 237/660 (35.909091%)  
    
- final : 3.0 - hyperparameter, model,
    
    Epoch 1/50  
    Train loss : 1793.363133072853  
    Test accuracy 70/660 (10.606061%)  
    Epoch 2/50  
    Train loss : 1551.2929408550262  
    Test accuracy 49/660 (7.424242%)  
    Epoch 3/50  
    Train loss : 1504.8062771558762  
    Test accuracy 100/660 (15.151515%)  
    Epoch 4/50  
    Train loss : 1460.146014213562  
    Test accuracy 103/660 (15.606061%)  
    Epoch 5/50  
    Train loss : 1437.745771765709  
    Test accuracy 95/660 (14.393939%)  
    Epoch 6/50  
    Train loss : 1410.1092879772186  
    Test accuracy 112/660 (16.969697%)  
    Epoch 7/50  
    Train loss : 1397.6455438137054  
    Test accuracy 127/660 (19.242424%)  
    Epoch 8/50  
    Train loss : 1371.1415835618973  
    Test accuracy 150/660 (22.727273%)  
    Epoch 9/50  
    Train loss : 1333.0799448490143  
    Test accuracy 152/660 (23.030303%)  
    Epoch 10/50  
    Train loss : 1320.4952311515808  
    Test accuracy 148/660 (22.424242%)  
    Epoch 11/50  
    Train loss : 1308.4253058433533  
    Test accuracy 151/660 (22.878788%)  
    Epoch 12/50  
    Train loss : 1280.7437851428986  
    Test accuracy 177/660 (26.818182%)  
    Epoch 13/50  
    Train loss : 1255.5697748064995  
    Test accuracy 176/660 (26.666667%)  
    Epoch 14/50  
    Train loss : 1225.0958684682846  
    Test accuracy 216/660 (32.727273%)  
    Epoch 15/50  
    Train loss : 1206.9239443540573  
    Test accuracy 200/660 (30.303030%)  
    Epoch 16/50  
    Train loss : 1213.9880170822144  
    Test accuracy 219/660 (33.181818%)  
    Epoch 17/50  
    Train loss : 1181.6942682266235  
    Test accuracy 221/660 (33.484848%)  
    Epoch 18/50  
    Train loss : 1138.7970014810562  
    Test accuracy 241/660 (36.515152%)  
    Epoch 19/50  
    Train loss : 1113.7802930474281  
    Test accuracy 235/660 (35.606061%)  
    Epoch 20/50  
    Train loss : 1118.654246687889  
    Test accuracy 214/660 (32.424242%)  
    Epoch 21/50  
    Train loss : 1100.40710324049  
    Test accuracy 225/660 (34.090909%)  
    Epoch 22/50  
    Train loss : 1051.9472254514694  
    Test accuracy 233/660 (35.303030%)  
    Epoch 23/50  
    Train loss : 1042.4470603466034  
    Test accuracy 249/660 (37.727273%)  
    Epoch 24/50  
    Train loss : 1012.4233772754669  
    Test accuracy 291/660 (44.090909%)  
    Epoch 25/50  
    Train loss : 1000.8358233571053  
    Test accuracy 310/660 (46.969697%)  
    Epoch 26/50  
    Train loss : 991.0700577795506  
    Test accuracy 295/660 (44.696970%)  
    Epoch 27/50  
    Train loss : 964.8318086862564  
    Test accuracy 249/660 (37.727273%)  
    Epoch 28/50  
    Train loss : 941.2342943847179  
    Test accuracy 264/660 (40.000000%)  
    Epoch 29/50  
    Train loss : 936.9816718995571  
    Test accuracy 276/660 (41.818182%)  
    Epoch 30/50  
    Train loss : 936.9653038084507  
    Test accuracy 288/660 (43.636364%)  
    Epoch 31/50  
    Train loss : 904.3021990954876  
    Test accuracy 297/660 (45.000000%)  
    Epoch 32/50  
    Train loss : 900.0194394290447  
    Test accuracy 266/660 (40.303030%)  
    Epoch 33/50  
    Train loss : 866.0663493275642  
    Test accuracy 277/660 (41.969697%)  
    Epoch 34/50  
    Train loss : 847.7931573092937  
    Test accuracy 293/660 (44.393939%)  
    Epoch 35/50  
    Train loss : 830.5360671877861  
    Test accuracy 326/660 (49.393939%)  
    Epoch 36/50  
    Train loss : 839.0527000427246  
    Test accuracy 325/660 (49.242424%)  
    Epoch 37/50  
    Train loss : 825.6711771786213  
    Test accuracy 324/660 (49.090909%)  
    Epoch 38/50  
    Train loss : 803.4922357797623  
    Test accuracy 331/660 (50.151515%)  
    Epoch 39/50  
    Train loss : 791.5872900784016  
    Test accuracy 352/660 (53.333333%)  
    Epoch 40/50  
    Train loss : 769.6625090166926  
    Test accuracy 319/660 (48.333333%)  
    Epoch 41/50  
    Train loss : 775.2774451971054  
    Test accuracy 322/660 (48.787879%)  
    Epoch 42/50  
    Train loss : 757.9739295393229  
    Test accuracy 338/660 (51.212121%)  
    Epoch 43/50  
    Train loss : 746.7388896644115  
    Test accuracy 342/660 (51.818182%)  
    Epoch 44/50  
    Train loss : 743.3831364661455  
    Test accuracy 341/660 (51.666667%)  
    Epoch 45/50  
    Train loss : 719.4649023413658  
    Test accuracy 361/660 (54.696970%)  
    Epoch 46/50  
    Train loss : 711.2443339377642  
    Test accuracy 350/660 (53.030303%)  
    Epoch 47/50  
    Train loss : 683.6624288931489  
    Test accuracy 319/660 (48.333333%)  
    Epoch 48/50  
    Train loss : 685.6579523980618  
    Test accuracy 353/660 (53.484848%)  
    Epoch 49/50  
    Train loss : 661.1975513920188  
    Test accuracy 354/660 (53.636364%)  
    Epoch 50/50  
    Train loss : 651.4748446829617  
    Test accuracy 340/660 (51.515152%)  
    

→ epoch 100.

- Epoch 1/100  
    Train loss : 1824.9245611429214  
    Test accuracy 64/660 (9.696970%)  
    Epoch 2/100  
    Train loss : 1558.438700079918  
    Test accuracy 75/660 (11.363636%)  
    Epoch 3/100  
    Train loss : 1505.2105588912964  
    Test accuracy 92/660 (13.939394%)  
    Epoch 4/100  
    Train loss : 1451.9469701051712  
    Test accuracy 111/660 (16.818182%)  
    Epoch 5/100  
    Train loss : 1439.1715704202652  
    Test accuracy 99/660 (15.000000%)  
    Epoch 6/100  
    Train loss : 1416.9947715997696  
    Test accuracy 95/660 (14.393939%)  
    Epoch 7/100  
    Train loss : 1390.8373898267746  
    Test accuracy 130/660 (19.696970%)  
    Epoch 8/100  
    Train loss : 1370.6020830869675  
    Test accuracy 111/660 (16.818182%)  
    Epoch 9/100  
    Train loss : 1346.0591424703598  
    Test accuracy 146/660 (22.121212%)  
    Epoch 10/100  
    Train loss : 1313.2320946455002  
    Test accuracy 145/660 (21.969697%)  
    Epoch 11/100  
    Train loss : 1311.0344195365906  
    Test accuracy 166/660 (25.151515%)  
    Epoch 12/100  
    Train loss : 1284.1567192077637  
    Test accuracy 150/660 (22.727273%)  
    Epoch 13/100  
    Train loss : 1259.0329936742783  
    Test accuracy 185/660 (28.030303%)  
    Epoch 14/100  
    Train loss : 1217.1180562973022  
    Test accuracy 183/660 (27.727273%)  
    Epoch 15/100  
    Train loss : 1200.7038453817368  
    Test accuracy 209/660 (31.666667%)  
    Epoch 16/100  
    Train loss : 1187.8286690711975  
    Test accuracy 223/660 (33.787879%)  
    Epoch 17/100  
    Train loss : 1149.2340105772018  
    Test accuracy 213/660 (32.272727%)  
    Epoch 18/100  
    Train loss : 1129.8083550333977  
    Test accuracy 215/660 (32.575758%)  
    Epoch 19/100  
    Train loss : 1116.8163486123085  
    Test accuracy 229/660 (34.696970%)  
    Epoch 20/100  
    Train loss : 1092.547711968422  
    Test accuracy 240/660 (36.363636%)  
    Epoch 21/100  
    Train loss : 1064.2384468317032  
    Test accuracy 229/660 (34.696970%)  
    Epoch 22/100  
    Train loss : 1071.9822427034378  
    Test accuracy 266/660 (40.303030%)  
    Epoch 23/100  
    Train loss : 1033.000556230545  
    Test accuracy 234/660 (35.454545%)  
    Epoch 24/100  
    Train loss : 1007.7808876037598  
    Test accuracy 228/660 (34.545455%)  
    Epoch 25/100  
    Train loss : 989.3699949383736  
    Test accuracy 264/660 (40.000000%)  
    Epoch 26/100  
    Train loss : 983.7640237808228  
    Test accuracy 253/660 (38.333333%)  
    Epoch 27/100  
    Train loss : 974.8145852386951  
    Test accuracy 251/660 (38.030303%)  
    Epoch 28/100  
    Train loss : 950.4126779139042  
    Test accuracy 251/660 (38.030303%)  
    Epoch 29/100  
    Train loss : 931.176960170269  
    Test accuracy 266/660 (40.303030%)  
    Epoch 30/100  
    Train loss : 937.330546438694  
    Test accuracy 307/660 (46.515152%)  
    Epoch 31/100  
    Train loss : 899.4397295117378  
    Test accuracy 267/660 (40.454545%)  
    Epoch 32/100  
    Train loss : 874.2076793313026  
    Test accuracy 318/660 (48.181818%)  
    Epoch 33/100  
    Train loss : 877.0160283148289  
    Test accuracy 293/660 (44.393939%)  
    Epoch 34/100  
    Train loss : 870.6642246246338  
    Test accuracy 313/660 (47.424242%)  
    Epoch 35/100  
    Train loss : 846.6755894422531  
    Test accuracy 326/660 (49.393939%)  
    Epoch 36/100  
    Train loss : 826.9032298326492  
    Test accuracy 284/660 (43.030303%)  
    Epoch 37/100  
    Train loss : 830.9779167175293  
    Test accuracy 280/660 (42.424242%)  
    Epoch 38/100  
    Train loss : 809.2221790552139  
    Test accuracy 298/660 (45.151515%)  
    Epoch 39/100  
    Train loss : 791.8859551548958  
    Test accuracy 302/660 (45.757576%)  
    Epoch 40/100  
    Train loss : 790.843866750598  
    Test accuracy 334/660 (50.606061%)  
    Epoch 41/100  
    Train loss : 763.3826350569725  
    Test accuracy 318/660 (48.181818%)  
    Epoch 42/100  
    Train loss : 780.3598033487797  
    Test accuracy 321/660 (48.636364%)  
    Epoch 43/100  
    Train loss : 740.0392558425665  
    Test accuracy 322/660 (48.787879%)  
    Epoch 44/100  
    Train loss : 730.7030105739832  
    Test accuracy 315/660 (47.727273%)  
    Epoch 45/100  
    Train loss : 713.504777610302  
    Test accuracy 327/660 (49.545455%)  
    Epoch 46/100  
    Train loss : 698.9630656987429  
    Test accuracy 346/660 (52.424242%)  
    Epoch 47/100  
    Train loss : 692.5018476545811  
    Test accuracy 373/660 (56.515152%)  
    Epoch 48/100  
    Train loss : 696.0330566465855  
    Test accuracy 353/660 (53.484848%)  
    Epoch 49/100  
    Train loss : 673.8017378151417  
    Test accuracy 346/660 (52.424242%)  
    Epoch 50/100  
    Train loss : 676.9622166454792  
    Test accuracy 341/660 (51.666667%)  
    Epoch 51/100  
    Train loss : 650.1986406967044  
    Test accuracy 375/660 (56.818182%)  
    Epoch 52/100  
    Train loss : 655.1659116074443  
    Test accuracy 343/660 (51.969697%)  
    Epoch 53/100  
    Train loss : 632.6462698355317  
    Test accuracy 373/660 (56.515152%)  
    Epoch 54/100  
    Train loss : 618.1452964320779  
    Test accuracy 338/660 (51.212121%)  
    Epoch 55/100  
    Train loss : 612.8296367228031  
    Test accuracy 362/660 (54.848485%)  
    Epoch 56/100  
    Train loss : 601.4944356903434  
    Test accuracy 346/660 (52.424242%)  
    Epoch 57/100  
    Train loss : 612.7612150087953  
    Test accuracy 383/660 (58.030303%)  
    Epoch 58/100  
    Train loss : 581.8777996003628  
    Test accuracy 368/660 (55.757576%)  
    Epoch 59/100  
    Train loss : 584.1199950650334  
    Test accuracy 363/660 (55.000000%)  
    Epoch 60/100  
    Train loss : 556.5121412687004  
    Test accuracy 359/660 (54.393939%)  
    Epoch 61/100  
    Train loss : 531.7213869988918  
    Test accuracy 376/660 (56.969697%)  
    Epoch 62/100  
    Train loss : 530.67882392928  
    Test accuracy 387/660 (58.636364%)  
    Epoch 63/100  
    Train loss : 515.9044560901821  
    Test accuracy 367/660 (55.606061%)  
    Epoch 64/100  
    Train loss : 524.2822845932096  
    Test accuracy 386/660 (58.484848%)  
    Epoch 65/100  
    Train loss : 514.5531164370477  
    Test accuracy 372/660 (56.363636%)  
    Epoch 66/100  
    Train loss : 501.54947379790246  
    Test accuracy 391/660 (59.242424%)  
    Epoch 67/100  
    Train loss : 517.2050592005253  
    Test accuracy 369/660 (55.909091%)  
    Epoch 68/100  
    Train loss : 503.16262786462903  
    Test accuracy 388/660 (58.787879%)  
    Epoch 69/100  
    Train loss : 479.8545928746462  
    Test accuracy 360/660 (54.545455%)  
    Epoch 70/100  
    Train loss : 521.8097349852324  
    Test accuracy 394/660 (59.696970%)  
    Epoch 71/100  
    Train loss : 485.53263778425753  
    Test accuracy 369/660 (55.909091%)  
    Epoch 72/100  
    Train loss : 469.34399265795946  
    Test accuracy 413/660 (62.575758%)  
    Epoch 73/100  
    Train loss : 471.3291343264282  
    Test accuracy 389/660 (58.939394%)  
    Epoch 74/100  
    Train loss : 471.89770578499883  
    Test accuracy 404/660 (61.212121%)  
    Epoch 75/100  
    Train loss : 444.6286405194551  
    Test accuracy 385/660 (58.333333%)  
    Epoch 76/100  
    Train loss : 441.5125313065946  
    Test accuracy 376/660 (56.969697%)  
    Epoch 77/100  
    Train loss : 446.41625471413136  
    Test accuracy 400/660 (60.606061%)  
    Epoch 78/100  
    Train loss : 429.35336108272895  
    Test accuracy 410/660 (62.121212%)  
    Epoch 79/100  
    Train loss : 417.8156395731494  
    Test accuracy 401/660 (60.757576%)  
    Epoch 80/100  
    Train loss : 418.8426562445238  
    Test accuracy 393/660 (59.545455%)  
    Epoch 81/100  
    Train loss : 411.91742643807083  
    Test accuracy 404/660 (61.212121%)  
    Epoch 82/100  
    Train loss : 407.93418421410024  
    Test accuracy 381/660 (57.727273%)  
    Epoch 83/100  
    Train loss : 404.67410549893975  
    Test accuracy 403/660 (61.060606%)  
    Epoch 84/100  
    Train loss : 394.75862002233043  
    Test accuracy 384/660 (58.181818%)  
    Epoch 85/100  
    Train loss : 381.82506050914526  
    Test accuracy 412/660 (62.424242%)  
    Epoch 86/100  
    Train loss : 404.1513839121908  
    Test accuracy 406/660 (61.515152%)  
    Epoch 87/100  
    Train loss : 410.08552837884054  
    Test accuracy 401/660 (60.757576%)  
    Epoch 88/100  
    Train loss : 355.4564240947366  
    Test accuracy 414/660 (62.727273%)  
    Epoch 89/100  
    Train loss : 352.66036033420824  
    Test accuracy 403/660 (61.060606%)  
    Epoch 90/100  
    Train loss : 387.6534295864403  
    Test accuracy 398/660 (60.303030%)  
    Epoch 91/100  
    Train loss : 361.65959601663053  
    Test accuracy 401/660 (60.757576%)  
    Epoch 92/100  
    Train loss : 354.9988845640328  
    Test accuracy 408/660 (61.818182%)  
    Epoch 93/100  
    Train loss : 348.1276288558729  
    Test accuracy 416/660 (63.030303%)  
    Epoch 94/100  
    Train loss : 347.3392227618024  
    Test accuracy 426/660 (64.545455%)  
    Epoch 95/100  
    Train loss : 332.6423339792527  
    Test accuracy 416/660 (63.030303%)  
    Epoch 96/100  
    Train loss : 330.5023876866326  
    Test accuracy 404/660 (61.212121%)  
    Epoch 97/100  
    Train loss : 315.10936888307333  
    Test accuracy 412/660 (62.424242%)  
    Epoch 98/100  
    Train loss : 328.2401503538713  
    Test accuracy 411/660 (62.272727%)  
    Epoch 99/100  
    Train loss : 328.8844599018339  
    Test accuracy 399/660 (60.454545%)  
    Epoch 100/100  
    Train loss : 317.85233451100066  
    Test accuracy 431/660 (65.303030%)